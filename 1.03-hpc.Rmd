---
output:
  pdf_document: default
  html_document: default
---
# High Performance Computing (HPC) Environments

Hey Eric!  You might consider breaking this into two separate chatpers: 1 = working on remote computers
and 2 = high-performance computing.  The first could include all the stuff about scp, globus, rclone,
and google drive.


This is going to be a chapter on using High Performance Computing clusters.

There is a lot to convey here about using queues and things.

I know SGE pretty well at this point, but others might use slurm. 

Here is a good page with a comparison:  [https://confluence.csiro.au/display/SC/Reference+Guide%3A+Migrating+from+SGE+to+SLURM](https://confluence.csiro.au/display/SC/Reference+Guide%3A+Migrating+from+SGE+to+SLURM)

And here is a good primer on SGE stuff: [https://confluence.si.edu/display/HPC/Monitoring+your+Jobs](https://confluence.si.edu/display/HPC/Monitoring+your+Jobs)

I guess I'll have to see what the CSU students have access to.

## Accessing remote computers

Start off with some stuff about ssh and scp.

Maybe have a section on public and private keys so you don't have to put
your password in every time (I would like to get better with that,
as well).

Here is the deal on that.  On a mac, you can use 
```
ssh-keygen -t rsa -b 4096
```
and be sure to give it a password, so that your private key is not unencrypted.
For, if it is, then there is a chance (I do believe) that if someone were to 
obtain that file, they could gain access to all the computers you are authorized on.

Note that a lot of tutorials on the web have you generating a private key without
any encryption.  That is lame.


Then copy the .ssh/id_rsa.pub key to the .ssh/authorized_keys file on the server
(creating it if it needs to be there).  Then ssh to the server and your Mac will pop
up a window asking for a password or will prompt on the command line for one (note that the password stays local!).  You put in
the password that you used when you created the private key.  That
can be saved in the Mac keychain (on old version of OSX you might
get asked if you want to save it in the Mac keychain).  Voila!
Now you have access to the server with no need to type a password in there.

Note, since Sierra, you need to add this to a `.ssh/config` file to get the password
stored in the keychain:
```
Host *
   AddKeysToAgent yes
   UseKeychain yes     
```
And, it turns out that you also need to make sure the permissions on that
file are set appropriately:
```sh
chmod 600 ~/.ssh/config
```
It is actually pretty darn simple...

Note, to have access from another computer to the server, you probably just create 
a keypair for that computer, and add the public key to the authorized_keys. 

Then add something like this to your .bashrc:
```sh
# for quick ssh to hoffman
alias hoffy='ssh eriq@hoffman2.idre.ucla.edu'

# to be used in scp.  like scp file $hoff:~/
hoff=eriq@hoffman2.idre.ucla.edu
```

Note that this should only be done on a private computer account, not on a shared account on a computer.
Otherwise, everyone using that account will have access.


Also, an interesting thing to investigate might be SSHF/FUSE which just might
let one mount the HPC filesystem on a local directory, so you can work with
files there using RStudio, etc, get them all debugged, and then run them.

Check out some information about that here:
[https://www.digitalocean.com/community/tutorials/how-to-use-sshfs-to-mount-remote-file-systems-over-ssh](https://www.digitalocean.com/community/tutorials/how-to-use-sshfs-to-mount-remote-file-systems-over-ssh).

I went to [https://osxfuse.github.io/](https://osxfuse.github.io/) and I downloaded SSHFS 2.5.0 and FUSE for Mac OS X. Then I installed them,
doing a simple default install for each.

Then, you make a mountpoint, which you can do in your home directory if you want:
```sh
mkdir ~/hoffman  # this is the mountpoint
sshfs -o allow_other,defer_permissions,IdentityFile=~/.ssh/id_rsa eriq@hoffman2.idre.ucla.edu:/u/home/e/eriq/  ~/hoffman

# close it out:
umount ~/hoffman/
```
Note that the absolute path to my home folder there seemed to be important.  

Also, I can't get to `/u/nobackup/kruegg/eriq` from here by going through the symlink in my home directory
on hoffman, since that is essentially a different volume. But I should
be able to do this:
```sh
sshfs -o allow_other,defer_permissions,IdentityFile=~/.ssh/id_rsa eriq@hoffman2.idre.ucla.edu:/u/nobackup/kruegg/eriq  ~/hoffman
```
Yep! that works.  And I can open all those files from within RStudio.  Cool.

Note, I will have to have a separate mount point for $SCRATCH as well.

Shoot! That works really seamlessly!

I can also add to my .bashrc:
```sh
alias hoffuse='sshfs -o allow_other,defer_permissions,IdentityFile=~/.ssh/id_rsa eriq@hoffman2.idre.ucla.edu:/u/nobackup/kruegg/eriq  ~/hoffman'
```

Now, the bad news: you can open an Rstudio project from that remote, mounted volume, but it 
doesn't seem to really work.  It is incredibly slow, and then fails to quit properly, etc. 

After force-quitting it, I am unable to unmount the hoffuse volume.  What a mess.  OK, I probably
won't try (running the rstudio project over FUSE).  But it might be useful still for editing
small things.


## Transferring files to remote computers

### scp

### Globus

### Interfacing with "The Cloud"

Increasingly, data scientists and tech companies alike are keeping their
data "in the cloud." This means that they
pay a large tech firm like Amazon, Dropbox, or Google to store their data for them in
a place that can be accessed via the internet.  There are many advantages to this
model.  For one thing, the company that serves the data often will create multiple copies
of the data for backup and redundancy: a fire in a single data center is not a calamity
because the data are also stored elsewhere, and can often be accessed seamlessly from those
other locations with no apparent disruption of service.  For another, companies that are
in the business of storing and serving 
data to multiple clients have data centers that are well-networked, so that getting
data onto and off of their storage systems can be done very quickly over the internet
by an end-user with a good internet connection.

Five years ago, the idea of storing next generation sequencing data might have
sounded a little
crazy---it always seemed a laborious task getting the data off of the remote server at the 
sequencing center, so why not just keep the data in-house once you have it?
To be sure, keeping a copy of your 
data in-house still can make sense for long-term data archiving needs, but, today, cloud 
storage for your sequencing data can make a lot of sense. A few reasons are:

1. Transferring your data from the cloud to the remote HPC system
that you use to process the data can be very fast.
2. As above, your data can be redundantly backed up.
3. If your institution (university, agency, etc.) has an agreement with a cloud storage
service that provides you with unlimited storage and free network access, then storing
your sequencing data in the cloud will cost considerably less than buying a dedicated
large system of hard drives for data backup.  (One must wonder if service
agreements might not be at risk of renegotiation if many researchers start using their
unlimited institutional cloud storage space to store and/or archive their 
next generation sequencing data sets. My own agency's contract with Google runs
through 2021...but I have to think that these services are making plenty of money, even
if a handful of researchers store big sequence data in the cloud.  Nonetheless, you
should be careful not to put multiple copies of data sets, or intermediate files that
are easily regenerated, up in the cloud.)
4. If you are a PI with many lab members wishing to access the same data set, or even if
you are just a regular Joe/Joanna researcher but you wish to share your data, it is 
possible to effect that using your cloud service's sharing settings.  We will discuss
how to do this with Google Drive.

There are clearly advantages to using the cloud, but one small hurdle remains.  Most
of the time, working in an HPC environment, we are using Unix, which provides a consistent
set of tools for interfacing with other computers using SSH-based protocols (like `scp` 
for copying files from one remote computer to another).  Unfortunately, many common
cloud storage services do not offer an SSH based interface.  Rather, they typically process
requests from clients using an HTTPS protocol.  This protocol, which effectively runs the
world-wide web, is a natural choice for cloud services that most people will access
using a web browser; however, Unix does not traditionally come with a utility or command
to easily process the types of HTTPS transactions needed to network with
cloud storage.  Furthermore, there must be some security when it comes to accessing
your cloud-based storage---you don't want everyone to be able to access your files, so
your cloud service needs to have some way of authenticating people
(you and your labmates for example) that are authorized to access your data.

These problems have been overcome by a utility called `rclone`, the product of a
comprehensive open-source software project that brings the functionality of the
`rsync` utility (a common Unix tool used to synchronize and mirror file systems)
to cloud-based storage. (Note: `rclone` has nothing to do with the R programming
language, despite its name that looks like an R package.)
Currently `rclone` provides a consistent interface for accessing
files from over 35 different cloud storage providers, including Box, Dropbox, Google Drive,
and Microsoft OneDrive. Binaries for `rclone` can be downloaded for your desktop
machine from [https://rclone.org/downloads/](https://rclone.org/downloads/).  We will
talk about how to install it on your HPC system later.

Once `rclone` is installed and in your `PATH`, you invoke it in your terminal
with the command `rclone`.  Before we get into the details of the various `rclone` subcommands,
it will be helpful to take a glance at the information `rclone` records when it
configures itself to talk to your cloud service.  To do so, it creates a file called `~/.config/rclone/rclone.conf`, where it stores information about all the different
connections to cloud services you have set up.  For example, that
file on my system looks like this:
```
[gdrive-rclone]
type = drive
scope = drive
root_folder_id = 1I2EDV465N5732Tx1FFAiLWOqZRJcAzUd
token = {"access_token":"bs43.94cUFOe6SjjkofZ","token_type":"Bearer","refresh_token":"1/MrtfsRoXhgc","expiry":"2019-04-29T22:51:58.148286-06:00"}
client_id = 2934793-oldk97lhld88dlkh301hd.apps.googleusercontent.com
client_secret = MMq3jdsjdjgKTGH4rNV_y-NbbG
```
In this configuration:

* `gdrive-rclone` is the name by which rclone refers to this cloud storage location
* `root_folder_id` is the ID of the Google Drive folder that can be thought of as the root directory of `gdrive-rclone`. This ID is not the simple name of that directory on
your Google Drive, rather it is the unique name given by Google Drive to that directory.
You can see it by navigating in your browser to the directory you want and finding it
after the last slash in the URL.  For example, in the above case, the URL is: 
`https://drive.google.com/drive/u/1/folders/1I2EDV465N5732Tx1FFAiLWOqZRJcAzUd`
* `client_id` and `client_secret` are like a username and a shared secret that `rclone` uses
to authenticate the user to Google Drive as who they say they are. 
* `token` are the credentials used by `rclone` to make requests of Google Drive on the basis
of the user.

Note: the above does not include my
real credentials, as then anyone could use them to access my Google Drive!

To set up your own configuration file to use Google Drive, you will use the `rclone config`
command, but before you do that, you will want to wrangle a client_id from Google. Follow
the directions at [https://rclone.org/drive/#making-your-own-client-id](https://rclone.org/drive/#making-your-own-client-id). Things are a little different from in their step
by step, but you can muddle through to get to a screen with a client_ID and a client
secret that you can copy onto your clipboard.


Once you have done that, then run `rclone config` and follow the prompts.  A
typical session of `rclone config` for Google Drive access is given 
[here](https://rclone.org/drive/).  Don't choose to do the advanced setup; however
do use "auto config," which will bounce up a web page and let you authenticate rclone 
to your Google account. 

#### Basic Maneuvers

The syntax for use is:
```sh
rclone [options] subcommand  parameter1 [parameter 2...]
```
The "subcommand" part tells `rclone` what you want to do, like `copy` or `sync`, and
the "parameter" part of the above syntax is typically a path
specification to a directory or a file.  In using rclone to access the
cloud there is not a root directory, like `/` in Unix.  Instead, each remote
cloud access point is treated as the root directory, and you refer to it
by the name of the configuration followed by a colon.  In our example,
`gdrive-rclone:` is the root, and we don't need to add a `/` after it to 
start a path with it.  Thus `gdrive-rclone:this_dir/that_dir` is a 
valid path for `rclone` to a location on my Google Drive.  



Very often when moving, copying, or syncing files, the parameters
consist of:
```s
source-directory  destination-directory
```

One very important point is that, unlike the Unix commands `cp` and `mv`, rclone
likes to operate on directories, not on multiple named files.  

A few key subcommands:

- `ls`,  `lsd`, and `lsl` are like `ls`, `ls -d` and `ls -l`
```sh
rclone  lsd gdrive-rclone:
rclone  lsd gdrive-rclone:NOFU
```
- `copy`:  copy the _contents_ of a source _directory_ to a destination _directory_. One super cool
thing about this is that `rclone` won't re-copy files that are already on the destination and which
are identical to those in the source directory.
```sh
rclone copy bams gdrive-rclone:NOFU/bams
```
Note that the destination directory will be created if it does not already exist.  
- `sync`: make the contents of the destination directory look just like the
contents of the source directory. *WARNING* This will delete files in the destination 
directory that do not appear in the source directory.  

A few key options:

- `--dry-run`: don't actually copy, sync, or move anything.  Just tell me what you would have done.
- `-P`, `-v`, `-vv`: give me progress information, verbose output, or super-verbose output, respectively.
- `--tpslimit 10`: don't make any more than 10 transactions a second with Google Drive (should always be used when transferring files)
- `---fast-list`: combine multiple transactions together.  Should always be used with Google Drive,
especially when handling lots of files.
- `--drive-shared-with-me`: make the "root" directory a directory that shows all
of the Google Drive folders that people have shared with you.  This is key for accessing
folders that have been shared with you.

For example, try something like:
```sh
rclone --drive-shared-with-me lsd gdrive-rclone:
```

#### filtering: Be particular about the files you transfer {#rclone-filter}

`rclone` works a little differently than the Unix utility `cp`.  In particular,
`rclone` is not set up very well to copy individual files.  While there is a
an `rclone` command known as `copyto` that will allow you copy a single file,
you cannot (apparently) specify multiple, individual files that you wish to copy.

In other words, you can't do:
```sh
rclone copyto this_file.txt that_file.txt another_file.bam gdrive-rclone:dest_dir
```

In general, you will be better off using `rclone` to copy the *contents* of a directory
to the inside of the destination directory.  However, there are options in `rclone` that
can keep you from being totally indiscriminate about the files you transfer.  In other words,
you can *filter* the files that get transferred.  You can read about that at
[https://rclone.org/filtering/](https://rclone.org/filtering/).

For a quick example, imagine that you have a directory called `Data` on you Google Drive
that contains both VCF and BAM files.  You want to get only the VCF files (ending with `.vcf.gz`, say)
onto the current working directory on your cluster.  Then something like this works:
```sh
rclone copy --include *.vcf.gz gdrive-rclone:Data ./
```
Note that, if you are issuing this command on a Unix system in a directory
where the pattern `*.vcf.gz` will expand (by globbing) to multiple files, you will
get an error.  In that case, wrap the pattern in a pair of single quotes to keep
the shell from expanding it, like this:
```sh
rclone copy --include '*.vcf.gz' gdrive-rclone:Data ./
```



#### Feel free to make lots of configurations

You might want to configure a remote for each directory-specific project.
You can do that by just editing the configuration file. For example,
if I had a directory deep within my Google Drive, inside a chain of folders that
looked like, say, `Projects/NGS/Species/Salmon/Chinook/CentralValley/WinterRun`
where I was keeping
all my data on a project concerning winter-run Chinook salmon, then it would be
quite inconvenient to type `Projects/NGS/Species/Salmon/Chinook/CentralValley/WinterRun`
every time I wanted to copy or sync something within that directory.   Instead,
I could add the following
lines to my configuration file, essentially copying the existing configuration and
then modifying the configuration name and the `root_folder_id` to be the
Google Drive identifier for the folder  `Projects/NGS/Species/Salmon/Chinook/CentralValley/WinterRun` (which 
one can find by navigating to that folder in a web browser and pulling the ID from the
end of the URL.)  The updated configuration could look like:
```
[gdrive-winter-run]
type = drive
scope = drive
root_folder_id = 1MjOrclmP1udhxOTvLWDHFBVET1dF6CIn
token = {"access_token":"bs43.94cUFOe6SjjkofZ","token_type":"Bearer","refresh_token":"1/MrtfsRoXhgc","expiry":"2019-04-29T22:51:58.148286-06:00"}
client_id = 2934793-oldk97lhld88dlkh301hd.apps.googleusercontent.com
client_secret = MMq3jdsjdjgKTGH4rNV_y-NbbG
```
As long as the directory is still within the same Google Drive account, you can re-use
all the authorization information, and just change the `[name]` part and the `root_folder_id`.
Now this:
```sh
rclone copy src_dir gdrive-winter-run: 
```
puts items into `Projects/NGS/Species/Salmon/Chinook/CentralValley/WinterRun` on the Google Drive
without having to type that God-awful long path name.






#### Installing rclone on a remote machine without sudo access

The instructions on the website require root access.  You don't have to have root 
access to install rclone locally in your home directory somewhere. 
Copy the download link from [https://rclone.org/downloads/](https://rclone.org/downloads/) for
the type of operating system your remote machine uses (most likely Linux if it is a cluster).
Then transfer that with `wget`, unzip it and put the binary in your PATH.  It will look 
something like this:
```
wget https://downloads.rclone.org/rclone-current-linux-amd64.zip
unzip rclone-current-linux-amd64.zip
cp rclone-current-linux-amd64/rclone ~/bin
```
You won't get manual pages on your system, but you can always find the docs on the web.


#### Setting up configurations on the remote machine...

Is as easy as copying your config file to where it should go, which
is easy to find using the command:
```sh
rclone config file
```

#### Encrypting your config file

This makes sense, and it easy: with `rclone config` encryption is one of
the options.  When it is encrypted, use `rclone config show` to see what
it looks like in clear text.

#### Some other usage tips

Following an email exchange with Ren, I should mention how to do an md5 
checksum on the remote server to make sure that everything is correctly there.








### Getting files from a sequencing center

Very often sequencing centers will post all the data from a single
run of a machine at a secured (or unsecured) http address. You will
need to download those files to operate on them on your cluster or
local machine.  However some of the files available on the server
will likely belong to other researchers and you don't want to waste time
downloading them.

Let's take an example.  Suppose you are sent an email from the sequencing
center that says something like:

>Your samples are AW_F1 (female) and AW_M1 (male).
>You should be able to access the data from this link provided by YCGA:
>http://sysg1.cs.yale.edu:3010/5lnO9bs3zfa8LOhESfsYfq3Dc/061719/

You can easily access this web address using `rclone`.  You could set up a new
remote in your rclone config to point to `http://sysg1.cs.yale.edu`,
but, since you will only be using this once, to get your data, it makes
more sense to just specify the remote on the command line.  This can be 
done by passing `rclone` the URL address via the `--http-url` option, and
then, after that, telling it what protocol to use by adding `:http:` to
the command.  Here is what you would use to list the directories available
at the sequencing center URL:
```sh
# here is the command
% rclone lsd --http-url http://sysg1.cs.yale.edu:3010/5lnO9bs3zfa8LOhESfsYfq3Dc/061719/ :http:

# and here is the output
          -1 1969-12-31 16:00:00        -1 sjg73_fqs
          -1 1969-12-31 16:00:00        -1 sjg73_supernova_fqs
```
Aha! There are two directories that might hold our sequencing data.
I wonder what is in those diretories?  The `rclone tree` command is the 
perfect way to drill down into those diretories and look at their contents:
```sh
% rclone tree --http-url http://sysg1.cs.yale.edu:3010/5lnO9bs3zfa8LOhESfsYfq3Dc/061719/ :http:
/
├── sjg73_fqs
│   ├── AW_F1
│   │   ├── AW_F1_S2_L001_I1_001.fastq.gz
│   │   ├── AW_F1_S2_L001_R1_001.fastq.gz
│   │   └── AW_F1_S2_L001_R2_001.fastq.gz
│   ├── AW_M1
│   │   ├── AW_M1_S3_L001_I1_001.fastq.gz
│   │   ├── AW_M1_S3_L001_R1_001.fastq.gz
│   │   └── AW_M1_S3_L001_R2_001.fastq.gz
│   └── ESP_A1
│       ├── ESP_A1_S1_L001_I1_001.fastq.gz
│       ├── ESP_A1_S1_L001_R1_001.fastq.gz
│       └── ESP_A1_S1_L001_R2_001.fastq.gz
└── sjg73_supernova_fqs
    ├── AW_F1
    │   ├── AW_F1_S2_L001_I1_001.fastq.gz
    │   ├── AW_F1_S2_L001_R1_001.fastq.gz
    │   └── AW_F1_S2_L001_R2_001.fastq.gz
    ├── AW_M1
    │   ├── AW_M1_S3_L001_I1_001.fastq.gz
    │   ├── AW_M1_S3_L001_R1_001.fastq.gz
    │   └── AW_M1_S3_L001_R2_001.fastq.gz
    └── ESP_A1
        ├── ESP_A1_S1_L001_I1_001.fastq.gz
        ├── ESP_A1_S1_L001_R1_001.fastq.gz
        └── ESP_A1_S1_L001_R2_001.fastq.gz

8 directories, 18 files
```
Whoa! That is pretty cool!.  From this output we see that there are 
subdirectories named `AW_F1` and `AW_M1` that hold the files that
we want.  And, of course, the `ESP_A1` samples must belong to someone
else.  It would be great if we could just download the files we wanted,
excluding the ones in the `ESP_A1` directories.  It turns out that there is!
`rclone` has an `--exclude` option to exclude paths that match certain
patterns (see Section \@ref(rclone-filter), above).  We can
experiment by giving `rclone copy` the `--dry-run` command to see which
files will be transferred. If we don't do any filtering, we see this
when we try to dry-run copy the directories to our local directory `Alewife/fastqs`:
```sh
% rclone copy --dry-run --http-url http://sysg1.cs.yale.edu:3010/5lnO9bs3zfa8LOhESfsYfq3Dc/061719/ :http: Alewife/fastqs/
2019/07/11 10:33:43 NOTICE: sjg73_fqs/ESP_A1/ESP_A1_S1_L001_I1_001.fastq.gz: Not copying as --dry-run
2019/07/11 10:33:43 NOTICE: sjg73_fqs/ESP_A1/ESP_A1_S1_L001_R1_001.fastq.gz: Not copying as --dry-run
2019/07/11 10:33:43 NOTICE: sjg73_fqs/ESP_A1/ESP_A1_S1_L001_R2_001.fastq.gz: Not copying as --dry-run
2019/07/11 10:33:43 NOTICE: sjg73_supernova_fqs/AW_M1/AW_M1_S3_L001_I1_001.fastq.gz: Not copying as --dry-run
2019/07/11 10:33:43 NOTICE: sjg73_supernova_fqs/AW_M1/AW_M1_S3_L001_R1_001.fastq.gz: Not copying as --dry-run
2019/07/11 10:33:43 NOTICE: sjg73_supernova_fqs/AW_M1/AW_M1_S3_L001_R2_001.fastq.gz: Not copying as --dry-run
2019/07/11 10:33:43 NOTICE: sjg73_supernova_fqs/AW_F1/AW_F1_S2_L001_I1_001.fastq.gz: Not copying as --dry-run
2019/07/11 10:33:43 NOTICE: sjg73_supernova_fqs/AW_F1/AW_F1_S2_L001_R1_001.fastq.gz: Not copying as --dry-run
2019/07/11 10:33:43 NOTICE: sjg73_supernova_fqs/AW_F1/AW_F1_S2_L001_R2_001.fastq.gz: Not copying as --dry-run
2019/07/11 10:33:43 NOTICE: sjg73_supernova_fqs/ESP_A1/ESP_A1_S1_L001_I1_001.fastq.gz: Not copying as --dry-run
2019/07/11 10:33:43 NOTICE: sjg73_supernova_fqs/ESP_A1/ESP_A1_S1_L001_R1_001.fastq.gz: Not copying as --dry-run
2019/07/11 10:33:43 NOTICE: sjg73_supernova_fqs/ESP_A1/ESP_A1_S1_L001_R2_001.fastq.gz: Not copying as --dry-run
2019/07/11 10:33:43 NOTICE: sjg73_fqs/AW_F1/AW_F1_S2_L001_I1_001.fastq.gz: Not copying as --dry-run
2019/07/11 10:33:43 NOTICE: sjg73_fqs/AW_F1/AW_F1_S2_L001_R1_001.fastq.gz: Not copying as --dry-run
2019/07/11 10:33:43 NOTICE: sjg73_fqs/AW_F1/AW_F1_S2_L001_R2_001.fastq.gz: Not copying as --dry-run
2019/07/11 10:33:43 NOTICE: sjg73_fqs/AW_M1/AW_M1_S3_L001_I1_001.fastq.gz: Not copying as --dry-run
2019/07/11 10:33:43 NOTICE: sjg73_fqs/AW_M1/AW_M1_S3_L001_R1_001.fastq.gz: Not copying as --dry-run
2019/07/11 10:33:43 NOTICE: sjg73_fqs/AW_M1/AW_M1_S3_L001_R2_001.fastq.gz: Not copying as --dry-run
```
Since we do not want to copy the `ESP_A1` files we see if we can exclude
them:
```sh
% rclone copy --exclude */ESP_A1/* --dry-run --http-url http://sysg1.cs.yale.edu:3010/5lnO9bs3zfa8LOhESfsYfq3Dc/061719/ :http: Alewife/fastqs/
2019/07/11 10:37:22 NOTICE: sjg73_fqs/AW_F1/AW_F1_S2_L001_I1_001.fastq.gz: Not copying as --dry-run
2019/07/11 10:37:22 NOTICE: sjg73_fqs/AW_F1/AW_F1_S2_L001_R2_001.fastq.gz: Not copying as --dry-run
2019/07/11 10:37:22 NOTICE: sjg73_fqs/AW_F1/AW_F1_S2_L001_R1_001.fastq.gz: Not copying as --dry-run
2019/07/11 10:37:22 NOTICE: sjg73_fqs/AW_M1/AW_M1_S3_L001_I1_001.fastq.gz: Not copying as --dry-run
2019/07/11 10:37:22 NOTICE: sjg73_fqs/AW_M1/AW_M1_S3_L001_R2_001.fastq.gz: Not copying as --dry-run
2019/07/11 10:37:22 NOTICE: sjg73_fqs/AW_M1/AW_M1_S3_L001_R1_001.fastq.gz: Not copying as --dry-run
2019/07/11 10:37:22 NOTICE: sjg73_supernova_fqs/AW_F1/AW_F1_S2_L001_I1_001.fastq.gz: Not copying as --dry-run
2019/07/11 10:37:22 NOTICE: sjg73_supernova_fqs/AW_F1/AW_F1_S2_L001_R2_001.fastq.gz: Not copying as --dry-run
2019/07/11 10:37:22 NOTICE: sjg73_supernova_fqs/AW_F1/AW_F1_S2_L001_R1_001.fastq.gz: Not copying as --dry-run
2019/07/11 10:37:22 NOTICE: sjg73_supernova_fqs/AW_M1/AW_M1_S3_L001_I1_001.fastq.gz: Not copying as --dry-run
2019/07/11 10:37:22 NOTICE: sjg73_supernova_fqs/AW_M1/AW_M1_S3_L001_R1_001.fastq.gz: Not copying as --dry-run
2019/07/11 10:37:22 NOTICE: sjg73_supernova_fqs/AW_M1/AW_M1_S3_L001_R2_001.fastq.gz: Not copying as --dry-run
```
Booyah!  That gets us just what we want.  So, then we remove the `--dry-run` option,
and maybe add `-v -P` to give us verbose output and progress information, and copy all of our files:
```sh
% rclone copy --exclude */ESP_A1/* -v -P  --http-url http://sysg1.cs.yale.edu:3010/5lnO9bs3zfa8LOhESfsYfq3Dc/061719/ :http: Alewife/fastqs/
```


## Activating/Installing software

### Modules 

This is if your sys admin has made it easy.

### Miniconda

This is how one will probably want to do it

#### Testing this on Summit

I just want to quickly try this:
```sh
ssh eriq@colostate.edu@login.rc.colorado.edu

# get on compile nodes
ssh scompile

# I checked modules and found no samtools, bcftools, etc.

# Now install miniconda
mkdir conda_install
cd conda_install/
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
chmod u+x Miniconda3-latest-Linux-x86_64.sh 
./Miniconda3-latest-Linux-x86_64.sh 

# then you follow the prompts and agree to the license and the 
# default install location.

## NOTE: Might want to install to a different location if there
## are serious caps on hard disk usage in home directory..

source ~/.bashrc

# after that I have it!
(base) [eriq@colostate.edu@shas0136 conda_install]$
```

Now, let's see if we can get samtools.  Just google "samtools conda" to
get an idea of how to do it:
```sh
conda install -c bioconda samtools
```
AFter that, it just works! Cool!
```sh
(base) [eriq@colostate.edu@shas0136 ~]$ samtools

Program: samtools (Tools for alignments in the SAM format)
Version: 1.9 (using htslib 1.9)

Usage:   samtools <command> [options]

Commands:
  -- Indexing
     dict           create a sequence dictionary file
     faidx          index/extract FASTA
     fqidx          index/extract FASTQ
     index          index alignment

...
```

All right! That is amazing.  Next steps:

1. Tell students about establishing different environments.
2. Learn about how to make a minimal environment for a project and how to 
record that and be able to distribute/propagate it.

Along those lines, I want to see if, when I install samtools into a new environment,
it re-downloads it or not...
```sh
conda create --name quick-test
conda activate quick-test

# i also made an environment in a specified directory (you
# could put these within a project directory)
conda create --prefix ./test-envs-dir 
conda activate ./test-envs-dir

# now, let's install bcftools there
conda install -c bioconda bcftools

# note that it doesn't re-download the dependencies, as far as I can tell.
conda activate base
bcftools # not found in environment base

conda activate ./test-envs-dir
bcftools  # it is found in this environment.  Cool.

conda activate quick-test
bcftools  # it ain't here
```
Now, after that, bcftools is in ./test-envs-dir/bcftools

So, what if we install it into another environment.  Does it symlink it?
```sh
conda activate base
conda install -c bioconda bcftools
bcftools

# whoa! That errored out!
bcftools: error while loading shared libraries: libcrypto.so.1.0.0: cannot open shared object file: No such file or directory

# that is a serious problem.  

# Can I get it in my other environment?
conda activate quick-test
conda install -c bioconda bcftools
bcftools

# that totally works, but it still doesn't in base...

# so, what if we add samtools to our other environments?
# that works fine.
```

But, samtools/bcftools dependency issues are a known problem:  https://github.com/sunbeam-labs/sunbeam/issues/181
Basically they rely on different versions of some ssh libs.  

Note that installing bcftools first things work.  But what if we make another environment
and install samtools first again?
```sh
conda create --name samtools-first
conda activate samtools-first
conda install -c bioconda samtools  # this didn't download anything new
conda install -c bioconda bcftools

# BOOM! THIS CREATES A FAIL.  SO, YOU GOTTA INSTALL BCFTOOLS
# FIRST.  FAR OUT.
```

### Exporting environments

Looks like we should be able to do this.  Let's do the one that works:
```sh
(quick-test) [~]--% conda env export > quick-test-env.yml

(quick-test) [~]--% cat quick-test-env.yml 
name: quick-test
channels:
  - bioconda
  - defaults
dependencies:
  - _libgcc_mutex=0.1=main
  - bcftools=1.9=ha228f0b_4
  - bzip2=1.0.8=h7b6447c_0
  - ca-certificates=2019.5.15=1
  - curl=7.65.3=hbc83047_0
  - htslib=1.9=ha228f0b_7
  - krb5=1.16.1=h173b8e3_7
  - libcurl=7.65.3=h20c2e04_0
  - libdeflate=1.0=h14c3975_1
  - libedit=3.1.20181209=hc058e9b_0
  - libgcc-ng=9.1.0=hdf63c60_0
  - libssh2=1.8.2=h1ba5d50_0
  - libstdcxx-ng=9.1.0=hdf63c60_0
  - ncurses=6.1=he6710b0_1
  - openssl=1.1.1c=h7b6447c_1
  - samtools=1.9=h10a08f8_12
  - tk=8.6.8=hbc83047_0
  - xz=5.2.4=h14c3975_4
  - zlib=1.2.11=h7b6447c_3
prefix: /home/eriq@colostate.edu/miniconda3/envs/quick-test


# OK, that is cool.  Now, if we wanted to email that to someone,
# we could, and then they could do this:
conda env create --name dupie-quick -f quick-test-env.yml 

# that environment then has samtools and bcftools

# note that it probably would try to name it quick-test if we didn't
# pass in the name there...

```

## Boneyard







This is a variant of rsync that let's you sync stuff up to google drive.  It might be a better
solution than rcp for getting stuff onto and off of google drive.  Here is a link:
[https://rclone.org/](https://rclone.org/).  I need to evaluate it.  It might also be a good way
to backup some of my workstuff on my laptop to Google Drive (and maybe also for other people to create replicas and have a decent backup if they have unlimited Google Drive storage).


I got this working.  It is important to set your own OAuth client ID:
[https://forum.rclone.org/t/very-slow-sync-to-google-drive/6903](https://forum.rclone.org/t/very-slow-sync-to-google-drive/6903)

After that I did like this:
```
rclone sync -vv --tpslimit 10 --fast-list Otsh_v1.0_genomic.fna  gdrive-rclone:spoogee-spoogee
```
which did 2 Gb of fasta into the spoogee-spoogee directory pretty quickly.

But, with something that has lots of files, it took longer:
```
# this is only about 100 Mb but took a long time
rclone copy -P --tpslimit 10 --fast-list  rubias  gdrive-rclone:rubias
```
However, once that is done, you can sync it and it finds that parts that have changed pretty quickly.

it appears to do that by file modification times:
```sh
2019-04-19 23:21 /Otsh_v1.0/--% rclone sync -vv --tpslimit 10 --fast-list Otsh_v1.0_genomic.fna  gdrive-rclone:spoogee-spoogee
2019/04/19 23:21:36 DEBUG : rclone: Version "v1.47.0" starting with parameters ["rclone" "sync" "-vv" "--tpslimit" "10" "--fast-list" "Otsh_v1.0_genomic.fna" "gdrive-rclone:spoogee-spoogee"]
2019/04/19 23:21:36 DEBUG : Using config file from "/Users/eriq/.config/rclone/rclone.conf"
2019/04/19 23:21:36 INFO  : Starting HTTP transaction limiter: max 10 transactions/s with burst 1
2019/04/19 23:21:37 DEBUG : GCF_002872995.1_Otsh_v1.0_genomic.gff.gz: Excluded
2019/04/19 23:21:37 DEBUG : Otsh_v1.0_genomic.dict: Excluded
2019/04/19 23:21:37 DEBUG : Otsh_v1.0_genomic.fna.amb: Excluded
2019/04/19 23:21:37 DEBUG : Otsh_v1.0_genomic.fna.ann: Excluded
2019/04/19 23:21:37 DEBUG : Otsh_v1.0_genomic.fna.bwt: Excluded
2019/04/19 23:21:37 DEBUG : Otsh_v1.0_genomic.fna.fai: Excluded
2019/04/19 23:21:37 DEBUG : Otsh_v1.0_genomic.fna.pac: Excluded
2019/04/19 23:21:37 DEBUG : Otsh_v1.0_genomic.fna.sa: Excluded
2019/04/19 23:21:37 INFO  : Google drive root 'spoogee-spoogee': Waiting for checks to finish
2019/04/19 23:21:37 DEBUG : Otsh_v1.0_genomic.fna: Size and modification time the same (differ by 0s, within tolerance 1s)
2019/04/19 23:21:37 DEBUG : Otsh_v1.0_genomic.fna: Unchanged skipping
2019/04/19 23:21:37 INFO  : Google drive root 'spoogee-spoogee': Waiting for transfers to finish
2019/04/19 23:21:37 INFO  : Waiting for deletions to finish
2019/04/19 23:21:37 INFO  : 
Transferred:   	         0 / 0 Bytes, -, 0 Bytes/s, ETA -
Errors:                 0
Checks:                 1 / 1, 100%
Transferred:            0 / 0, -
Elapsed time:        1.3s

2019/04/19 23:21:37 DEBUG : 5 go routines active
2019/04/19 23:21:37 DEBUG : rclone: Version "v1.47.0" finishing with parameters ["rclone" "sync" "-vv" "--tpslimit" "10" "--fast-list" "Otsh_v1.0_genomic.fna" "gdrive-rclone:spoogee-spoogee"]
2
```

So, for moving big files around that might be a good way forward.  I will have to do a test with some big files.

And I need to test it with team drives so that multiple individuals can pull stuff off of the Bird Genoscape drive for example.

It would be nice to have safeguards so people don't trash stuff accidentally....

#### rclone on Hoffman

Their default install script expects sudo access to put it in /usr/local
but I don't on hoffman, obviously, so I just downloaded the the install script and edited
the section for Linux to look like this at the relevant part
```sh
case $OS in
  'linux')
    #binary
    cp rclone ~/bin/rclone.new
    chmod 755 ~/bin/rclone.new
    #chown root:root /usr/bin/rclone.new
    mv ~/bin/rclone.new ~/bin/rclone
    #manuals
    #mkdir -p /usr/local/share/man/man1
    #cp rclone.1 /usr/local/share/man/man1/
    #mandb
    ;;
```
I don't get man pages, but I get it in ~/bin no problem.

To set up the configuration, check where it belongs:
```sh
% rclone config file
Configuration file doesn't exist, but rclone will use this path:
/u/home/e/eriq/.config/rclone/rclone.conf
```
And then I just put my config file from my laptop on there.  I just pasted the stuff 
in whilst emacsing it.  Holy cow!  That is super easy.

Note that the config file is where you can also set default options like tpslimit and fast-list I think.

So, the OAuth stuff is all stored in that config file. And if you can set it up on one machine you can
go put it on any others that you want.  That is awesome.

When it was done, I tested it:
```sh
% rclone sync -vv  --drive-shared-with-me  gdrive-rclone:BaselinePaper  BaselinePaper_here
2019/04/29 14:49:24 DEBUG : rclone: Version "v1.47.0" starting with parameters ["rclone" "sync" "-vv" "--drive-shared-with-me" "gdrive-rclone:BaselinePaper" "BaselinePaper_here"]
2019/04/29 14:49:24 DEBUG : Using config file from "/u/home/e/eriq/.config/rclone/rclone.conf"
2019/04/29 14:49:25 INFO  : Local file system at /u/home/e/eriq/BaselinePaper_here: Waiting for checks to finish
2019/04/29 14:49:25 INFO  : Local file system at /u/home/e/eriq/BaselinePaper_here: Waiting for transfers to finish
2019/04/29 14:49:26 DEBUG : Local file system at /u/home/e/eriq/BaselinePaper_here: File to upload is small (41922 bytes), uploading instead of streaming
2019/04/29 14:49:26 DEBUG : BaselinePaper_Body.docx: Failed to pre-allocate: operation not supported
2019/04/29 14:49:26 INFO  : BaselinePaper_Body.docx: Copied (new)
2019/04/29 14:49:26 DEBUG : BaselinePaper_Body.docx: Updating size of doc after download to 41922
2019/04/29 14:49:26 INFO  : BaselinePaper_Body.docx: Copied (Rcat, new)
2019/04/29 14:49:27 DEBUG : Local file system at /u/home/e/eriq/BaselinePaper_here: File to upload is small (57172 bytes), uploading instead of streaming
2019/04/29 14:49:27 DEBUG : ResponseToReviewers_eca.docx: Failed to pre-allocate: operation not supported
2019/04/29 14:49:27 INFO  : ResponseToReviewers_eca.docx: Copied (new)
2019/04/29 14:49:27 DEBUG : ResponseToReviewers_eca.docx: Updating size of doc after download to 57172
2019/04/29 14:49:27 INFO  : ResponseToReviewers_eca.docx: Copied (Rcat, new)
2019/04/29 14:49:27 INFO  : Waiting for deletions to finish
2019/04/29 14:49:27 INFO  : 
Transferred:   	  193.543k / 193.543 kBytes, 100%, 79.377 kBytes/s, ETA 0s
Errors:                 0
Checks:                 0 / 0, -
Transferred:            4 / 4, 100%
Elapsed time:        2.4s

2019/04/29 14:49:27 DEBUG : 6 go routines active
2019/04/29 14:49:27 DEBUG : rclone: Version "v1.47.0" finishing with parameters ["rclone" "sync" "-vv" "--drive-shared-with-me" "gdrive-rclone:BaselinePaper" "BaselinePaper_here"]
```
That was fast and super solid.


#### Encrypt the config file

You can use `rclone config edit` to set a password for the config file.  Then it
encrypts that so no one is able to run wild if they just get that file.  You have to
provide your password to do any of the rclone commands.  If you want to see the
config file use `rclone config show`.  You could always copy that elsewhere, and then
re-encrypt it.








Here is some nice stuff for summarizing all the information from the different runs from the chinook-wgs project:
```sh
qacct -o eriq -b 09271925 -j ml | tidy-qacct
```

Explain scratch space and how clusters are configured with respect to storage, etc.

Strategies---break names up with consistent characters:

- dashes within population names
- underscores for different groups of chromosomes
- periods for catenating pairs of pops

etc.  Basically, it just makes it much easier to split things up
when the time comes.

## The Queue  (SLURM/SGE/UGE)

## Modules package

## Compiling programs without admin privileges

Inevitably you will want to use a piece of software that is not available as
a module or is not otherwise installed on they system.

Typically these software programs have a frightful web of dependencies.

Unix/Linux distros typically maintain all these dependencies as libraries or packages
that can be installed using a `rpm` or `yum`.  However, the simple "plug-and-play" approach
to using these programs requires have administrator privileges so that the software can
be installed in one of the (typically protected) paths in the root (like `/usr/bin`).

But, you can use these programs to install packages into your home directory.  Once you have done
that, you need to let your system know where to look for these packages when it needs them
(i.e., when running a program or _linking_ to it whilst compiling up a program that uses it
as a dependency.

Hoffman2 runs CentOS.  Turns out that CentOS uses `yum` as a package manager.

Let's see if we can install llvm using yum.

```sh
yum search all llvm # <- this got me to devtoolset-7-all.x86_64 : Package shipping all available toolsets.

# a little web searching made it look like llvm-toolset-7-5.0.1-4.el7.x86_64.rpm or devtoolset-7-llvm-7.0-5.el7.x86_64.rpm
# might be what we want.  The first is a dependency of the second...
mkdir ~/centos

```
Was using instructions at [https://stackoverflow.com/questions/36651091/how-to-install-packages-in-linux-centos-without-root-user-with-automatic-depen](https://stackoverflow.com/questions/36651091/how-to-install-packages-in-linux-centos-without-root-user-with-automatic-depen) 

Couldn't get yum downloader to download any packages.  The whole thing looked like it was going to
be a mess, so I thought I would try with miniconda.

I installed miniconda (python 2.7 version) into `/u/nobackup/kruegg/eriq/programs/miniconda/` and then did this:
```sh
# probably could have listed them all at once, but wanted to watch them go 
# one at a time...
conda install numpy
conda install scipy
conda install pandas
conda install numba

# those all ran great.

conda install pysnptools

# that one didn't find a match, but I found on the web that I should try:
conda install -c bioconda pysnptools 

# that worked!
```


Also we want to touch briefly on LD_PATH (linking failures---and note that libraries are often
named libxxx.a) and CPATH (for failure to find xxxx.h), etc.




## Job arrays

Definitely mention the `eval` keyword in bash for when you want to print 
command lines with redirects.  

Show the routine for it, and develop a good approach to efficiently
orchestrating redos.  If you know the taskIDs of the ones that failed
then it is pretty easy to write an awk script that picks out the
commands and puts them in a new file.  Actually, it is probably
better to just cycle over the numbers and use the -t option
to launch each.  Then there is now changing the job-ids file.  

In fact, I am starting to think that the -t option is better than
putting it into the file.

Question: if you give something on the command line, does that override
the directive in the header of the file?  If so, then you don't even
need to change the file.  Note that using the qsub command line options
instead of the directives really opens up a lot of possibilities for
writing useful scripts that are flexible.  

Also use short names for the jobs and have a system for naming the
redos (append numbers so you know which round it is, too) 
possibly base the name on the ways things failed the first time.  Like,
`fsttf1` = "Fst run for things that failed due to time limits, 1". Or
structure things so that redos can just be done by invoking it with -t 
and the jobid.

## Writing stdout and stderr to files

This is always good to do.  Note that `stdbuf` is super useful here so that
things don't get buffered super long. (PCAngsd doesn't seem to write antyhing till
the end...)


## Breaking stuff down

It is probably worth talking about how problems can be broken down into
smaller ones.  Maybe give an example, and then say that we will be talking about
this for every step of the way in bioinformatic pipelines.

One thing to note---sometimes processes go awry for one reason or another.
When things are in smaller chunks it is not such a huge investment to
re-run it. (Unlike stuff that runs for two weeks before you realize that
it ain't working right).

