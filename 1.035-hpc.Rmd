---
output:
  pdf_document: default
  html_document: default
---
# High Performance Computing Clusters (HPCC's)

One interesting aspect of modern next generation sequencing data is simply its
sheer size:  it is not uncommon to receive a half or a full terabyte of data from
a sequencing center.  It is impractical to store this much data on your
laptop, let alone analyze it there.  Crunching through such a quantity of data on a single
computer or server could take a very long time.  Instead, you will likely break
up the analysis of such data into a number of smaller jobs, and send them off to
run on an assortment of different computers in a High Performance Computing Cluster (HPCC).  

Thus, even if you have immersed yourself in bioinformatic data file formats and honed
your skills at shell programming and accessing remote computers, sequence data analysis
remains such a formidable foe that there is still one last key area of computing
in which you must be fluent, in order to comfortably do bioinformatics: you must understand
how to submit and manage jobs sent to an HPCC.  

My first experience with HPCC's occurred when I started analyzing
high-throughput sequencer output.  I had over 15 years experience in shell
programming at that time, and I was given some example analysis scripts to emulate, but I still
found it took several weeks before I was moderately comfortable in an HPCC environment.  Most
HPCC's have some sort of tutorial web pages that provide a little bit of background
on cluster computing, but I didn't find the ones available to me, at the time, to
be particularly helpful.  

The goal of this chapter is to provide the sort of background I wish that I had
when I started doing cluster computing for bioinformatics.  I will not be providing
a comprehensive overview of parallel computation. For example, we will not focus at all
upon the rich tradition of parallel computing applications through "message passing" interfaces which
can maintain a synchronized analysis from a single program executing
on multiple computers at the same time.  Rather, we will focus on the manner in which
most bioinformatic problems can be broken down into a series of smaller jobs,
each of which can be run, independently, on its own processor without the
need for maintaining synchrony between multiple processes.  

We start with an overview of what an HPCC consists of, defining a few important
terms. Then we provide some background on the fundamental problem of cluster
computing: namely that a lot of people want to use the computing power of the cluster,
but it needs to be allocated to users in an equitable fashion.  An understanding of this
forms the basis for our discussion of _job scheduling_ and the methods you must
use to tell the _job scheduler_ what resources you will need, so that those resources
will eventually be allocated to you.  We will cover a job scheduler called SLURM, which
stands for the Simple Linux Utility for Resource Management.  It is the scheduler used
on the Summit supercomputer in Boulder and the Hummingbird and Sedna clusters deployed
at UCSC and the NWFSC, respectively.  

Interspersed with our discussion of SLURM, we will cover
methods for installing software to use for your analyses.

## An oversimplified, but useful, view of a computing cluster

At its simplest, an HPCC can be thought of as a whole lot of computers that are all
put in a room somewhere for the purposes of doing a lot of computations.  Most of these
computers do not possess all the elements that typically come to mind when you
think of a "computer."  For example, none of them are attached to monitors---each
computer resembles just the "box" part of a desktop computer.  Each of these "boxes" is called
a _node_. The node is the unit in a cluster that corresponds most closely to what you think of
as a "computer."  As is typical of most computers today, each of these nodes has some
amount of Random Access Memory (RAM) that is _only accessible by the node itself_.  RAM
is the space in memory that is used for active computation and calculation.

Each of these
nodes might also have a hard drive used for operating system software.  The bulk of the
hard drive space that each node can access, however, is in the form of a large array of hard disks
that are connected to _all_ of the nodes by an interface that allows data to be transferred
back and forth between each node and the hard-drive array at speeds that would be expected of
an internal hard drive (i.e., this array of hard drives is not just plugged in with a USB
cable). Memory on hard drives is used for storing data and the results of calculations, but
is not used for active calculations the way RAM is used.  In order for calculations to be done
on data that are on the hard drive array, it must first be read into a node's RAM.  After the calculations
are done, the results are typically written back out onto the hard drive array.

On a typical cluster, there are usually several different "portions" of the hard drive
array attached to every _node_.  One part of the array holds _home directories_ of the
different users.  Each user typically has a limited amount of storage in their home directory, but
the data in these home directories is usually safe or protected, meaning you can put a file there
and expect that it will be there next week, month, or year, etc. It is also likely that the home directories are
backed up (but check the docs for your cluster to confirm this!). Since the space in home directories is limited,
you typically will not put large data sets in your home directory for long-term storage, but you will
store scripts and programs, and such items as cloned GitHub repositories there.  Another
type of storage that exists on the hard drive array is called _persistent long-term storage_.  This type of storage
is purchased for use by research groups to store large quantities of
data on the cluster for long periods of time.  As discussed in the last chapter, the rise of cloud-based storage solutions,
like Google Drive, offering unlimited storage to institutional users, makes persistent long-term storage less
important (and less cost-effective) for many research groups.  Finally, the third type of
storage in the hard drive array is called _scratch storage_.  There are usually fairly
light limits (if any at all) to how much data can be placed in scratch storage, but there
will typically be Draconian _time limits_ placed on your scratch storage.  For example, on the
Hoffman2 cluster at UCLA, you are granted 2 Tb of `scratch` storage, but any files that
have sat unmodified in `scratch` for more than 14 days will be deleted (and if space is tight, the
system administrators may delete things from `scratch` in far fewer then 14 days.)  Check
your local cluster documentation for information about time and space limits on `scratch`.

On many clusters, scratch space is also configured to be very fast for input and output
(for example, on many systems, the scratch storage will be composed of solid state drives rather
than spinning hard disks).  On jobs that require that a lot of data be accessed from the
drive or written to it (this includes most operations on BAM files), significant decreases
in overall running time can be seen by using fast storage. Finally, scratch space exists on a cluster
expressly as _the place_ to put data and outputs on the hard drive _when running jobs_.  For all these
reasons, when you run jobs, you will always want to read and write data from and to `scratch`.  We
will talk more about the specifics of doing so, but for now, you should be developing a generic picture
of your cluster computing workflow that looks like:

1. Download big data files from the cloud to `scratch`
2. Run analyses on those data files, writing output back to `scratch`.
3. When done, copy, to the cloud, any products that you wish to keep.
4. Remove data and outputs left on `scratch`.

As is the case with virtually all modern desktop or laptop computers, within each node,
there are multiple (typically between 16 and 48) _cores_,  which are the computer chip units that actually
do the computations within a node. A _serial job_ is one that just runs on a single core
within a _node_, while a _parallel job_ might run on multiple cores, at the same time, within
a single node.  In such a parallel job, each core has access to the same data within the
node's RAM (a "shared-memory," parallel job).  The _core_
is the fundamental unit of computing machinery that gets allocated to perform jobs in an HPCC.

Most of the nodes in a cluster are there to hold the cores which are the computational workhorses,
slogging through calculations for the HPCC's myriad users.  However, some nodes
are more appropriate to certain types of computations than others (for example, some
might have lots of memory for doing genome assembly, while others will have
big, hurkin', graphical processing units to be used for GPU calculations).  Or, some nodes
might be available preferentially for different users than for others. For these
reasons, nodes are grouped into different collections.  Depending on the system you are
using, these collections of different nodes are called, either, _partitions_ or _queues_.
The world of SLURM uses the term _partitions_ for these collections, and we will adopt
that language as well, using _queue_ to refer to the line of jobs that are waiting to start
on an HPCC; however we warn the reader that other job schedulers (like the Univa Grid Engine)
use the term "queues" to refer to collections of nodes.

On every cluster, however, there will be one to several nodes that are reserved not for
doing computation, but for allowing users to access the cluster.  These are called the
_login_ nodes or the _head_ nodes.  These nodes are _solely_ for logging in, light editing of
scripts, minor manipulation of directories, and scheduling and managing jobs.  They are absolutely _not_
for doing major computations.  For example, you should never login to the head node and immediately
start using it, in an interactive `bash` session to, say, sort BAM files or run `bwa mem`
to do alignments.  Running commands that require a lot of computation, or a lot of
input and output from the disk, on the login nodes is an egregious
_faux pas_ of cluster computing.  Doing so can
negatively impact the ability of other users to login or otherwise get their work done, and it
might lead the system administrators to disable your account.  Therefore,
never do it!  All of your hardcore computation on a cluster _has_ to be done on
a _compute node_.  We will show how to do that shortly, but first we will talk about why.

## Cluster computing and the job scheduler

When you do work, or stream a video, or surf the web on your laptop computer, there are numerous
different computer processes running, to make sure that your computer keeps working and doing what it
is supposed to be doing.  In the case of your laptop, the operating system, itself, orchestrates
all these different processes, making sure that each one is given some compute time on your
laptop's processors in order to get its work done.  Your laptop's operating system has a fair bit
of flexibility in how it allocates resources to these different processes: it has multiple
cores to assign different processes to, _and_ it
allows multiple processes to run on a single core, alternating between these different processes over
different _cycles_ of the central processing unit.  Things work differently
on a shared resource like an HPCC. The main, interesting problem of cluster computing is basically
this: lots of people want to use the cluster to run big jobs, but the cluster does not
run like a single computer.

The cluster is not happy to give lots of different jobs
from lots of different users a chance to all run on the same core, sharing time by dividing up cycles.
When a user wants to use the computational resources of an HPCC,
she cannot just start a job and be confident that it will launch immediately and be granted
at least a few CPU cycles every now and again.  Rather, on an HPCC, every _job_ that is submitted
by a user will be assigned to a dedicated core (or several cores, if requested, and granted)
with a dedicated amount of memory.
If a core is not available for use, the job "gets in line" (into the "queue", so to speak)
where it sits and waits (doing nothing) until a core
(with sufficient associated resources, like RAM memory) becomes available.  When such a core
becomes available in the cluster, the job gets launched on it.  All of this is orchestrated by the job scheduler,
of which SLURM is an example.  

In this computing model, a job, once it is launched, ties up the core and the memory
that has been allocated to it until the job is finished.  While that job is running, no one
else's jobs or processes can run on the core or share the RAM memory that was allocated to the job.
For this reason, the job scheduler, needs to know, _ahead of time_ how long each job might run and
what resources will be required during that time.  A simple contrived example illustrates things easily:
imagine that Joe and Cheryl each have 1000 separate jobs to run.  Each of Cheryl's jobs involves running
a machine-learning algorithm to identify seabirds in high-resolution, aerial images of the ocean, and
takes only about 20 minutes running on a single core.  Each of Joe's jobs, on the other hand, involves mapping billions of 
sequencing reads, a task which requires about 36 hours when run on a single core.
If their cluster has only 640 cores, and Joe submits his jobs first,
then, if the job scheduler were naive, it might put all of his jobs in line first, requiring some 50 or 60 hours
before the first of Cheryl's jobs even runs. This would be a huge buzz kill for Cheryl.
However, if Cheryl and Joe both have to provide estimates to the scheduler of how
long their jobs will run, the scheduler can make more equitable decisions, starting a few of Joe's jobs, but retaining
many more cores for Cheryl's jobs, each of which runs much faster.

Thus, when you want to run any jobs on a cluster, you must provide an estimate of the resources
that the job will require.  The three main axes upon which these resources are measured are:

1. The number of cores the job will require.
2. The maximum amount of RAM (memory) the job will require.
3. The amount of time for which the job will run.  

Requests for large amounts of resources for long periods of time generally take longer to start.
There are two main reasons for this: either 1) the scheduler does not want to launch too many
long-duration, high-memory jobs because it anticipates other users will want to use resources
down the road and no single user should tie up the compute resources
for too long; or 2) there are so many jobs running on myriad nodes and cores, that only
infrequently do nodes with sufficient numbers of cores and RAM come available to start
the new jobs.  

The second reason is a particular bane of new cluster users who unwittingly request more resources
than actually exist (i.e. 52 cores, when no single node has more than 32; or 50 Gb of RAM when no single
node has more than 48 Gb).  Unfortunately (or, perhaps comically, if you have a sick sense of
humor), most job schedulers will not notify you of this sort of transgression.
Rather, your job will just sit in line waiting to be launched, but it never will be, because sufficient
resources never become available!  

It is worth noting that regardless of whether reason 1 or reason 2 is the dominant cause influencing
how long it takes to start a job, asking for fewer resources for less time will generally allow your
jobs to start faster.  Particularly because of reason #2, however, breaking your jobs down (if possible) into
small chunks that will run relatively quickly on a single core with low RAM needs can render many more
opportunities for your jobs to start, letting you tap into resources that are not often fully utilized
in a cluster.  Since I started working on large cluster in which it took a long time to start a job
that required all or most of the cores on a single node, but in which there were many nodes
harboring a few cores that were not being used, I tend to endorse this approach...

Since the requested resources for a job play such a large role in wait times for jobs to start,
you might wonder why people don't intentionally underestimate the resources they request for their
jobs.  The answer is simple:  the job scheduler is a highly efficient and completely dispassionate
manager.  If you requested 2 hours for your job, but your job has not finished in that amount of time,
the job scheduler will waste no time hemming and hawing or having an emotional struggle with itself
about whether it should stop your job.  No, at 2 hours and 0.2 seconds it WILL kill your job, regardless
of whether it is just about to finish, or not.  Similarly, if you requested 4 Gb or RAM, but five hours into
your job, the program you are running ends up using 5 Gb or RAM to store a large chunk of data, your
job WILL be killed, immediately.  

Thus, it is best to be able to accurately estimate the time and resources a job will
require. You always want to request more time and resources than your job will
actually need, but not too much more.  A large part of getting good at computing
in a shared cluster resource is gaining experience in predicting how long different jobs will
run, and how much RAM they will require. Later we will describe how the records of your
jobs, stored by the job scheduler, can be accessed and organized to aid in predicting
the resource demand of future jobs.  

## Learning about the resources on your HPCC

Most computing clusters have a web page that describes the configuration of the system
and the resources available on it.  However, you can use various SLURM commands to
learn about those resources as well, and also to gain information about which
of the resources are in use and how many users are waiting to use them.

Requests for resources, and for _information_ about the computing cluster, are made to the job scheduler
using a few basic commands.  As said, we will focus on the commands available in a SLURM-based system.
In a later section, (after a discussion of installing software that you might need on your cluster)
we will more fully cover the commands used to _launch_, _schedule_, and manage jobs.  Here
we will first explore the SLURM commands that
you can use to "get to know" your cluster.

All SLURM commands begin with an `s`, and all SLURM systems support the `sinfo` command
that gives you information about the cluster's nodes and their status (whether they are currently
running jobs or not.)  On your cluster, `man sinfo` will tell you about this command.
On the `Sedna` cluster, which just got installed and therefore does not
have many users, we see:
```sh
% sinfo
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
nodes*       up   infinite     28   idle node[01-28]
himem        up   infinite      1  alloc himem01
himem        up   infinite      2   idle himem[02-03]
```
which tells us that there are two _partitions_ (collection of nodes)
named `nodes` and `himem`.  The `himem` partition has one node which is
currently allocated to a job (`STATE` = `alloc`), and two more that
are not currently allocated to jobs. The `himem` partition holds machines
with lots of RAM memory for tasks like sequence assembly (and, when I ran that
command, one of the nodes in `himem` was busy assembling the genome of
some interesting sea creature [need to ask Krista again what the hell it was...].
It also has 28 compute nodes in the 
`nodes` partition that are free.  This is a very small cluster.

If you do the same command on SUMMIT you get many more lines of output.  There
are many more different partitions, and there is a lot of information about how many
nodes are in each:
```sh
% sinfo
PARTITION        AVAIL  TIMELIMIT  NODES  STATE NODELIST
shas*               up 1-00:00:00      2 drain* shas[0136-0137]
shas*               up 1-00:00:00      2  down* shas[0404,0506]
shas*               up 1-00:00:00      1  drain shas0101
shas*               up 1-00:00:00      3   resv shas[0102,0521,0853]
shas*               up 1-00:00:00     74    mix shas[0125,0130,0133,0138,0141,0149,0156,0158,0218,0222,0229,0236-0237,0240-0241,0243,0246,0255,0303,0311,0314,0322,0335,0337,0341,0343,0351,0357,0402,0411,0413,0415-0416,0418,0423,0428,0432-0433,0435-0436,0440,0452,0455-0456,0459,0501,0504,0514,0522-0524,0526-0527,0556,0608,0611-0613,0615-0616,0631,0637,0801,0810-0811,0815,0834-0835,0850,0855,0907-0909,0921]
shas*               up 1-00:00:00    359  alloc shas[0103-0124,0126-0129,0131-0132,0134-0135,0139-0140,0142-0148,0150-0155,0157,0159-0160,0201-0217,0219-0221,0223-0225,0227-0228,0230-0235,0238-0239,0242,0247-0254,0256-0260,0301-0302,0304-0310,0312-0313,0315-0321,0323-0334,0336,0338-0340,0342,0344-0350,0352-0356,0358-0360,0401,0405-0410,0412,0414,0417,0419-0422,0424-0427,0429-0431,0434,0437-0439,0442-0451,0453-0454,0457-0458,0460,0502-0503,0505,0507-0513,0515-0520,0525,0528-0555,0560,0605-0607,0609,0614,0617-0630,0632-0636,0638-0650,0652-0664,0802-0809,0812-0814,0816-0833,0836-0849,0851-0852,0854,0856-0860,0901-0906,0910-0913,0915-0920,0922-0932]
shas*               up 1-00:00:00     11   idle shas[0226,0244-0245,0403,0441,0557-0559,0610,0651,0914]
shas-testing        up   infinite      2 drain* shas[0136-0137]
shas-testing        up   infinite      2  down* shas[0404,0506]
shas-testing        up   infinite      1  drain shas0101
shas-testing        up   infinite      3   resv shas[0102,0521,0853]
shas-testing        up   infinite     74    mix shas[0125,0130,0133,0138,0141,0149,0156,0158,0218,0222,0229,0236-0237,0240-0241,0243,0246,0255,0303,0311,0314,0322,0335,0337,0341,0343,0351,0357,0402,0411,0413,0415-0416,0418,0423,0428,0432-0433,0435-0436,0440,0452,0455-0456,0459,0501,0504,0514,0522-0524,0526-0527,0556,0608,0611-0613,0615-0616,0631,0637,0801,0810-0811,0815,0834-0835,0850,0855,0907-0909,0921]
shas-testing        up   infinite    359  alloc shas[0103-0124,0126-0129,0131-0132,0134-0135,0139-0140,0142-0148,0150-0155,0157,0159-0160,0201-0217,0219-0221,0223-0225,0227-0228,0230-0235,0238-0239,0242,0247-0254,0256-0260,0301-0302,0304-0310,0312-0313,0315-0321,0323-0334,0336,0338-0340,0342,0344-0350,0352-0356,0358-0360,0401,0405-0410,0412,0414,0417,0419-0422,0424-0427,0429-0431,0434,0437-0439,0442-0451,0453-0454,0457-0458,0460,0502-0503,0505,0507-0513,0515-0520,0525,0528-0555,0560,0605-0607,0609,0614,0617-0630,0632-0636,0638-0650,0652-0664,0802-0809,0812-0814,0816-0833,0836-0849,0851-0852,0854,0856-0860,0901-0906,0910-0913,0915-0920,0922-0932]
shas-testing        up   infinite     11   idle shas[0226,0244-0245,0403,0441,0557-0559,0610,0651,0914]
shas-interactive    up   infinite      2 drain* shas[0136-0137]
shas-interactive    up   infinite      2  down* shas[0404,0506]
shas-interactive    up   infinite      1  drain shas0101
shas-interactive    up   infinite      3   resv shas[0102,0521,0853]
shas-interactive    up   infinite     74    mix shas[0125,0130,0133,0138,0141,0149,0156,0158,0218,0222,0229,0236-0237,0240-0241,0243,0246,0255,0303,0311,0314,0322,0335,0337,0341,0343,0351,0357,0402,0411,0413,0415-0416,0418,0423,0428,0432-0433,0435-0436,0440,0452,0455-0456,0459,0501,0504,0514,0522-0524,0526-0527,0556,0608,0611-0613,0615-0616,0631,0637,0801,0810-0811,0815,0834-0835,0850,0855,0907-0909,0921]
shas-interactive    up   infinite    359  alloc shas[0103-0124,0126-0129,0131-0132,0134-0135,0139-0140,0142-0148,0150-0155,0157,0159-0160,0201-0217,0219-0221,0223-0225,0227-0228,0230-0235,0238-0239,0242,0247-0254,0256-0260,0301-0302,0304-0310,0312-0313,0315-0321,0323-0334,0336,0338-0340,0342,0344-0350,0352-0356,0358-0360,0401,0405-0410,0412,0414,0417,0419-0422,0424-0427,0429-0431,0434,0437-0439,0442-0451,0453-0454,0457-0458,0460,0502-0503,0505,0507-0513,0515-0520,0525,0528-0555,0560,0605-0607,0609,0614,0617-0630,0632-0636,0638-0650,0652-0664,0802-0809,0812-0814,0816-0833,0836-0849,0851-0852,0854,0856-0860,0901-0906,0910-0913,0915-0920,0922-0932]
shas-interactive    up   infinite     11   idle shas[0226,0244-0245,0403,0441,0557-0559,0610,0651,0914]
sgpu                up 1-00:00:00      1   resv sgpu0501
sgpu                up 1-00:00:00      1  alloc sgpu0502
sgpu                up 1-00:00:00      9   idle sgpu[0101-0102,0201-0202,0301-0302,0401-0402,0801]
sgpu-testing        up   infinite      1   resv sgpu0501
sgpu-testing        up   infinite      1  alloc sgpu0502
sgpu-testing        up   infinite      9   idle sgpu[0101-0102,0201-0202,0301-0302,0401-0402,0801]
sknl                up 1-00:00:00      1  drain sknl0710
sknl                up 1-00:00:00      1   resv sknl0706
sknl                up 1-00:00:00     18  alloc sknl[0701-0705,0707-0709,0711-0720]
sknl-testing        up   infinite      1  drain sknl0710
sknl-testing        up   infinite      1   resv sknl0706
sknl-testing        up   infinite     18  alloc sknl[0701-0705,0707-0709,0711-0720]
smem                up 7-00:00:00      1   drng smem0201
smem                up 7-00:00:00      4  alloc smem[0101,0301,0401,0501]
ssky                up 1-00:00:00      1   drng ssky0944
ssky                up 1-00:00:00      1    mix ssky0952
ssky                up 1-00:00:00      3  alloc ssky[0942-0943,0951]
ssky-preemptable    up 1-00:00:00      1   drng ssky0944
ssky-preemptable    up 1-00:00:00      1    mix ssky0952
ssky-preemptable    up 1-00:00:00      9  alloc ssky[0933-0934,0937-0940,0942-0943,0951]
ssky-preemptable    up 1-00:00:00      9   idle ssky[0935-0936,0941,0945-0950]
ssky-ucb-aos        up 7-00:00:00      6  alloc ssky[0933-0934,0937-0940]
ssky-ucb-aos        up 7-00:00:00      3   idle ssky[0935-0936,0941]
ssky-csu-mbp        up 7-00:00:00      1   idle ssky0945
ssky-csu-asb        up 7-00:00:00      1   idle ssky0946
ssky-csu-rsp        up 7-00:00:00      4   idle ssky[0947-0950]
```

Yikes!  That is a lot of info.  The numbers that you see on the ends of the lines there
are node numbers.

If you wanted to see information about each node in the `shas` partition, you could
print long information for each node  like this:
```sh
% sinfo -l -N
```
On Summit, that creates almost 1500 lines of output.  Some nodes are listed several times
because they belong to different partitions.  To look at results for just the standard compute
parition (`shas`: 380 nodes with Intel Xeon Haswell processors) you can use
```sh
% sinfo -l -n -p shas
```
Try that command.  (Or try a similar command with an appropriate partition name on your
own cluster.)  If you want to see explicitly how many cores are available vs allocated
on each node, how much total memory each node has, and how much of that total
memory is free, in that partition you can do:
```sh
% sinfo -N -p shas -O nodelist,cpusstate,memory,allocmem,freemem
```
The top part of the output from that command on SUMMIT looks like:
```
NODELIST            CPUS(A/I/O/T)       MEMORY              ALLOCMEM            FREE_MEM            
shas0101            0/0/24/24           116368              0                   125156              
shas0102            0/24/0/24           116368              0                   112325              
shas0103            24/0/0/24           116368              116352              87228               
shas0104            24/0/0/24           116368              116352              80769  
```
This says `shas0101` has 24 CPUs that are _Out_ (not functional at this point). 
`shas0102`, on the other hand, has 24 CPUs that are _Idle_, while `shas0103` has
24 CPUs that are allocated, and so forth. (In our parlance, here, 
CPU is being used to mean "core").  All of the nodes have
116 Gb of memory total. Most of them have about that much memory allocated to
the jobs they are running.  

We can throw down some `awk` to count the total number of available cores
(and the total number of all the cores):
```sh
% sinfo -N -p shas -O  nodelist,cpusstate | awk -F"/" '{avail+=$2; tots+=$4} END {print "Out of", tots, "cores, there are", avail, "available"}' 
Out of 10848 cores, there are 331 available
```
That could explain why it can be hard to get time on the supercomputer: at this time, only about 3% of the cores in the system
are idle, waiting for jobs to go on them.

If you want to see how many jobs are in line, waiting to be launched, you can use the
`squeue` command.  Simply issuing the command `squeue` will give a (typically very long)
list of all jobs that are either currently running or are in the queue,
waiting to be launched. This is worth doing in order to see how many 
different people are using (or waiting to use) the resources.  

If we run the command on Sedna, we see that (like we saw before)
only one job is currently running:
```sh
% squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
               160     himem MaSuRCA_   ggoetz  R 34-15:56:16      1 himem01
```
Holy cow! Looking at the TIME column you can see that the genome assembly job
has been running for over 34 days!  (The time format is DAYS-Hours:Minutes:Seconds).



The output can be filtered to just PENDING
or RUNNING jobs using the `-t` option. So,
```sh
squeue -t PENDING
```
lists all jobs waiting to be launched.  If you wanted to see jobs that a particular
user (like yourself) has running or pending, you can use the `-u` option. For example,
```sh
squeue -u eriq@colostate.edu
```
In fact that is such a worthwhile command that it is worth creating an alias
for it by adding a line such as the following to your `~/.bashrc`:
```sh
alias myjobs='squeue -u eriq@colostate.edu'
```
Then, typing `myjobs` shows all of the jobs that you have running, or waiting to be launched
on the cluster.






## Installing Software on an HPCC

In order to do anything useful on a cluster you will need to have software programs
for analyzing data. As we have seen, a lot of the nuts and bolts of writing command
lines uses utilities that are found on every Unix computer.  However almost always
your bioinformatic analyses will require programs or software that
do not come "standard" with Unix. For example, the specialized programs for
sequence assembly and alignment will have to be _installed_ on the cluster
in order for you to be able to use them.  

It turns out that installing software on a Unix machine (or cluster) has not
always been a particularly easy thing to do for a number or reasons.  First, for
a long time, Unix software was largely distributed in the form of _source code_: the
actual programming code (text) written by the developers that describes the actions
that a program takes. Such computer code cannot be run directly, it first must be
_compiled_ into a _binary_ or _executable_ program.  Doing this can be a challenging
process. First, computer code compilation can be very time consuming (if you use R
on Linux, and install all your packages from CRAN---which requires compilation---you
will know that!). Secondly, vexing errors and failures can occur when the
compiler or the computer architecture is in conflict with the program code.  (I have lost
entire days trying to solve compiling problems).  On top of that, in order to run, most
programs do not operate in a standalone fashion; rather, while a program is running, it
typically depends on computer code and routines that must be stored in separate _libraries_ on your
Unix computer. These libraries are known as program _dependencies_.  So, installing a program
requires not just installing the program itself, but also ensuring that the program's dependencies
are installed _and_ that the program knows where they are installed.  As if that were not enough,
the dependencies of some programs can conflict (be incompatible) with the dependencies of other
programs, and particular versions of a program might require particular versions of
the dependencies.  Additionally, some versions of some programs might not work with particular
versions (types of chips) of some computer systems.  Finally, most systems for installing
software that were in place on Unix
machines a decade ago required that whoever was installing software have _administrative privileges_
on the computer.  On an HPCC, none of the typical users have administrative privileges which are,
as you might guess, reserved for the system administrators.  

For all the reasons above, installing software on an HPCC used to be a harrowing
affair: you either had to be fluent in compilers and libraries to do it yourself in your
home directory or you had to beg your system administrator.  (Though our cluster computing
sysadmins at NMFS are wonderful, that is not always the case...[see Dilbert](https://www.wiw.org/~chris/archive/dilbert/)).
On HPCC's the system administrators have to contend with requests from multiple users
for different software and different versions. They solve this (somewhat headachey)
problem by installing software into separate "compartments" that allow different software
and versions to be maintained on the system without all of it being accessible at once.
Doing so, they create _modules_ of software.  This is discussed in a few more sections.  

Today, however, a large group of motivated people have created a software management
system called Miniconda that tries to solve many of the problems encountered in maintaining
software on a computer system.  First, Miniconda maintains a huge repository of
programs that are already _pre-compiled_ for a number of different chip architectures, so that
programs can usually be installed without the time-consuming compiling process.  Second, the repository
maintains critical information on the dependencies for each software program, and about
conflicts and incompatibilities between different versions of programs, architectures and
dependencies.  Third, the Miniconda system is built from the ground up to make it easy to maintain
separate software _environments_ on your system.  These different environments have different
software programs or different versions of different software programs.  Such an approach
was originally used so developers could use a single computer to test any new code
they had written in a number of different
computing environments; however, it has become an incredibly valuable tool for ensuring
that your analyses are reproducible: you can give people not just the data and scripts that you
used for the analysis, but also the computing/software environment (with all the same
software versions) that you used for the analysis.  And, finally, all of this
can be done with Miniconda without have administrative privileges.  Effectively,
Miniconda manages all these software programs and dependencies _within your home directory_.

### Miniconda {#miniconda}

We will first walk you through a few steps with Miniconda to install some
bioinformatic software into an environment on your cluster.  After that we will discuss more about
the underlying philosophy of Miniconda, and how it is operating.

#### Installing or updating Miniconda

1. To do installation of software, you probably should not be on SUMMIT's login nodes.
They offer "compile nodes" that should be suitable for installing with Miniconda. So, do this:
    ```{sh, eval=FALSE}
    ssh scompile
    ```
1. If you are on Hummingbird, be sure to get a `bash` shell before doing anything else, by typing `bash`
(if you have not already set `bash` as your default shell (see the previous chapter)).

1. First, check if you have Miniconda. Update it if you do, and install it if you don't:
    ```{sh, eval=FALSE}
    # just type conda at the command line:
    conda
    ```
If you see some help information, then you already have Miniconda (or Anaconda) and you
should merely update it with:
    ```{sh, eval=FALSE}
    conda update conda
    ```
If you get an error telling you that your computer does not know about a command, `conda`, then
you do not have Miniconda and you must install it.  You do that by downloading the Miniconda package
with `wget` and then running the Miniconda installer, like this:
    ```{sh, eval=FALSE}
    # start in your home directory and do the following:
    mkdir conda_install
    cd conda_install/
    wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
    chmod u+x Miniconda3-latest-Linux-x86_64.sh 
    ./Miniconda3-latest-Linux-x86_64.sh 
    ```
That launches the Miniconda installer (it is a shell script). Follow the prompts and agree
to the license and to the default install location and to initialize conda.
At the end, it tells you to log out of your shell and log back in for changes to take
effect.  It turns out that it suffices to do `cd ~; source .bash_profile`

Once you complete the above, your command prompt should have been changed to something that looks like:
```sh
(base) [~]--%
```
The `(base)` is telling you that you are in Miniconda's _base_ environment.  Typically you want to keep the
_base_ environment clean of installed software, so we will install software into a new environment.

#### Installing software into a bioinformatics environment

If everything went according to plan above, then we are ready to use Miniconda to
install some software for bioinformatics.  We will install a few programs that we will
use extensively in the next few weeks: `bwa`, `samtools`, and `bcftools`.  We will
install these programs into a conda environment that we will name `bioinf` (short
for "bioinformatics").  It takes just a single command:
```sh
conda create -n bioinf -c bioconda bwa samtools bcftools
```
That should only take a few seconds.

To test that we got the programs we must _activate_ the `bioinf` environment, and then issue
the commands, `bwa`, `samtools`, and `bcftools`.  Each of those should spit back some help
information.  If so, that means they are installed correctly!  It looks like this:
```sh
conda activate bioinf
```
After that you should get a command prompt that starts with `(bioinf)`, telling you that the
active conda environment is `bioinf`.  Now, try these commands:
```sh
bwa
samtools
bcftools
```

#### Uninstalling Miniconda and its associated environments

It may become necessary at some point to uninstall Miniconda.  One important case of this
is if you end up overflowing your home directory with conda-installed software.  In this case,
unless you have installed numerous, complex environments, the simplest thing to do is
to "uninstall" Miniconda, reinstall it in a location with fewer hard-drive space constraints,
and then simply recreate the environments you need, as you did originally.  

This is actually quite germane to SUMMIT users.  The size quota on home directories on SUMMIT is
only 2 Gb, so you can easily fill up your home directory by installing a few conda environments.
To check how much of the hard
drive space allocated to you is in use on SUMMIT, use the `curc-quota` command. (Check the documentation
for how to check space on other HPCCs, but note that Hummingbird users get 1 TB on their home
directories).  Instead of
using your home directory to house your Miniconda software, on SUMMIT you can put it in your
`projects` storage area.  Each user gets more storage (like 250 Gb) in a directory
called `/projects/username` where `username` is replaced by your SUMMIT username,
for example: `/projects/eriq@colostate.edu`

To "uninstall" Miniconda, you first must delete the `miniconda3` directory in your
home directory (if that is where it got installed to).  This can take a while.  It is done with:
```sh
rm -rf ~/miniconda3
```
Then you have to delete the lines between `# >>>` and `# <<<`, wherever they occur in your `~/.bashrc` and `~/bash_profile`
files, i.e., you will have to remove all of the lines that look
something like thius:
```sh
# >>> conda initialize >>>
# !! Contents within this block are managed by 'conda init' !!
__conda_setup="$('/Users/eriq/miniconda3/bin/conda' 'shell.bash' 'hook' 2> /dev/null)"
if [ $? -eq 0 ]; then
    eval "$__conda_setup"
else
    if [ -f "/Users/eriq/miniconda3/etc/profile.d/conda.sh" ]; then
        . "/Users/eriq/miniconda3/etc/profile.d/conda.sh"
    else
        export PATH="/Users/eriq/miniconda3/bin:$PATH"
    fi
fi
unset __conda_setup
# <<< conda initialize <<<
```
After all those conda lines are removed from your `~/.bashrc` and `~/bash_profile`, logging out
and logging back in, you should be free from conda and ready to reinstall it in
a different location.  

To reinstall miniconda in a different location, just follow the
installation instructions above, but when you are running the
`./Miniconda3-latest-Linux-x86_64.sh` script, instead of choosing the default
install location, use a location in your project directory. For example, for me, that is:
`/projects/eriq@colostate.edu/miniconda3`.

Then, recreate the `bioinf` environment described above.

If you are having fun making environments and you think that you might like to use R on
the cluster, then you might want to make an environment with some bioinformatics software
that also has
the latest version of R on miniconda installed.  At the time of writing that was R 3.6.1. So,
do:
```sh
conda create -n binfr -c bioconda bwa samtools bcftools r-base=3.6.1 r-essentials
```
That makes an environment called `binfr` (which turns out to also be **way** easier to type that `bioinfr`).
The `r-essentials` in the above command line is the name for a collection of 200 commonly used R packages (including
the `tidyverse`).  This procedure takes a little while, but it is still far less painful than using the
version of R that is installed on SUMMIT with the `modules` packages, and then trying to build the tidyverse
from source with `install.packages()`.

#### What is Miniconda doing?

This is a good question.  We won't go deeply into the specifics, but will skim the
surface of a few topics that can help you understand what Miniconda is doing.

First, Miniconda is downloading programs and their dependencies into the `miniconda3`
directory. Based on the lists of dependencies and conflicts for each program that is being
installed, it makes a sort of "equation," which it can "solve" to find the versions of
different programs and libraries that can be installed and which should "play nicely with
one another (and with your specific computer architecture." While it is solving this
"equation" it is doing so while also doing its best
to optimize features of the programs (like using the latest versions, if possible).
Solving this "equation" is an example of a Boolean Satisfiability problem, which is a known
class of difficult (time-consuming) problems.  If you are requesting a lot of programs, and
especially if you do not constrain your request (by demanding a certain version of
the program) then "solving" the request may take a long time.  However, when installing
just a few bioinformatics programs it is unlikely to ever take too terribly long.

Once miniconda has decided on which versions of which programs and dependencies to install,
it downloads them and then places them into the requested environment (or the active environment
if no environment is specifically requested).  If a program is installed into an environment,  then you
can access that program by activating the environment (i.e. `conda activate bioinf`).  Importantly,
if you don't activate the environment, you won't be able to use the programs installed there.
We will see later in writing bioinformatic scripts, you will always have to explicitly
activate a desired conda environment when you run a script on a compute node through the job
scheduler.

The way that Miniconda delivers programs in an environment is by storing all the programs
in a special environment directory (within the `miniconda3/envs` directory), and then, when
the environment is activated, the main thing that is happening is that `conda` is manipulating your
PATH variable to include directories within the environment's directory
within the `miniconda3/envs` directory.  An easy way to see this is simply by
inspecting your path variable while in different environments.  Here we compare the PATH
variable in the `base` environment, versus in the `bioinf` environment, versus in the
`binfr` environment:
```sh
(base) [~]--% echo $PATH
/projects/eriq@colostate.edu/miniconda3/bin:/projects/eriq@colostate.edu/miniconda3/condabin:/usr/local/bin:/bin:/usr/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/eriq@colostate.edu/bin:/home/eriq@colostate.edu/bin
(base) [~]--% conda activate bioinf
(bioinf) [~]--% echo $PATH
/projects/eriq@colostate.edu/miniconda3/envs/bioinf/bin:/projects/eriq@colostate.edu/miniconda3/condabin:/usr/local/bin:/bin:/usr/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/eriq@colostate.edu/bin:/home/eriq@colostate.edu/bin
(bioinf) [~]--% conda activate binfr
(binfr) [~]--% echo $PATH
/projects/eriq@colostate.edu/miniconda3/envs/binfr/bin:/projects/eriq@colostate.edu/miniconda3/condabin:/usr/local/bin:/bin:/usr/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/eriq@colostate.edu/bin:/home/eriq@colostate.edu/bin
```
(To be sure, `miniconda` can change a few more things than just your PATH variable when you activate an environment, but
for the typical user, the changes to PATH are most important.)

#### What programs are available on Minconda?

There are quite a few programs for multiple platforms.  If you are wondering
whether a particular program is available from Miniconda, the easiest first
step is to Google it.  For example, search for `miniconda bowtie`.

You can also search from the command line using `conda search`.  Note that most
bioinformatics programs you will be interested in are available on a conda
_channel_ called `bioconda`.  You probably saw the `-c bioconda` option
applied to the `conda create` commands above. That options tells conda to search
the Bioconda channel for programs and packages.

Here, you can try searching for a couple of packages that you might
end up using to analyze genomic data:
```sh
conda search -c bioconda plink

# and next:

conda search -c bioconda angsd
```


#### Can I add more programs to an environment?

This is a worthwhile question.  Imagine that we have been happily working in our `bioinf` conda environment
for a few months.  We have finished all our tasks with `bwa`, `samtools`, and `bcftools`, but perhaps now we
want to analyze some of the data with `angsd` or `plink`.  Can we add those programs to our
`bioinf` environment?  The short answer is "Yes!".  The steps are easy.

1. Activate the environment you wish to add the programs to (i.e. `conda activate bioinf` for example).
2. Then use `conda install`.  For example to install specific versions of `plink` and `angsd` that we
saw above while searching for those packages we might do:
    ```{sh, eval=FALSE}
    conda install -c bioconda plink=1.90b6.12 angsd=0.931
    ```

Now, the longer answer is "Yes, but..."   The big "but" there occurs because if different
programs require the same dependencies, but rely on different versions of the dependencies,
installing programs over different commands can cause miniconda to not identify some
incompatibilities between program dependencies.  A germane example occurs if you first install
`samtools` into an environment, and then, after that, you install `bcftools`, like this:
```sh
conda create -n samtools-first        # create an empty environment
conda activate samtools-first         # activate the environment
conda install -c bioconda samtools    # install samtools
conda install -c bioconda bcftools    # install bcftools
bcftools                              # try running bcftools
```
When you try running the last line, `bcftools` barfs on you like so:
```sh
bcftools: error while loading shared libraries: libcrypto.so.1.0.0: cannot open shared object file: No such file or directory
```
So, often, installing extra programs does not create problems, but it can.  If you find yourself battling
errors from conda-installed programs, see if you can correct that by creating a new environment and installing all the programs
you want at the same time, in one fell swoop, using `conda create`, as in:
```sh
conda create -n binfr -c bioconda bwa samtools bcftools r-base=3.6.1 r-essentials
```


#### Exporting environments

In our introduction to Miniconda, we mentioned that it is a great boon to
reproducibility.  Clearly, your analyses will be more reproducible if it is
easier for others to install software to repeat your analyses.  However, Miniconda
takes that one step further, allowing you to generate a list of
the specific versions of
all software and dependencies in a conda environment.  This list is a complete
record of your environment, and, supplied to conda, it is a specification of
exactly how to recreate that environment. 

The process of creating such a list is called _exporting_ the conda
environment.  Here we demonstrate its use by exporting the `bioinf`
environment from SUMMIT to a simple text file.  Then we use that text file
to recreate the environment on my laptop.
```sh
# on summit:
conda activate bioinf          # activate the environment
conda env export               # export the environment
```
The last command above just sends the exported environment
to stdout, looking like this:
```
name: bioinf
channels:
  - bioconda
  - defaults
dependencies:
  - _libgcc_mutex=0.1=main
  - bcftools=1.9=ha228f0b_4
  - bwa=0.7.17=hed695b0_7
  - bzip2=1.0.8=h7b6447c_0
  - ca-certificates=2020.1.1=0
  - curl=7.68.0=hbc83047_0
  - htslib=1.9=ha228f0b_7
  - krb5=1.17.1=h173b8e3_0
  - libcurl=7.68.0=h20c2e04_0
  - libdeflate=1.0=h14c3975_1
  - libedit=3.1.20181209=hc058e9b_0
  - libgcc-ng=9.1.0=hdf63c60_0
  - libssh2=1.8.2=h1ba5d50_0
  - libstdcxx-ng=9.1.0=hdf63c60_0
  - ncurses=6.1=he6710b0_1
  - openssl=1.1.1d=h7b6447c_4
  - perl=5.26.2=h14c3975_0
  - samtools=1.9=h10a08f8_12
  - tk=8.6.8=hbc83047_0
  - xz=5.2.4=h14c3975_4
  - zlib=1.2.11=h7b6447c_3
prefix: /projects/eriq@colostate.edu/miniconda3/envs/bioinf
```
The format of this information is YAML (Yet Another Markup Language),
(which we saw in the headers of RMarkdown documents, too).

If we stored that output in a file:
```sh
conda env export > bioinf.yml
```
And then copied that file to another computer, then we can recreate
the environment on that other computer with:
```sh
conda env create -f bioinf.yml
```
That should work fine if the new computer is of the same architecture (i.e., both
are Linux computers, or both are Macs).  However, the specific build numbers
referenced in the YAML (i.e. things like the `h7b6447c_3` part of the program name)
can create problems when installing on other architectures. In that case, we must
export without the build names:
```sh
conda env export --no-builds > bioinf.yml
```
Even that might fail if the dependencies differ on different architectures,
in which case you can export just the list of the actual programs that
you reqested be installed, by using the `--from-history` option.  For example:
```sh
% conda env export --from-history
name: bioinf
channels:
  - defaults
dependencies:
  - bwa
  - bcftools
  - samtools
prefix: /projects/eriq@colostate.edu/miniconda3/envs/bioinf
```
Though, even that fails, cuz it doesn't list bioconda in there.

### Modules 

This is if your sys admin has made it easy.

The big point I must make here is that these don't work like conda environments.
Environments are "all-or-nothing," whereas modules are "cumulative" and can be
layered atop one another.  (If doing so creates conflicts, then the sysadmins have to
figure that out...perhaps this is one reason that modules will often carry older (if not
completely antiquated) versions of software.)




## Getting compute resources allocated to your jobs on an HPCC

### Interactive sessions

Although most of your serious computing on the cluster will occur in _batch_ jobs
that get submitted to the scheduler and run without any further interaction from you, the user,
we will begin our foray into the SLURM job scheduling commands by getting an _interactive shell_
on a compute node.  "Why?" you might ask?  Well, before you launch any newly-written
script as a _batch_ job
on the cluster, you really should run through the code inside that script and ensure that
things work as they should.  Since your script likely does some heavy computing, you can't do this on the 
head nodes (i.e., login nodes) of the cluster.

One standard way to get a shell on a compute node of a cluster is to request it with:
```sh
srun --pty /bin/bash
```
This tells slurm to run `bash` (which is typically found at `/bin/bash` on all Unix system) within
a pseodo-terminal (the `--pty` part). If you run this command you will be given a bash shell on a compute
node (usually). 

However, that doesn't work on all systems.  It does on sedna, but you should always check the webpage for your
HPCC to find out what is used on your HPCC.  For example on SUMMIT, there is
a different way of getting an interactive shell that is recommended.  The
sysadmins have written their own script to do it, called `sinteractive`.
So, you could do:
```sh
sinteractive
``` 

On Hummingbird it takes a few more steps.  The webpage at [https://www.hb.ucsc.edu/getting-started/](https://www.hb.ucsc.edu/getting-started/)
tells you how to do that.  Go to that page and search for `Job Allocation – Interactive, Serial`.  That takes you to
the instructions.  Minimally, here is what it looks like to get one core
on a node on the Instruction partition, for 1 hour, with 1 Gigabyte of memory
allocated to you:
```sh
[~]--% salloc --partition=Instruction --nodes=1 --time=01:00:00 --mem=1G --cpus-per-task=1
salloc: Granted job allocation 48130
[~]--% ssh $SLURM_NODELIST

# check that you are on the right host.
[~]--% hostname
hbcomp-005.hbhpc.ucsc.edu
```


Check which node you are on, by following up with:
```sh
hostname
```

### An in-class exercise to make sure everything is configured correctly

Here, we run through a number of steps to start aligning sequence data to a reference
genome.  The main purpose of this exercise is to ensure that everyone has their systems
configured correctly.  Namely, we want to make sure that:

* You can log in to your cluster (SUMMIT, Hummingbird, etc.)
* You can use a few SLURM (job scheduler) commands.
* You can find your way to your _scratch_ space on SUMMIT (or Hummingbird). _scratch_ is fast, largely
unlimited short-term storage.  It is where you will do the majority of your bioinformatics.
* You can use `rclone` to tranfer files from Google Drive to your cluster. Doing things this way will allow
you to access your Lab's Team Drive.  The model for bioinformatic work is: 
    a. Copy data from your Lab's Google Team Drive to _scratch_ on the cluster.
    b. Do analyses of the data (in _scratch_)
    c. Copy the results back to your Lab's Google Team Drive before it gets deleted (by the system administrators)
    off of _scratch_
Note: today you are not using your lab's Team drive, you are using the "class" shared drive,
and you won't be copying anything back to it.  But this is a start at least...
* You have Miniconda installed, that you have a conda environment with bioinformatic
tools in it, and that you can activate that environment and use the tools within it.
Miniconda makes it easy to install software that you need to run analyses.

Being able to successfully do all these things will be necessary to complete
the next homework assignment which involves aligning short reads to a reference
genome and will be assigned after spring break.



1. If you are not already in a tmux session, start a new `tmux` session called `inclass` on the login node.
    ```{sh, eval=FALSE}
    tmux new -s inclass
    ```
1. Get a bash shell on a compute node
    ```{sh, eval=FALSE}
    # on summit:
    sinteractive
    
    # on hummingbird
    salloc --partition=Instruction --nodes=1 --time=02:00:00 --mem=4G --cpus-per-task=1
    ssh $SLURM_NODELIST
    
    # otherwise, you can try this (will work on Sedna)
    srun --pty /bin/bash
    ```
2. Check what host you are on with `hostname`
    ```{sh, eval=FALSE}
    hostname
    
    # The hostname should be something like shasXXX. 
    #  Do not proceed if you are on a login node still (ie. hostname is loginXX)
    ```
    If you are still on a login node, try running `sinteractive` again.
    
    
3. Use your alias `myjobs` (if you have created it) to see what jobs you have running.
    ```{sh, eval=FALSE}
    myjobs
    
    # or, if you don't have that aliased:
    squeue -u your_username
    
    # You should see that you have one job running.  That "job" is a bash shell
    # that you are interactively working with.
    ```
4. Navigate to your scratch directory using `cd`.  On SUMMIT that will look like this:
    ```{sh, eval=FALSE}
    cd /scratch/summit/wcfunk\@colostate.edu/
    ```
    You can make a symbolic link to that in your home directory like this:
    ```{sh, eval=FALSE}
    cd  # returns you to hour home directory
    
    # this line makes a symbolic link called scratch in your home directory
    # that points to your scratch space at /scratch/summit/wcfunk\@colostate.edu/
    ln -s /scratch/summit/wcfunk\@colostate.edu/ scratch
    ```
    Just be sure to use your own user name instead of wcfunk.  Once that symbolic link is set up, you can
    `cd` into it, just like it was your scratch directory:
    ```{sh, eval=FALSE}
    cd scratch
    ```
    But if you don't want to set up a symbolic link just use the full path:
    ```{sh, eval=FALSE}
    cd /scratch/summit/wcfunk\@colostate.edu/
    ```
    On hummingbird you go to
    ```{sh, eval=FALSE}
    cd /hb/scratch/username/
    ```
    where username is your ucsc username.  Note that if you want to make a symbolic link to that, then
    _while in your home directory_ do this:
    ```{sh, eval=FALSE}
    ln -s /hb/scratch/username scratch
    ```
4. Within your account's scratch space, make a directory called `chinook-play` and `cd` into it:
    ```{sh, eval=FALSE}
    mkdir chinook-play
    cd chinook-play/
    ```
5. Make sure you have followed the link in your email to the shared google drive folder I sent you last night. If the email
went to an email address that is not associated with your rclone configuration for google drive, then request access
(at google drive) for that email. (Or setup a new rclone config for your other gmail address...)
6. Now, use rclone to list the files in the folder I shared with you:
    ```{sh, eval=FALSE}
    rclone lsd --drive-shared-with-me gdrive-pers:CSU-con-gen-2020
    ```
    Note that `gdrive-pers` above is the name of my rclone configuration associated with the gmail account where
    I got an email (from myself at my CSU account) about the shared folder.  You will have to change `gdrive-pers`
    to be appropriate to your config, depending on how you set it up. Note also how you access folders shared with
    you using `--drive-shared-with-me`. 
7. After you get the above command to work, you should see something like the following, though it
will change as more things get added to this directory...
    ```{sh, eval=FALSE}
    -1 2020-03-09 17:51:06        -1 chr32-160-chinook
    -1 2020-03-05 07:33:21        -1 pre-indexed-chinook-genome
    ```
8. Use rclone to download `big-fastq-play.zip` (about 170 MB).  Once again you have to change
the name of the rclone remoate (`gdrive-pers` in this case), to the name of your Google drive
rclone remote.
    ```{sh, eval=FALSE}
    rclone copy -P  --drive-shared-with-me gdrive-pers:CSU-con-gen-2020/big-fastq-play.zip ./
    ```
9. Unzip the file you just downloaded.  Note: on Hummingbird, `unzip` is not installed by default.
So I recommend doing `conda install unzip` to add it to your base Miniconda environment.
    ```{sh, eval=FALSE}
    unzip big-fastq-play.zip
    ```
    This creates a directory called `big-fastq-play` within your `chinook-play` directory.
9. Create a directory called `fastq` and move the two paired-end FASTQ files from `big-fastq-play` into it:
    ```{sh, eval=FALSE}
    mkdir fastq 
    mv big-fastq-play/data/Battle_Creek_01_chinook_R* fastq/
    # then make sure that they got moved
    ls fastq
    # if the files were moved, then feel free to remove the big fastq stuff
    rm -r big-fastq-play*
    ```
11. Download the Chinook salmon genome into a directory called `genome`
    ```{sh, eval=FALSE}
     mkdir genome
     cd genome
     wget ftp://ftp.ncbi.nlm.nih.gov/genomes/genbank/vertebrate_other/Oncorhynchus_tshawytscha/all_assembly_versions/GCA_002872995.1_Otsh_v1.0/GCA_002872995.1_Otsh_v1.0_genomic.fna.gz
    ```
    That dude is 760 Mb, but it goes really quickly (at least on SUMMIT.  Hummingbird seems to be a different story...)
12. Activate your `bioinf` conda environment to be able to use `bwa`:
    ```{sh, eval=FALSE}
    conda activate bioinf
    ```
13. Index the genome with `bwa` in order to align reads to it:
    ```{sh, eval=FALSE}
    bwa index GCA_002872995.1_Otsh_v1.0_genomic.fna.gz 
    ```
    This will take about 10 to 15 minutes.  During that time, do this:
    - If you are doing this within a `tmux` session, create a new tmux window (`cntrl-b c`). Note that this shell opens on the login node (the one that you started your tmux session on.)
    - Rename this new tmux window "browse" (`cntrl-b ,`)
    - Navigate to `~/scratch/chinook-play/genome/` and list the files there.  These new files
    are being created by bwa.
    - Switch back to your original tmux window.
14. If `bwa` is still running, ask me some questions.  Or maybe I will tell you more about how conda works...
15. If `bwa` is still running after that, ask me about _nested tmux sessions_.
16. Truth be told.  Indexing this thing can take a long time (30 to 40 minutes on a single core, perhaps).  So,
instead, here is what we will do: 
    a. Kill the current `bwa` job but issuing `cntrl-c` in the shell where it is running.
    b. Use `rclone` to get the genome and a `bwa` index for it from my Google Drive.  We want to put the 
    result in the `chinook-play` directory inside a directory called `chinook-genome-idx`. NOTE: Again,
    you will likely have to use your name for your rclone remote, rather than `gdrive-pers`.
    ```{sh, eval=FALSE}
    cd ../ # move up to the chinook-play directory
    rclone copy  -P  --drive-shared-with-me gdrive-pers:CSU-con-gen-2020/pre-indexed-chinook-genome chinook-genome-idx 
    ```
    That thing is 4.6 Gb, but can take less than a minute to transfer. (Let's see if things slow down with everyone
    running this command at the same time.)
17. Now when you do `ls` you should see something like this:
    ```{sh, eval=FALSE}
    (bioinf) [chinook-play]--% ls
    chinook-genome-idx  fastq  genome
    ```
18. For a final hurrah, we will start aligning those fastqs to the chinook genome. We will put the result in a directory
we create called `sam`. To make it clearer what we are doing we
will define some shell variables.:
    ```{sh, eval=FALSE}
    mkdir sam
    GENOME=chinook-genome-idx/GCA_002872995.1_Otsh_v1.0_genomic.fna.gz
    FQB=fastq/Battle_Creek_01_chinook_R
    SAMOUT=$(basename $FQB).sam
    
    # for fun, try echoing each of those variables and make sure they look right.
    # for example:
    echo $FQB
    
    # ...
    
    # then, launch bwa mem to map those reads to the genome.
    # the syntax is:
    # Usage: bwa mem [options] <idxbase> <in1.fq> [in2.fq]
    bwa mem $GENOME ${FQB}1.fq.gz ${FQB}2.fq.gz > sam/$SAMOUT 2>sam/$SAMOUT.error
    ```
19. Once that has started, it is hard to know anything is happening, because we have
redirected both stdout and stderr to files.  So, do what you did before, use tmux to
go to another shell and then use `tail` or `less` to look at the output files, which
are in `sam/Battle_Creek_01_chinook_R.sam` and `sam/Battle_Creek_01_chinook_R.sam.error`
within the `chinook-play` directory in `scratch`.  You might even count how many
lines of alignments file have been formed:
    ```{sh, eval=FALSE}
    awk '!/^@/' Battle_Creek_01_chinook_R.sam | wc 
    ```
20. Note that, in practice, we would not usually save the `.sam` file to disk.  Rather, we would
convert it into a compressed `.bam` file right as it comes off of `bwa mem`, and maybe even sort it
as it is coming off...
21. Note that this process might not finish before the end of class.  That is not a huge problem
if you are running the job within `tmux`.  You can logout and it will still keep running.

However, as said before, most of the time you will run _batch_ jobs rather than interactive jobs.
I just wanted to first give everyone a chance to step through these tasks in an interactive shell
on a compute node because that is crucial when developing your scripts, and we wanted to
make sure that everyone has rclone, and miniconda installed and working.










## More Boneyard...


Here is some nice stuff for summarizing all the information from the different runs from the chinook-wgs project:
```sh
qacct -o eriq -b 09271925 -j ml | tidy-qacct
```

Explain scratch space and how clusters are configured with respect to storage, etc.

Strategies---break names up with consistent characters:

- dashes within population names
- underscores for different groups of chromosomes
- periods for catenating pairs of pops

etc.  Basically, it just makes it much easier to split things up
when the time comes.

## The Queue  (SLURM/SGE/UGE)

## Modules package

## Compiling programs without admin privileges

Inevitably you will want to use a piece of software that is not available as
a module or is not otherwise installed on they system.

Typically these software programs have a frightful web of dependencies.

Unix/Linux distros typically maintain all these dependencies as libraries or packages
that can be installed using a `rpm` or `yum`.  However, the simple "plug-and-play" approach
to using these programs requires have administrator privileges so that the software can
be installed in one of the (typically protected) paths in the root (like `/usr/bin`).

But, you can use these programs to install packages into your home directory.  Once you have done
that, you need to let your system know where to look for these packages when it needs them
(i.e., when running a program or _linking_ to it whilst compiling up a program that uses it
as a dependency.

Hoffman2 runs CentOS.  Turns out that CentOS uses `yum` as a package manager.

Let's see if we can install llvm using yum.

```sh
yum search all llvm # <- this got me to devtoolset-7-all.x86_64 : Package shipping all available toolsets.

# a little web searching made it look like llvm-toolset-7-5.0.1-4.el7.x86_64.rpm or devtoolset-7-llvm-7.0-5.el7.x86_64.rpm
# might be what we want.  The first is a dependency of the second...
mkdir ~/centos

```
Was using instructions at [https://stackoverflow.com/questions/36651091/how-to-install-packages-in-linux-centos-without-root-user-with-automatic-depen](https://stackoverflow.com/questions/36651091/how-to-install-packages-in-linux-centos-without-root-user-with-automatic-depen) 

Couldn't get yum downloader to download any packages.  The whole thing looked like it was going to
be a mess, so I thought I would try with miniconda.

I installed miniconda (python 2.7 version) into `/u/nobackup/kruegg/eriq/programs/miniconda/` and then did this:
```sh
# probably could have listed them all at once, but wanted to watch them go 
# one at a time...
conda install numpy
conda install scipy
conda install pandas
conda install numba

# those all ran great.

conda install pysnptools

# that one didn't find a match, but I found on the web that I should try:
conda install -c bioconda pysnptools 

# that worked!
```


Also we want to touch briefly on LD_PATH (linking failures---and note that libraries are often
named libxxx.a) and CPATH (for failure to find xxxx.h), etc.




## Job arrays


Quick note: Redefine IFS to break on TABs so you can have full commands in there.
This is super useful for parsing job-array COMMLINES files.  
```
IFS=$'\t\n'; BOP=($(echo boing | awk '{printf("first\tsecond\tthird that is long\tfourth\n");}')); IFS=$' \t\n';
```


Definitely mention the `eval` keyword in bash for when you want to print 
command lines with redirects.  

Show the routine for it, and develop a good approach to efficiently
orchestrating redos.  If you know the taskIDs of the ones that failed
then it is pretty easy to write an awk script that picks out the
commands and puts them in a new file.  Actually, it is probably
better to just cycle over the numbers and use the -t option
to launch each.  Then there is now changing the job-ids file.  

In fact, I am starting to think that the -t option is better than
putting it into the file.

Question: if you give something on the command line, does that override
the directive in the header of the file?  If so, then you don't even
need to change the file.  Note that using the qsub command line options
instead of the directives really opens up a lot of possibilities for
writing useful scripts that are flexible.  

Also use short names for the jobs and have a system for naming the
redos (append numbers so you know which round it is, too) 
possibly base the name on the ways things failed the first time.  Like,
`fsttf1` = "Fst run for things that failed due to time limits, 1". Or
structure things so that redos can just be done by invoking it with -t 
and the jobid.

## Writing stdout and stderr to files

This is always good to do.  Note that `stdbuf` is super useful here so that
things don't get buffered super long. (PCAngsd doesn't seem to write antyhing till
the end...)


## Breaking stuff down

It is probably worth talking about how problems can be broken down into
smaller ones.  Maybe give an example, and then say that we will be talking about
this for every step of the way in bioinformatic pipelines.

One thing to note---sometimes processes go awry for one reason or another.
When things are in smaller chunks it is not such a huge investment to
re-run it. (Unlike stuff that runs for two weeks before you realize that
it ain't working right).

