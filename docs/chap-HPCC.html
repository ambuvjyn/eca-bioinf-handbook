<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 High Performance Computing Clusters (HPCC’s) | Practical Computing and Bioinformatics for Conservation and Evolutionary Genomics</title>
  <meta name="description" content="A book example for a Chapman &amp; Hall book." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 High Performance Computing Clusters (HPCC’s) | Practical Computing and Bioinformatics for Conservation and Evolutionary Genomics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A book example for a Chapman &amp; Hall book." />
  <meta name="github-repo" content="yihui/bookdown-crc" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 High Performance Computing Clusters (HPCC’s) | Practical Computing and Bioinformatics for Conservation and Evolutionary Genomics" />
  
  <meta name="twitter:description" content="A book example for a Chapman &amp; Hall book." />
  

<meta name="author" content="Eric C. Anderson" />


<meta name="date" content="2021-06-02" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="working-on-remote-servers.html"/>
<link rel="next" href="introduction-to-reproducible-research.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">ECA's Bioinformatics Handbook</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="erics-notes-of-what-he-might-do.html"><a href="erics-notes-of-what-he-might-do.html"><i class="fa fa-check"></i><b>2</b> Eric’s Notes of what he might do</a><ul>
<li class="chapter" data-level="2.1" data-path="erics-notes-of-what-he-might-do.html"><a href="erics-notes-of-what-he-might-do.html#table-of-topics"><i class="fa fa-check"></i><b>2.1</b> Table of topics</a></li>
</ul></li>
<li class="part"><span><b>I Part I: Essential Computing Skills</b></span></li>
<li class="chapter" data-level="3" data-path="overview-of-essential-computing-skills.html"><a href="overview-of-essential-computing-skills.html"><i class="fa fa-check"></i><b>3</b> Overview of Essential Computing Skills</a></li>
<li class="chapter" data-level="4" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html"><i class="fa fa-check"></i><b>4</b> Essential Unix/Linux Terminal Knowledge</a><ul>
<li class="chapter" data-level="4.1" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#getting-a-bash-shell-on-your-system"><i class="fa fa-check"></i><b>4.1</b> Getting a bash shell on your system</a></li>
<li class="chapter" data-level="4.2" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#navigating-the-unix-filesystem"><i class="fa fa-check"></i><b>4.2</b> Navigating the Unix filesystem</a><ul>
<li class="chapter" data-level="4.2.1" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#changing-the-working-directory-with-cd"><i class="fa fa-check"></i><b>4.2.1</b> Changing the working directory with <code>cd</code></a></li>
<li class="chapter" data-level="4.2.2" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#comm-prompt"><i class="fa fa-check"></i><b>4.2.2</b> Updating your command prompt</a></li>
<li class="chapter" data-level="4.2.3" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#tab-completion-for-paths"><i class="fa fa-check"></i><b>4.2.3</b> TAB-completion for paths</a></li>
<li class="chapter" data-level="4.2.4" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#listing-the-contents-of-a-directory-with-ls"><i class="fa fa-check"></i><b>4.2.4</b> Listing the contents of a directory with <code>ls</code></a></li>
<li class="chapter" data-level="4.2.5" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#globbing"><i class="fa fa-check"></i><b>4.2.5</b> Globbing</a></li>
<li class="chapter" data-level="4.2.6" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#what-makes-a-good-file-name"><i class="fa fa-check"></i><b>4.2.6</b> What makes a good file-name?</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#the-anatomy-of-a-unix-command"><i class="fa fa-check"></i><b>4.3</b> The anatomy of a Unix command</a><ul>
<li class="chapter" data-level="4.3.1" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#anatomy-command"><i class="fa fa-check"></i><b>4.3.1</b> The <code>command</code></a></li>
<li class="chapter" data-level="4.3.2" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#the-options"><i class="fa fa-check"></i><b>4.3.2</b> The <em>options</em></a></li>
<li class="chapter" data-level="4.3.3" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#arguments"><i class="fa fa-check"></i><b>4.3.3</b> Arguments</a></li>
<li class="chapter" data-level="4.3.4" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#getting-information-about-unix-commands"><i class="fa fa-check"></i><b>4.3.4</b> Getting information about Unix commands</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#handling-manipulating-and-viewing-files-and-streams"><i class="fa fa-check"></i><b>4.4</b> Handling, Manipulating, and Viewing files and streams</a><ul>
<li class="chapter" data-level="4.4.1" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#creating-new-directories"><i class="fa fa-check"></i><b>4.4.1</b> Creating new directories</a></li>
<li class="chapter" data-level="4.4.2" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#fundamental-file-handling-commands"><i class="fa fa-check"></i><b>4.4.2</b> Fundamental file-handling commands</a></li>
<li class="chapter" data-level="4.4.3" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#viewing-files"><i class="fa fa-check"></i><b>4.4.3</b> “Viewing” Files</a></li>
<li class="chapter" data-level="4.4.4" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#redirecting-standard-output-and"><i class="fa fa-check"></i><b>4.4.4</b> Redirecting standard output: <code>&gt;</code> and <code>&gt;&gt;</code></a></li>
<li class="chapter" data-level="4.4.5" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#stdin-and"><i class="fa fa-check"></i><b>4.4.5</b> stdin, <code>&lt;</code> and <code>|</code></a></li>
<li class="chapter" data-level="4.4.6" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#stderr"><i class="fa fa-check"></i><b>4.4.6</b> stderr</a></li>
<li class="chapter" data-level="4.4.7" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#symbolic-links"><i class="fa fa-check"></i><b>4.4.7</b> Symbolic links</a></li>
<li class="chapter" data-level="4.4.8" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#file-permissions"><i class="fa fa-check"></i><b>4.4.8</b> File Permissions</a></li>
<li class="chapter" data-level="4.4.9" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#editing-text-files-at-the-terminal"><i class="fa fa-check"></i><b>4.4.9</b> Editing text files at the terminal</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#unix-env"><i class="fa fa-check"></i><b>4.5</b> Customizing your Environment</a><ul>
<li class="chapter" data-level="4.5.1" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#appearances-matter"><i class="fa fa-check"></i><b>4.5.1</b> Appearances matter</a></li>
<li class="chapter" data-level="4.5.2" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#where-are-my-programscommands-at"><i class="fa fa-check"></i><b>4.5.2</b> Where are my programs/commands at?!</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#a-few-more-important-keystrokes"><i class="fa fa-check"></i><b>4.6</b> A Few More Important Keystrokes</a></li>
<li class="chapter" data-level="4.7" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#a-short-list-of-additional-useful-commands."><i class="fa fa-check"></i><b>4.7</b> A short list of additional useful commands.</a></li>
<li class="chapter" data-level="4.8" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#two-important-computing-concepts"><i class="fa fa-check"></i><b>4.8</b> Two important computing concepts</a><ul>
<li class="chapter" data-level="4.8.1" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#compression"><i class="fa fa-check"></i><b>4.8.1</b> Compression</a></li>
<li class="chapter" data-level="4.8.2" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#hashing"><i class="fa fa-check"></i><b>4.8.2</b> Hashing</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#unix-quick-study-guide"><i class="fa fa-check"></i><b>4.9</b> Unix: Quick Study Guide</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="shell-programming.html"><a href="shell-programming.html"><i class="fa fa-check"></i><b>5</b> Shell programming</a><ul>
<li class="chapter" data-level="5.1" data-path="shell-programming.html"><a href="shell-programming.html#an-example-script"><i class="fa fa-check"></i><b>5.1</b> An example script</a></li>
<li class="chapter" data-level="5.2" data-path="shell-programming.html"><a href="shell-programming.html#the-structure-of-a-bash-script"><i class="fa fa-check"></i><b>5.2</b> The Structure of a Bash Script</a><ul>
<li class="chapter" data-level="5.2.1" data-path="shell-programming.html"><a href="shell-programming.html#paragraph-before-shebang"><i class="fa fa-check"></i><b>5.2.1</b> A bit more on <code>;</code> and <code>&amp;</code></a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="shell-programming.html"><a href="shell-programming.html#variables"><i class="fa fa-check"></i><b>5.3</b> Variables</a><ul>
<li class="chapter" data-level="5.3.1" data-path="shell-programming.html"><a href="shell-programming.html#assigning-values-to-variables"><i class="fa fa-check"></i><b>5.3.1</b> Assigning values to variables</a></li>
<li class="chapter" data-level="5.3.2" data-path="shell-programming.html"><a href="shell-programming.html#accessing-values-from-variables"><i class="fa fa-check"></i><b>5.3.2</b> Accessing values from variables</a></li>
<li class="chapter" data-level="5.3.3" data-path="shell-programming.html"><a href="shell-programming.html#what-does-the-shell-do-with-the-value-substituted-for-a-variable"><i class="fa fa-check"></i><b>5.3.3</b> What does the shell do with the value substituted for a variable?</a></li>
<li class="chapter" data-level="5.3.4" data-path="shell-programming.html"><a href="shell-programming.html#quotes-and-var-subs"><i class="fa fa-check"></i><b>5.3.4</b> Double and Single Quotation Marks and Variable Substitution</a></li>
<li class="chapter" data-level="5.3.5" data-path="shell-programming.html"><a href="shell-programming.html#one-useful-fancy-variable-substitution-method"><i class="fa fa-check"></i><b>5.3.5</b> One useful, fancy, variable-substitution method</a></li>
<li class="chapter" data-level="5.3.6" data-path="shell-programming.html"><a href="shell-programming.html#bash-arithmetic"><i class="fa fa-check"></i><b>5.3.6</b> Integer Arithmetic with Shell Variables</a></li>
<li class="chapter" data-level="5.3.7" data-path="shell-programming.html"><a href="shell-programming.html#variable-arrays"><i class="fa fa-check"></i><b>5.3.7</b> Variable arrays</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="shell-programming.html"><a href="shell-programming.html#evaluate-a-command-and-substitute-the-result-on-the-command-line"><i class="fa fa-check"></i><b>5.4</b> Evaluate a command and substitute the result on the command line</a></li>
<li class="chapter" data-level="5.5" data-path="shell-programming.html"><a href="shell-programming.html#groupingcollecting-output-from-multiple-commands-commands-and-commands"><i class="fa fa-check"></i><b>5.5</b> Grouping/Collecting output from multiple commands: <code>(commands)</code> and <code>{ commands; }</code></a></li>
<li class="chapter" data-level="5.6" data-path="shell-programming.html"><a href="shell-programming.html#exit-status"><i class="fa fa-check"></i><b>5.6</b> Exit Status</a><ul>
<li class="chapter" data-level="5.6.1" data-path="shell-programming.html"><a href="shell-programming.html#combinations-of-exit-statuses"><i class="fa fa-check"></i><b>5.6.1</b> Combinations of exit statuses</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="shell-programming.html"><a href="shell-programming.html#unix-for-loops"><i class="fa fa-check"></i><b>5.7</b> Loops and repetition</a></li>
<li class="chapter" data-level="5.8" data-path="shell-programming.html"><a href="shell-programming.html#more-conditional-evaluation-if-then-else-and-friends"><i class="fa fa-check"></i><b>5.8</b> More Conditional Evaluation: <code>if</code>, <code>then</code>, <code>else</code>, and friends</a></li>
<li class="chapter" data-level="5.9" data-path="shell-programming.html"><a href="shell-programming.html#finallypositional-parameters"><i class="fa fa-check"></i><b>5.9</b> Finally…positional parameters</a></li>
<li class="chapter" data-level="5.10" data-path="shell-programming.html"><a href="shell-programming.html#basename-and-dirname-two-useful-little-utilities"><i class="fa fa-check"></i><b>5.10</b> <code>basename</code> and <code>dirname</code> two useful little utilities</a></li>
<li class="chapter" data-level="5.11" data-path="shell-programming.html"><a href="shell-programming.html#bash-functions"><i class="fa fa-check"></i><b>5.11</b> <code>bash</code> functions</a></li>
<li class="chapter" data-level="5.12" data-path="shell-programming.html"><a href="shell-programming.html#reading-files-line-by-line"><i class="fa fa-check"></i><b>5.12</b> reading files line by line</a></li>
<li class="chapter" data-level="5.13" data-path="shell-programming.html"><a href="shell-programming.html#further-reading"><i class="fa fa-check"></i><b>5.13</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="sed-awk-and-regular-expressions.html"><a href="sed-awk-and-regular-expressions.html"><i class="fa fa-check"></i><b>6</b> Sed, awk, and regular expressions</a><ul>
<li class="chapter" data-level="6.1" data-path="sed-awk-and-regular-expressions.html"><a href="sed-awk-and-regular-expressions.html#awk"><i class="fa fa-check"></i><b>6.1</b> awk</a><ul>
<li class="chapter" data-level="6.1.1" data-path="sed-awk-and-regular-expressions.html"><a href="sed-awk-and-regular-expressions.html#line-cycling-tests-and-actions"><i class="fa fa-check"></i><b>6.1.1</b> Line-cycling, tests and actions</a></li>
<li class="chapter" data-level="6.1.2" data-path="sed-awk-and-regular-expressions.html"><a href="sed-awk-and-regular-expressions.html#column-splitting-fields--f-nf-print-ofs-and-begin"><i class="fa fa-check"></i><b>6.1.2</b> Column splitting, fields, <code>-F</code>, <code>$</code>, <code>NF</code>, <code>print</code>, <code>OFS</code> and <code>BEGIN</code></a></li>
<li class="chapter" data-level="6.1.3" data-path="sed-awk-and-regular-expressions.html"><a href="sed-awk-and-regular-expressions.html#a-brief-introduction-to-regular-expressions"><i class="fa fa-check"></i><b>6.1.3</b> A brief introduction to regular expressions</a></li>
<li class="chapter" data-level="6.1.4" data-path="sed-awk-and-regular-expressions.html"><a href="sed-awk-and-regular-expressions.html#a-variety-of-tests"><i class="fa fa-check"></i><b>6.1.4</b> A variety of tests</a></li>
<li class="chapter" data-level="6.1.5" data-path="sed-awk-and-regular-expressions.html"><a href="sed-awk-and-regular-expressions.html#code-in-the-action-blocks"><i class="fa fa-check"></i><b>6.1.5</b> Code in the action blocks</a></li>
<li class="chapter" data-level="6.1.6" data-path="sed-awk-and-regular-expressions.html"><a href="sed-awk-and-regular-expressions.html#using-awk-to-assign-to-shell-variables"><i class="fa fa-check"></i><b>6.1.6</b> Using <code>awk</code> to assign to shell variables</a></li>
<li class="chapter" data-level="6.1.7" data-path="sed-awk-and-regular-expressions.html"><a href="sed-awk-and-regular-expressions.html#passing-variables-into-awk-with--v"><i class="fa fa-check"></i><b>6.1.7</b> Passing Variables into <code>awk</code> with <code>-v</code></a></li>
<li class="chapter" data-level="6.1.8" data-path="sed-awk-and-regular-expressions.html"><a href="sed-awk-and-regular-expressions.html#writing-awk-scripts-in-files"><i class="fa fa-check"></i><b>6.1.8</b> Writing awk scripts in files</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="sed-awk-and-regular-expressions.html"><a href="sed-awk-and-regular-expressions.html#sed"><i class="fa fa-check"></i><b>6.2</b> sed</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html"><i class="fa fa-check"></i><b>7</b> Working on remote servers</a><ul>
<li class="chapter" data-level="7.1" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#accessing-remote-computers"><i class="fa fa-check"></i><b>7.1</b> Accessing remote computers</a><ul>
<li class="chapter" data-level="7.1.1" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#windows"><i class="fa fa-check"></i><b>7.1.1</b> Windows</a></li>
<li class="chapter" data-level="7.1.2" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#hummingbird"><i class="fa fa-check"></i><b>7.1.2</b> Hummingbird</a></li>
<li class="chapter" data-level="7.1.3" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#summit"><i class="fa fa-check"></i><b>7.1.3</b> Summit</a></li>
<li class="chapter" data-level="7.1.4" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#sedna"><i class="fa fa-check"></i><b>7.1.4</b> Sedna</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#transferring-files-to-remote-computers"><i class="fa fa-check"></i><b>7.2</b> Transferring files to remote computers</a><ul>
<li class="chapter" data-level="7.2.1" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#sftp-via-lftp"><i class="fa fa-check"></i><b>7.2.1</b> <code>sftp</code> (via <code>lftp</code>)</a></li>
<li class="chapter" data-level="7.2.2" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#git"><i class="fa fa-check"></i><b>7.2.2</b> git</a></li>
<li class="chapter" data-level="7.2.3" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#globus"><i class="fa fa-check"></i><b>7.2.3</b> Globus</a></li>
<li class="chapter" data-level="7.2.4" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#interfacing-with-the-cloud"><i class="fa fa-check"></i><b>7.2.4</b> Interfacing with “The Cloud”</a></li>
<li class="chapter" data-level="7.2.5" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#get-seqs"><i class="fa fa-check"></i><b>7.2.5</b> Getting files from a sequencing center</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#tmux"><i class="fa fa-check"></i><b>7.3</b> <code>tmux</code>: the terminal multiplexer</a><ul>
<li class="chapter" data-level="7.3.1" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#an-analogy-for-how-tmux-works"><i class="fa fa-check"></i><b>7.3.1</b> An analogy for how <code>tmux</code> works</a></li>
<li class="chapter" data-level="7.3.2" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#first-steps-with-tmux"><i class="fa fa-check"></i><b>7.3.2</b> First steps with <code>tmux</code></a></li>
<li class="chapter" data-level="7.3.3" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#further-steps-with-tmux"><i class="fa fa-check"></i><b>7.3.3</b> Further steps with <code>tmux</code></a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#tmux-for-mac-users"><i class="fa fa-check"></i><b>7.4</b> <code>tmux</code> for Mac users</a></li>
<li class="chapter" data-level="7.5" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#installing-software-on-an-hpcc"><i class="fa fa-check"></i><b>7.5</b> Installing Software on an HPCC</a><ul>
<li class="chapter" data-level="7.5.1" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#modules"><i class="fa fa-check"></i><b>7.5.1</b> Modules</a></li>
<li class="chapter" data-level="7.5.2" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#miniconda"><i class="fa fa-check"></i><b>7.5.2</b> Miniconda</a></li>
<li class="chapter" data-level="7.5.3" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#installing-java-programs"><i class="fa fa-check"></i><b>7.5.3</b> Installing Java Programs</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#vim-its-time-to-get-serious-with-text-editing"><i class="fa fa-check"></i><b>7.6</b> <code>vim</code>: it’s time to get serious with text editing</a><ul>
<li class="chapter" data-level="7.6.1" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#using-neovim-and-nvim-r-and-tmux-to-use-r-well-on-the-cluster"><i class="fa fa-check"></i><b>7.6.1</b> Using neovim and Nvim-R and tmux to use R well on the cluster</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chap-HPCC.html"><a href="chap-HPCC.html"><i class="fa fa-check"></i><b>8</b> High Performance Computing Clusters (HPCC’s)</a><ul>
<li class="chapter" data-level="8.1" data-path="chap-HPCC.html"><a href="chap-HPCC.html#an-oversimplified-but-useful-view-of-a-computing-cluster"><i class="fa fa-check"></i><b>8.1</b> An oversimplified, but useful, view of a computing cluster</a></li>
<li class="chapter" data-level="8.2" data-path="chap-HPCC.html"><a href="chap-HPCC.html#cluster-computing-and-the-job-scheduler"><i class="fa fa-check"></i><b>8.2</b> Cluster computing and the job scheduler</a></li>
<li class="chapter" data-level="8.3" data-path="chap-HPCC.html"><a href="chap-HPCC.html#slurm-info"><i class="fa fa-check"></i><b>8.3</b> Learning about the resources on your HPCC</a></li>
<li class="chapter" data-level="8.4" data-path="chap-HPCC.html"><a href="chap-HPCC.html#getting-compute-resources-allocated-to-your-jobs-on-an-hpcc"><i class="fa fa-check"></i><b>8.4</b> Getting compute resources allocated to your jobs on an HPCC</a><ul>
<li class="chapter" data-level="8.4.1" data-path="chap-HPCC.html"><a href="chap-HPCC.html#interactive-sessions"><i class="fa fa-check"></i><b>8.4.1</b> Interactive sessions</a></li>
<li class="chapter" data-level="8.4.2" data-path="chap-HPCC.html"><a href="chap-HPCC.html#slurm-batch"><i class="fa fa-check"></i><b>8.4.2</b> Batch jobs</a></li>
<li class="chapter" data-level="8.4.3" data-path="chap-HPCC.html"><a href="chap-HPCC.html#slurm-job-arrays"><i class="fa fa-check"></i><b>8.4.3</b> SLURM Job Arrays</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="chap-HPCC.html"><a href="chap-HPCC.html#prepation-interlude-an-in-class-exercise-to-make-sure-everything-is-configured-correctly"><i class="fa fa-check"></i><b>8.5</b> PREPATION INTERLUDE: An in-class exercise to make sure everything is configured correctly</a></li>
<li class="chapter" data-level="8.6" data-path="chap-HPCC.html"><a href="chap-HPCC.html#more-boneyard"><i class="fa fa-check"></i><b>8.6</b> More Boneyard…</a></li>
<li class="chapter" data-level="8.7" data-path="chap-HPCC.html"><a href="chap-HPCC.html#the-queue-slurmsgeuge"><i class="fa fa-check"></i><b>8.7</b> The Queue (SLURM/SGE/UGE)</a></li>
<li class="chapter" data-level="8.8" data-path="chap-HPCC.html"><a href="chap-HPCC.html#modules-package"><i class="fa fa-check"></i><b>8.8</b> Modules package</a></li>
<li class="chapter" data-level="8.9" data-path="chap-HPCC.html"><a href="chap-HPCC.html#compiling-programs-without-admin-privileges"><i class="fa fa-check"></i><b>8.9</b> Compiling programs without admin privileges</a></li>
<li class="chapter" data-level="8.10" data-path="chap-HPCC.html"><a href="chap-HPCC.html#job-arrays"><i class="fa fa-check"></i><b>8.10</b> Job arrays</a></li>
<li class="chapter" data-level="8.11" data-path="chap-HPCC.html"><a href="chap-HPCC.html#writing-stdout-and-stderr-to-files"><i class="fa fa-check"></i><b>8.11</b> Writing stdout and stderr to files</a></li>
<li class="chapter" data-level="8.12" data-path="chap-HPCC.html"><a href="chap-HPCC.html#breaking-stuff-down"><i class="fa fa-check"></i><b>8.12</b> Breaking stuff down</a></li>
</ul></li>
<li class="part"><span><b>II Part II: Reproducible Research Strategies</b></span></li>
<li class="chapter" data-level="9" data-path="introduction-to-reproducible-research.html"><a href="introduction-to-reproducible-research.html"><i class="fa fa-check"></i><b>9</b> Introduction to Reproducible Research</a></li>
<li class="chapter" data-level="10" data-path="rstudio-and-project-centered-organization.html"><a href="rstudio-and-project-centered-organization.html"><i class="fa fa-check"></i><b>10</b> Rstudio and Project-centered Organization</a><ul>
<li class="chapter" data-level="10.1" data-path="rstudio-and-project-centered-organization.html"><a href="rstudio-and-project-centered-organization.html#organizing-big-projects"><i class="fa fa-check"></i><b>10.1</b> Organizing big projects</a></li>
<li class="chapter" data-level="10.2" data-path="rstudio-and-project-centered-organization.html"><a href="rstudio-and-project-centered-organization.html#using-rstudio-in-workflows-with-remote-computers-and-hpccs"><i class="fa fa-check"></i><b>10.2</b> Using RStudio in workflows with remote computers and HPCCs</a><ul>
<li class="chapter" data-level="10.2.1" data-path="rstudio-and-project-centered-organization.html"><a href="rstudio-and-project-centered-organization.html#keeping-an-rstudio-project-in-sync-with-github"><i class="fa fa-check"></i><b>10.2.1</b> Keeping an RStudio project “in sync” with GitHub</a></li>
<li class="chapter" data-level="10.2.2" data-path="rstudio-and-project-centered-organization.html"><a href="rstudio-and-project-centered-organization.html#evaluating-scripts-line-by-line-on-a-remote-machine-from-within-rstudio"><i class="fa fa-check"></i><b>10.2.2</b> Evaluating scripts line by line on a remote machine from within RStudio</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="version-control.html"><a href="version-control.html"><i class="fa fa-check"></i><b>11</b> Version control</a><ul>
<li class="chapter" data-level="11.1" data-path="version-control.html"><a href="version-control.html#why-use-version-control"><i class="fa fa-check"></i><b>11.1</b> Why use version control?</a></li>
<li class="chapter" data-level="11.2" data-path="version-control.html"><a href="version-control.html#git-workings"><i class="fa fa-check"></i><b>11.2</b> How git works</a></li>
<li class="chapter" data-level="11.3" data-path="version-control.html"><a href="version-control.html#git-workflow-patterns"><i class="fa fa-check"></i><b>11.3</b> git workflow patterns</a></li>
<li class="chapter" data-level="11.4" data-path="version-control.html"><a href="version-control.html#using-git-with-rstudio"><i class="fa fa-check"></i><b>11.4</b> using git with Rstudio</a></li>
<li class="chapter" data-level="11.5" data-path="version-control.html"><a href="version-control.html#git-on-the-command-line"><i class="fa fa-check"></i><b>11.5</b> git on the command line</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="a-fast-furious-overview-of-the-tidyverse.html"><a href="a-fast-furious-overview-of-the-tidyverse.html"><i class="fa fa-check"></i><b>12</b> A fast, furious overview of the tidyverse</a></li>
<li class="chapter" data-level="13" data-path="authoring-reproducibly-with-rmarkdown.html"><a href="authoring-reproducibly-with-rmarkdown.html"><i class="fa fa-check"></i><b>13</b> Authoring reproducibly with Rmarkdown</a><ul>
<li class="chapter" data-level="13.1" data-path="authoring-reproducibly-with-rmarkdown.html"><a href="authoring-reproducibly-with-rmarkdown.html#notebooks"><i class="fa fa-check"></i><b>13.1</b> Notebooks</a></li>
<li class="chapter" data-level="13.2" data-path="authoring-reproducibly-with-rmarkdown.html"><a href="authoring-reproducibly-with-rmarkdown.html#references"><i class="fa fa-check"></i><b>13.2</b> References</a><ul>
<li class="chapter" data-level="13.2.1" data-path="authoring-reproducibly-with-rmarkdown.html"><a href="authoring-reproducibly-with-rmarkdown.html#zotero-and-rmarkdown"><i class="fa fa-check"></i><b>13.2.1</b> Zotero and Rmarkdown</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="authoring-reproducibly-with-rmarkdown.html"><a href="authoring-reproducibly-with-rmarkdown.html#bookdown"><i class="fa fa-check"></i><b>13.3</b> Bookdown</a></li>
<li class="chapter" data-level="13.4" data-path="authoring-reproducibly-with-rmarkdown.html"><a href="authoring-reproducibly-with-rmarkdown.html#google-docs"><i class="fa fa-check"></i><b>13.4</b> Google Docs</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="managing-workflows-with-snakemake.html"><a href="managing-workflows-with-snakemake.html"><i class="fa fa-check"></i><b>14</b> Managing Workflows with Snakemake</a><ul>
<li class="chapter" data-level="14.1" data-path="managing-workflows-with-snakemake.html"><a href="managing-workflows-with-snakemake.html#a-simple-snakemake-example"><i class="fa fa-check"></i><b>14.1</b> A Simple Snakemake Example</a><ul>
<li class="chapter" data-level="14.1.1" data-path="managing-workflows-with-snakemake.html"><a href="managing-workflows-with-snakemake.html#snakefile-1-a-simple-rule-to-trimmomcawesome-the-files-from-001"><i class="fa fa-check"></i><b>14.1.1</b> Snakefile #1: A simple rule to TrimmoMcAwesome the files from 001</a></li>
<li class="chapter" data-level="14.1.2" data-path="managing-workflows-with-snakemake.html"><a href="managing-workflows-with-snakemake.html#snakefile-2-a-single-trimmomcawesome-rule-using-wildcards"><i class="fa fa-check"></i><b>14.1.2</b> Snakefile #2: A single TrimmoMcAwesome rule using wildcards</a></li>
<li class="chapter" data-level="14.1.3" data-path="managing-workflows-with-snakemake.html"><a href="managing-workflows-with-snakemake.html#snakefile-3-add-trim_stupendous-in-there"><i class="fa fa-check"></i><b>14.1.3</b> Snakefile #3: Add <code>trim_stupendous</code> in there</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="managing-workflows-with-snakemake.html"><a href="managing-workflows-with-snakemake.html#ugly-complex-file-namesa-job-for-snakemake-input-functions"><i class="fa fa-check"></i><b>14.2</b> Ugly, complex file names…a job for Snakemake input functions!</a><ul>
<li class="chapter" data-level="14.2.1" data-path="managing-workflows-with-snakemake.html"><a href="managing-workflows-with-snakemake.html#playing-with-pandas"><i class="fa fa-check"></i><b>14.2.1</b> Playing with Pandas</a></li>
<li class="chapter" data-level="14.2.2" data-path="managing-workflows-with-snakemake.html"><a href="managing-workflows-with-snakemake.html#a-quick-sidebar-spoofing-a-snakemake-input-function"><i class="fa fa-check"></i><b>14.2.2</b> A Quick Sidebar: Spoofing a Snakemake input function</a></li>
<li class="chapter" data-level="14.2.3" data-path="managing-workflows-with-snakemake.html"><a href="managing-workflows-with-snakemake.html#snakefile-4-an-input-function-for-trim_awesome"><i class="fa fa-check"></i><b>14.2.3</b> Snakefile #4: an input function for trim_awesome</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="managing-workflows-with-snakemake.html"><a href="managing-workflows-with-snakemake.html#input-functions-from-python-dictionaries"><i class="fa fa-check"></i><b>14.3</b> Input functions from Python Dictionaries</a></li>
<li class="chapter" data-level="14.4" data-path="managing-workflows-with-snakemake.html"><a href="managing-workflows-with-snakemake.html#generating-lists-of-files-with-expand"><i class="fa fa-check"></i><b>14.4</b> Generating lists of files with expand()</a></li>
<li class="chapter" data-level="14.5" data-path="managing-workflows-with-snakemake.html"><a href="managing-workflows-with-snakemake.html#a-python-primer-for-using-snakemake"><i class="fa fa-check"></i><b>14.5</b> A Python Primer for Using Snakemake</a></li>
<li class="chapter" data-level="14.6" data-path="managing-workflows-with-snakemake.html"><a href="managing-workflows-with-snakemake.html#using-snakemake-on-a-computing-cluster"><i class="fa fa-check"></i><b>14.6</b> Using Snakemake on a Computing Cluster</a><ul>
<li class="chapter" data-level="14.6.1" data-path="managing-workflows-with-snakemake.html"><a href="managing-workflows-with-snakemake.html#installing-and-configuring-the-snakemake-slurm-profile"><i class="fa fa-check"></i><b>14.6.1</b> Installing and Configuring the Snakemake SLURM profile</a></li>
<li class="chapter" data-level="14.6.2" data-path="managing-workflows-with-snakemake.html"><a href="managing-workflows-with-snakemake.html#playing-with-our-new-slurm-profile"><i class="fa fa-check"></i><b>14.6.2</b> Playing with our new SLURM profile</a></li>
<li class="chapter" data-level="14.6.3" data-path="managing-workflows-with-snakemake.html"><a href="managing-workflows-with-snakemake.html#managing-cluster-resources-with-snakemake"><i class="fa fa-check"></i><b>14.6.3</b> Managing cluster resources with Snakemake</a></li>
<li class="chapter" data-level="14.6.4" data-path="managing-workflows-with-snakemake.html"><a href="managing-workflows-with-snakemake.html#grouping-short-jobs-to-run-on-a-single-node"><i class="fa fa-check"></i><b>14.6.4</b> Grouping short jobs to run on a single node</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Part III: Bioinformatic Analyses</b></span></li>
<li class="chapter" data-level="15" data-path="overview-of-bioinformatic-analyses.html"><a href="overview-of-bioinformatic-analyses.html"><i class="fa fa-check"></i><b>15</b> Overview of Bioinformatic Analyses</a></li>
<li class="chapter" data-level="16" data-path="dna-sequences-and-sequencing.html"><a href="dna-sequences-and-sequencing.html"><i class="fa fa-check"></i><b>16</b> DNA Sequences and Sequencing</a><ul>
<li class="chapter" data-level="16.1" data-path="dna-sequences-and-sequencing.html"><a href="dna-sequences-and-sequencing.html#dna-stuff"><i class="fa fa-check"></i><b>16.1</b> DNA Stuff</a><ul>
<li class="chapter" data-level="16.1.1" data-path="dna-sequences-and-sequencing.html"><a href="dna-sequences-and-sequencing.html#dna-replication-with-dna-polymerase"><i class="fa fa-check"></i><b>16.1.1</b> DNA Replication with DNA Polymerase</a></li>
<li class="chapter" data-level="16.1.2" data-path="dna-sequences-and-sequencing.html"><a href="dna-sequences-and-sequencing.html#the-importance-of-the-3-hydroxyl"><i class="fa fa-check"></i><b>16.1.2</b> The importance of the 3’ hydroxyl…</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="dna-sequences-and-sequencing.html"><a href="dna-sequences-and-sequencing.html#sanger-sequencing"><i class="fa fa-check"></i><b>16.2</b> Sanger sequencing</a></li>
<li class="chapter" data-level="16.3" data-path="dna-sequences-and-sequencing.html"><a href="dna-sequences-and-sequencing.html#illumina-sequencing-by-synthesis"><i class="fa fa-check"></i><b>16.3</b> Illumina Sequencing by Synthesis</a></li>
<li class="chapter" data-level="16.4" data-path="dna-sequences-and-sequencing.html"><a href="dna-sequences-and-sequencing.html#library-prep-protocols"><i class="fa fa-check"></i><b>16.4</b> Library Prep Protocols</a><ul>
<li class="chapter" data-level="16.4.1" data-path="dna-sequences-and-sequencing.html"><a href="dna-sequences-and-sequencing.html#wgs"><i class="fa fa-check"></i><b>16.4.1</b> WGS</a></li>
<li class="chapter" data-level="16.4.2" data-path="dna-sequences-and-sequencing.html"><a href="dna-sequences-and-sequencing.html#rad-seq-methods"><i class="fa fa-check"></i><b>16.4.2</b> RAD-Seq methods</a></li>
<li class="chapter" data-level="16.4.3" data-path="dna-sequences-and-sequencing.html"><a href="dna-sequences-and-sequencing.html#amplicon-sequencing"><i class="fa fa-check"></i><b>16.4.3</b> Amplicon Sequencing</a></li>
<li class="chapter" data-level="16.4.4" data-path="dna-sequences-and-sequencing.html"><a href="dna-sequences-and-sequencing.html#capture-arrays-rapture-etc."><i class="fa fa-check"></i><b>16.4.4</b> Capture arrays, RAPTURE, etc.</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html"><i class="fa fa-check"></i><b>17</b> Bioinformatic file formats</a><ul>
<li class="chapter" data-level="17.1" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#sequences"><i class="fa fa-check"></i><b>17.1</b> Sequences</a></li>
<li class="chapter" data-level="17.2" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#fastq"><i class="fa fa-check"></i><b>17.2</b> FASTQ</a><ul>
<li class="chapter" data-level="17.2.1" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#illumina-ids"><i class="fa fa-check"></i><b>17.2.1</b> Line 1: Illumina identifier lines</a></li>
<li class="chapter" data-level="17.2.2" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#bqscores"><i class="fa fa-check"></i><b>17.2.2</b> Line 4: Base quality scores</a></li>
<li class="chapter" data-level="17.2.3" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#a-fastq-tidyverse-interlude"><i class="fa fa-check"></i><b>17.2.3</b> A FASTQ ‘tidyverse’ Interlude</a></li>
<li class="chapter" data-level="17.2.4" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#comparing-read-1-to-read-2"><i class="fa fa-check"></i><b>17.2.4</b> Comparing read 1 to read 2</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#fasta"><i class="fa fa-check"></i><b>17.3</b> FASTA</a><ul>
<li class="chapter" data-level="17.3.1" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#genomic-ranges"><i class="fa fa-check"></i><b>17.3.1</b> Genomic ranges</a></li>
<li class="chapter" data-level="17.3.2" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#extracting-genomic-ranges-from-a-fasta-file"><i class="fa fa-check"></i><b>17.3.2</b> Extracting genomic ranges from a FASTA file</a></li>
<li class="chapter" data-level="17.3.3" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#downloading-reference-genomes-from-ncbi"><i class="fa fa-check"></i><b>17.3.3</b> Downloading reference genomes from NCBI</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#alignments"><i class="fa fa-check"></i><b>17.4</b> Alignments</a><ul>
<li class="chapter" data-level="17.4.1" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#how-might-i-align-to-thee-let-me-count-the-ways"><i class="fa fa-check"></i><b>17.4.1</b> How might I align to thee? Let me count the ways…</a></li>
<li class="chapter" data-level="17.4.2" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#play-with-simple-alignments"><i class="fa fa-check"></i><b>17.4.2</b> Play with simple alignments</a></li>
<li class="chapter" data-level="17.4.3" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#sam-flags"><i class="fa fa-check"></i><b>17.4.3</b> SAM Flags</a></li>
<li class="chapter" data-level="17.4.4" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#cigar"><i class="fa fa-check"></i><b>17.4.4</b> The CIGAR string</a></li>
<li class="chapter" data-level="17.4.5" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#the-seq-and-qual-columns"><i class="fa fa-check"></i><b>17.4.5</b> The SEQ and QUAL columns</a></li>
<li class="chapter" data-level="17.4.6" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#sam-headers"><i class="fa fa-check"></i><b>17.4.6</b> SAM File Headers</a></li>
<li class="chapter" data-level="17.4.7" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#the-bam-format"><i class="fa fa-check"></i><b>17.4.7</b> The BAM format</a></li>
<li class="chapter" data-level="17.4.8" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#quick-self-study"><i class="fa fa-check"></i><b>17.4.8</b> Quick self study</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#variants"><i class="fa fa-check"></i><b>17.5</b> Variants</a><ul>
<li class="chapter" data-level="17.5.1" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#vcf-format-the-body"><i class="fa fa-check"></i><b>17.5.1</b> VCF Format – The Body</a></li>
<li class="chapter" data-level="17.5.2" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#vcf-format-the-header"><i class="fa fa-check"></i><b>17.5.2</b> VCF Format – The Header</a></li>
<li class="chapter" data-level="17.5.3" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#boneyard"><i class="fa fa-check"></i><b>17.5.3</b> Boneyard</a></li>
</ul></li>
<li class="chapter" data-level="17.6" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#segments"><i class="fa fa-check"></i><b>17.6</b> Segments</a></li>
<li class="chapter" data-level="17.7" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#conversionextractions-between-different-formats"><i class="fa fa-check"></i><b>17.7</b> Conversion/Extractions between different formats</a></li>
<li class="chapter" data-level="17.8" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#visualization-of-genomic-data"><i class="fa fa-check"></i><b>17.8</b> Visualization of Genomic Data</a><ul>
<li class="chapter" data-level="17.8.1" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#sample-data"><i class="fa fa-check"></i><b>17.8.1</b> Sample Data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="genome-assembly.html"><a href="genome-assembly.html"><i class="fa fa-check"></i><b>18</b> Genome Assembly</a></li>
<li class="chapter" data-level="19" data-path="alignment-of-sequence-data-to-a-reference-genome-and-associated-steps.html"><a href="alignment-of-sequence-data-to-a-reference-genome-and-associated-steps.html"><i class="fa fa-check"></i><b>19</b> Alignment of sequence data to a reference genome (and associated steps)</a><ul>
<li class="chapter" data-level="19.1" data-path="alignment-of-sequence-data-to-a-reference-genome-and-associated-steps.html"><a href="alignment-of-sequence-data-to-a-reference-genome-and-associated-steps.html#the-journey-of-each-dna-fragment-from-organism-to-sequencing-read"><i class="fa fa-check"></i><b>19.1</b> The Journey of each DNA Fragment from Organism to Sequencing Read</a></li>
<li class="chapter" data-level="19.2" data-path="alignment-of-sequence-data-to-a-reference-genome-and-associated-steps.html"><a href="alignment-of-sequence-data-to-a-reference-genome-and-associated-steps.html#read-groups"><i class="fa fa-check"></i><b>19.2</b> Read Groups</a></li>
<li class="chapter" data-level="19.3" data-path="alignment-of-sequence-data-to-a-reference-genome-and-associated-steps.html"><a href="alignment-of-sequence-data-to-a-reference-genome-and-associated-steps.html#aligning-reads-with-bwa"><i class="fa fa-check"></i><b>19.3</b> Aligning reads with <code>bwa</code></a><ul>
<li class="chapter" data-level="19.3.1" data-path="alignment-of-sequence-data-to-a-reference-genome-and-associated-steps.html"><a href="alignment-of-sequence-data-to-a-reference-genome-and-associated-steps.html#indexing-the-genome-for-alignment"><i class="fa fa-check"></i><b>19.3.1</b> Indexing the genome for alignment</a></li>
<li class="chapter" data-level="19.3.2" data-path="alignment-of-sequence-data-to-a-reference-genome-and-associated-steps.html"><a href="alignment-of-sequence-data-to-a-reference-genome-and-associated-steps.html#mapping-reads-with-bwa-mem"><i class="fa fa-check"></i><b>19.3.2</b> Mapping reads with <code>bwa mem</code></a></li>
<li class="chapter" data-level="19.3.3" data-path="alignment-of-sequence-data-to-a-reference-genome-and-associated-steps.html"><a href="alignment-of-sequence-data-to-a-reference-genome-and-associated-steps.html#hold-it-right-there-buddy-what-about-the-read-groups"><i class="fa fa-check"></i><b>19.3.3</b> Hold it Right There, Buddy! What about the Read Groups?</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="alignment-of-sequence-data-to-a-reference-genome-and-associated-steps.html"><a href="alignment-of-sequence-data-to-a-reference-genome-and-associated-steps.html#processing-alignment-output-with-samtools"><i class="fa fa-check"></i><b>19.4</b> Processing alignment output with <code>samtools</code></a><ul>
<li class="chapter" data-level="19.4.1" data-path="alignment-of-sequence-data-to-a-reference-genome-and-associated-steps.html"><a href="alignment-of-sequence-data-to-a-reference-genome-and-associated-steps.html#samtools-subcommands"><i class="fa fa-check"></i><b>19.4.1</b> <code>samtools</code> subcommands</a></li>
</ul></li>
<li class="chapter" data-level="19.5" data-path="alignment-of-sequence-data-to-a-reference-genome-and-associated-steps.html"><a href="alignment-of-sequence-data-to-a-reference-genome-and-associated-steps.html#boneyard-below-here"><i class="fa fa-check"></i><b>19.5</b> BONEYARD BELOW HERE</a></li>
<li class="chapter" data-level="19.6" data-path="alignment-of-sequence-data-to-a-reference-genome-and-associated-steps.html"><a href="alignment-of-sequence-data-to-a-reference-genome-and-associated-steps.html#preprocess"><i class="fa fa-check"></i><b>19.6</b> Preprocess ?</a></li>
<li class="chapter" data-level="19.7" data-path="alignment-of-sequence-data-to-a-reference-genome-and-associated-steps.html"><a href="alignment-of-sequence-data-to-a-reference-genome-and-associated-steps.html#quick-notes-to-self-on-chaining-things"><i class="fa fa-check"></i><b>19.7</b> Quick notes to self on chaining things:</a></li>
<li class="chapter" data-level="19.8" data-path="alignment-of-sequence-data-to-a-reference-genome-and-associated-steps.html"><a href="alignment-of-sequence-data-to-a-reference-genome-and-associated-steps.html#merging-bam-files"><i class="fa fa-check"></i><b>19.8</b> Merging BAM files</a></li>
<li class="chapter" data-level="19.9" data-path="alignment-of-sequence-data-to-a-reference-genome-and-associated-steps.html"><a href="alignment-of-sequence-data-to-a-reference-genome-and-associated-steps.html#divide-and-conquer-strategies"><i class="fa fa-check"></i><b>19.9</b> Divide and Conquer Strategies</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="variant-calling.html"><a href="variant-calling.html"><i class="fa fa-check"></i><b>20</b> Variant calling</a><ul>
<li class="chapter" data-level="20.1" data-path="variant-calling.html"><a href="variant-calling.html#genotype-likelihoods"><i class="fa fa-check"></i><b>20.1</b> Genotype Likelihoods</a><ul>
<li class="chapter" data-level="20.1.1" data-path="variant-calling.html"><a href="variant-calling.html#basic-sketch-of-genotype-likelihood-calculations"><i class="fa fa-check"></i><b>20.1.1</b> Basic Sketch of Genotype Likelihood Calculations</a></li>
<li class="chapter" data-level="20.1.2" data-path="variant-calling.html"><a href="variant-calling.html#specifics-of-different-genotype-likelihoods"><i class="fa fa-check"></i><b>20.1.2</b> Specifics of different genotype likelihoods</a></li>
<li class="chapter" data-level="20.1.3" data-path="variant-calling.html"><a href="variant-calling.html#computing-genotype-likelihoods-with-three-different-softwares"><i class="fa fa-check"></i><b>20.1.3</b> Computing genotype likelihoods with three different softwares</a></li>
<li class="chapter" data-level="20.1.4" data-path="variant-calling.html"><a href="variant-calling.html#a-directed-acyclic-graph-for-genotype-likelihoods"><i class="fa fa-check"></i><b>20.1.4</b> A Directed Acyclic Graph For Genotype Likelihoods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="21" data-path="boneyard-1.html"><a href="boneyard-1.html"><i class="fa fa-check"></i><b>21</b> Boneyard</a></li>
<li class="chapter" data-level="22" data-path="basic-handling-of-vcf-files.html"><a href="basic-handling-of-vcf-files.html"><i class="fa fa-check"></i><b>22</b> Basic Handling of VCF files</a><ul>
<li class="chapter" data-level="22.1" data-path="basic-handling-of-vcf-files.html"><a href="basic-handling-of-vcf-files.html#bcftools"><i class="fa fa-check"></i><b>22.1</b> bcftools</a><ul>
<li class="chapter" data-level="22.1.1" data-path="basic-handling-of-vcf-files.html"><a href="basic-handling-of-vcf-files.html#tell-me-about-my-vcf-file"><i class="fa fa-check"></i><b>22.1.1</b> Tell me about my VCF file!</a></li>
<li class="chapter" data-level="22.1.2" data-path="basic-handling-of-vcf-files.html"><a href="basic-handling-of-vcf-files.html#get-fragmentsparts-of-my-vcf-file"><i class="fa fa-check"></i><b>22.1.2</b> Get fragments/parts of my VCF file</a></li>
<li class="chapter" data-level="22.1.3" data-path="basic-handling-of-vcf-files.html"><a href="basic-handling-of-vcf-files.html#combine-vcf-files-in-various-ways"><i class="fa fa-check"></i><b>22.1.3</b> Combine VCF files in various ways</a></li>
<li class="chapter" data-level="22.1.4" data-path="basic-handling-of-vcf-files.html"><a href="basic-handling-of-vcf-files.html#filter-out-variants-for-a-variety-of-reasons"><i class="fa fa-check"></i><b>22.1.4</b> Filter out variants for a variety of reasons</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="bioinformatics-for-rad-seq-data-with-and-without-a-reference-genome.html"><a href="bioinformatics-for-rad-seq-data-with-and-without-a-reference-genome.html"><i class="fa fa-check"></i><b>23</b> Bioinformatics for RAD seq data with and without a reference genome</a></li>
<li class="chapter" data-level="24" data-path="processing-amplicon-sequencing-data.html"><a href="processing-amplicon-sequencing-data.html"><i class="fa fa-check"></i><b>24</b> Processing amplicon sequencing data</a></li>
<li class="chapter" data-level="25" data-path="genome-annotation.html"><a href="genome-annotation.html"><i class="fa fa-check"></i><b>25</b> Genome Annotation</a></li>
<li class="chapter" data-level="26" data-path="whole-genome-alignment-strategies.html"><a href="whole-genome-alignment-strategies.html"><i class="fa fa-check"></i><b>26</b> Whole genome alignment strategies</a><ul>
<li class="chapter" data-level="26.1" data-path="whole-genome-alignment-strategies.html"><a href="whole-genome-alignment-strategies.html#mapping-of-scaffolds-to-a-closely-related-genome"><i class="fa fa-check"></i><b>26.1</b> Mapping of scaffolds to a closely related genome</a></li>
<li class="chapter" data-level="26.2" data-path="whole-genome-alignment-strategies.html"><a href="whole-genome-alignment-strategies.html#obtaining-ancestral-states-from-an-outgroup-genome"><i class="fa fa-check"></i><b>26.2</b> Obtaining Ancestral States from an Outgroup Genome</a><ul>
<li class="chapter" data-level="26.2.1" data-path="whole-genome-alignment-strategies.html"><a href="whole-genome-alignment-strategies.html#using-lastz-to-align-coho-to-the-chinook-genome"><i class="fa fa-check"></i><b>26.2.1</b> Using LASTZ to align coho to the chinook genome</a></li>
<li class="chapter" data-level="26.2.2" data-path="whole-genome-alignment-strategies.html"><a href="whole-genome-alignment-strategies.html#try-on-the-chinook-chromosomes"><i class="fa fa-check"></i><b>26.2.2</b> Try on the chinook chromosomes</a></li>
<li class="chapter" data-level="26.2.3" data-path="whole-genome-alignment-strategies.html"><a href="whole-genome-alignment-strategies.html#explore-the-other-parameters-more"><i class="fa fa-check"></i><b>26.2.3</b> Explore the other parameters more</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Part IV: Analysis of Big Variant Data</b></span></li>
<li class="chapter" data-level="27" data-path="bioinformatic-analysis-on-variant-data.html"><a href="bioinformatic-analysis-on-variant-data.html"><i class="fa fa-check"></i><b>27</b> Bioinformatic analysis on variant data</a></li>
<li class="part"><span><b>V Part V: Population Genomics</b></span></li>
<li class="chapter" data-level="28" data-path="topics-in-pop-gen.html"><a href="topics-in-pop-gen.html"><i class="fa fa-check"></i><b>28</b> Topics in pop gen</a><ul>
<li class="chapter" data-level="28.1" data-path="topics-in-pop-gen.html"><a href="topics-in-pop-gen.html#coalescent"><i class="fa fa-check"></i><b>28.1</b> Coalescent</a></li>
<li class="chapter" data-level="28.2" data-path="topics-in-pop-gen.html"><a href="topics-in-pop-gen.html#measures-of-genetic-diversity-and-such"><i class="fa fa-check"></i><b>28.2</b> Measures of genetic diversity and such</a></li>
<li class="chapter" data-level="28.3" data-path="topics-in-pop-gen.html"><a href="topics-in-pop-gen.html#demographic-inference-with-partial-a-partial-i-and-moments"><i class="fa fa-check"></i><b>28.3</b> Demographic inference with <span class="math inline">\(\partial a \partial i\)</span> and <em>moments</em></a></li>
<li class="chapter" data-level="28.4" data-path="topics-in-pop-gen.html"><a href="topics-in-pop-gen.html#balls-in-boxes"><i class="fa fa-check"></i><b>28.4</b> Balls in Boxes</a></li>
<li class="chapter" data-level="28.5" data-path="topics-in-pop-gen.html"><a href="topics-in-pop-gen.html#some-landscape-genetics"><i class="fa fa-check"></i><b>28.5</b> Some landscape genetics</a></li>
<li class="chapter" data-level="28.6" data-path="topics-in-pop-gen.html"><a href="topics-in-pop-gen.html#relationship-inference"><i class="fa fa-check"></i><b>28.6</b> Relationship Inference</a></li>
<li class="chapter" data-level="28.7" data-path="topics-in-pop-gen.html"><a href="topics-in-pop-gen.html#tests-for-selection"><i class="fa fa-check"></i><b>28.7</b> Tests for Selection</a></li>
<li class="chapter" data-level="28.8" data-path="topics-in-pop-gen.html"><a href="topics-in-pop-gen.html#multivariate-associations-gea-etc."><i class="fa fa-check"></i><b>28.8</b> Multivariate Associations, GEA, etc.</a></li>
<li class="chapter" data-level="28.9" data-path="topics-in-pop-gen.html"><a href="topics-in-pop-gen.html#estimating-heritability-in-the-wild"><i class="fa fa-check"></i><b>28.9</b> Estimating heritability in the wild</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Practical Computing and Bioinformatics for Conservation and Evolutionary Genomics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chap-HPCC" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> High Performance Computing Clusters (HPCC’s)</h1>
<p>One interesting aspect of modern next generation sequencing data is simply its
sheer size: it is not uncommon to receive a half or a full terabyte of data from
a sequencing center. It is impractical to store this much data on your
laptop, let alone analyze it there. Crunching through such a quantity of data on a single
computer or server could take a very long time. Instead, you will likely break
up the analysis of such data into a number of smaller jobs, and send them off to
run on an assortment of different computers in a High Performance Computing Cluster (HPCC).</p>
<p>Thus, even if you have immersed yourself in bioinformatic data file formats and honed
your skills at shell programming and accessing remote computers, sequence data analysis
remains such a formidable foe that there is still one last key area of computing
in which you must be fluent, in order to comfortably do bioinformatics: you must understand
how to submit and manage jobs sent to an HPCC.</p>
<p>My first experience with HPCC’s occurred when I started analyzing
high-throughput sequencer output. I had over 15 years experience in shell
programming at that time, and I was given some example analysis scripts to emulate, but I still
found it took several weeks before I was moderately comfortable in an HPCC environment. Most
HPCC’s have some sort of tutorial web pages that provide a little bit of background
on cluster computing, but I didn’t find the ones available to me, at the time, to
be particularly helpful.</p>
<p>The goal of this chapter is to provide the sort of background I wish that I had
when I started doing cluster computing for bioinformatics. I will not be providing
a comprehensive overview of parallel computation. For example, we will not focus at all
upon the rich tradition of parallel computing applications through “message passing” interfaces which
can maintain a synchronized analysis from a single program executing
on multiple computers at the same time. Rather, we will focus on the manner in which
most bioinformatic problems can be broken down into a series of smaller jobs,
each of which can be run, independently, on its own processor without the
need for maintaining synchrony between multiple processes.</p>
<p>We start with an overview of what an HPCC consists of, defining a few important
terms. Then we provide some background on the fundamental problem of cluster
computing: namely that a lot of people want to use the computing power of the cluster,
but it needs to be allocated to users in an equitable fashion. An understanding of this
forms the basis for our discussion of <em>job scheduling</em> and the methods you must
use to tell the <em>job scheduler</em> what resources you will need, so that those resources
will eventually be allocated to you. We will cover a job scheduler called SLURM, which
stands for the Simple Linux Utility for Resource Management. It is the scheduler used
on the Summit supercomputer in Boulder and the Hummingbird and Sedna clusters deployed
at UCSC and the NWFSC, respectively.</p>
<div id="an-oversimplified-but-useful-view-of-a-computing-cluster" class="section level2">
<h2><span class="header-section-number">8.1</span> An oversimplified, but useful, view of a computing cluster</h2>
<p>At its simplest, an HPCC can be thought of as a whole lot of computers that are all
put in a room somewhere for the purposes of doing a lot of computations. Most of these
computers do not possess all the elements that typically come to mind when you
think of a “computer.” For example, none of them are attached to monitors—each
computer resembles just the “box” part of a desktop computer. Each of these “boxes” is called
a <em>node</em>. The node is the unit in a cluster that corresponds most closely to what you think of
as a “computer.” As is typical of most computers today, each of these nodes has some
amount of Random Access Memory (RAM) that is <em>only accessible by the node itself</em>. RAM
is the space in memory that is used for active computation and calculation.</p>
<p>Each of these
nodes might also have a hard drive used for operating system software. The bulk of the
hard drive space that each node can access, however, is in the form of a large array of hard disks
that are connected to <em>all</em> of the nodes by an interface that allows data to be transferred
back and forth between each node and the hard-drive array at speeds that would be expected of
an internal hard drive (i.e., this array of hard drives is not just plugged in with a USB
cable). Memory on hard drives is used for storing data and the results of calculations, but
is not used for active calculations the way RAM is used. In order for calculations to be done
on data that are on the hard drive array, it must first be read into a node’s RAM. After the calculations
are done, the results are typically written back out onto the hard drive array.</p>
<p>On a typical cluster, there are usually several different “portions” of the hard drive
array attached to every <em>node</em>. One part of the array holds <em>home directories</em> of the
different users. Each user typically has a limited amount of storage in their home directory, but
the data in these home directories is usually safe or protected, meaning you can put a file there
and expect that it will be there next week, month, or year, etc. It is also likely that the home directories are
backed up (but check the docs for your cluster to confirm this!). Since the space in home directories is limited,
you typically will not put large data sets in your home directory for long-term storage, but you will
store scripts and programs, and such items as cloned GitHub repositories there. Another
type of storage that exists on the hard drive array is called <em>persistent long-term storage</em>. This type of storage
is purchased for use by research groups to store large quantities of
data on the cluster for long periods of time. As discussed in the last chapter, the rise of cloud-based storage solutions,
like Google Drive, offering unlimited storage to institutional users, makes persistent long-term storage less
important (and less cost-effective) for many research groups. Finally, the third type of
storage in the hard drive array is called <em>scratch storage</em>. There are usually fairly
light limits (if any at all) to how much data can be placed in scratch storage, but there
will typically be Draconian <em>time limits</em> placed on your scratch storage. For example, on the
Hoffman2 cluster at UCLA, you are granted 2 Tb of <code>scratch</code> storage, but any files that
have sat unmodified in <code>scratch</code> for more than 14 days will be deleted (and if space is tight, the
system administrators may delete things from <code>scratch</code> in far fewer then 14 days.) Check
your local cluster documentation for information about time and space limits on <code>scratch</code>.</p>
<p>On many clusters, scratch space is also configured to be very fast for input and output
(for example, on many systems, the scratch storage will be composed of solid state drives rather
than spinning hard disks). On jobs that require that a lot of data be accessed from the
drive or written to it (this includes most operations on BAM files), significant decreases
in overall running time can be seen by using fast storage. Finally, scratch space exists on a cluster
expressly as <em>the place</em> to put data and outputs on the hard drive <em>when running jobs</em>. For all these
reasons, when you run jobs, you will always want to read and write data from and to <code>scratch</code>. We
will talk more about the specifics of doing so, but for now, you should be developing a generic picture
of your cluster computing workflow that looks like:</p>
<ol style="list-style-type: decimal">
<li>Download big data files from the cloud to <code>scratch</code></li>
<li>Run analyses on those data files, writing output back to <code>scratch</code>.</li>
<li>When done, copy, to the cloud, any products that you wish to keep.</li>
<li>Remove data and outputs left on <code>scratch</code>.</li>
</ol>
<p>As is the case with virtually all modern desktop or laptop computers, within each node,
there are multiple (typically between 16 and 48) <em>cores</em>, which are the computer chip units that actually
do the computations within a node. A <em>serial job</em> is one that just runs on a single core
within a <em>node</em>, while a <em>parallel job</em> might run on multiple cores, at the same time, within
a single node. In such a parallel job, each core has access to the same data within the
node’s RAM (a “shared-memory,” parallel job). The <em>core</em>
is the fundamental unit of computing machinery that gets allocated to perform jobs in an HPCC.</p>
<p>Most of the nodes in a cluster are there to hold the cores which are the computational workhorses,
slogging through calculations for the HPCC’s myriad users. However, some nodes
are more appropriate to certain types of computations than others (for example, some
might have lots of memory for doing genome assembly, while others will have
big, hurkin’, graphical processing units to be used for GPU calculations). Or, some nodes
might be available preferentially for different users than for others. For these
reasons, nodes are grouped into different collections. Depending on the system you are
using, these collections of different nodes are called, either, <em>partitions</em> or <em>queues</em>.
The world of SLURM uses the term <em>partitions</em> for these collections, and we will adopt
that language as well, using <em>queue</em> to refer to the line of jobs that are waiting to start
on an HPCC; however we warn the reader that other job schedulers (like the Univa Grid Engine)
use the term “queues” to refer to collections of nodes.</p>
<p>On every cluster, however, there will be one to several nodes that are reserved not for
doing computation, but for allowing users to access the cluster. These are called the
<em>login</em> nodes or the <em>head</em> nodes. These nodes are <em>solely</em> for logging in, light editing of
scripts, minor manipulation of directories, and scheduling and managing jobs. They are absolutely <em>not</em>
for doing major computations. For example, you should never login to the head node and immediately
start using it, in an interactive <code>bash</code> session to, say, sort BAM files or run <code>bwa mem</code>
to do alignments. Running commands that require a lot of computation, or a lot of
input and output from the disk, on the login nodes is an egregious
<em>faux pas</em> of cluster computing. Doing so can
negatively impact the ability of other users to login or otherwise get their work done, and it
might lead the system administrators to disable your account. Therefore,
never do it! All of your hardcore computation on a cluster <em>has</em> to be done on
a <em>compute node</em>. We will show how to do that shortly, but first we will talk about why.</p>
</div>
<div id="cluster-computing-and-the-job-scheduler" class="section level2">
<h2><span class="header-section-number">8.2</span> Cluster computing and the job scheduler</h2>
<p>When you do work, or stream a video, or surf the web on your laptop computer, there are numerous
different computer processes running, to make sure that your computer keeps working and doing what it
is supposed to be doing. In the case of your laptop, the operating system, itself, orchestrates
all these different processes, making sure that each one is given some compute time on your
laptop’s processors in order to get its work done. Your laptop’s operating system has a fair bit
of flexibility in how it allocates resources to these different processes: it has multiple
cores to assign different processes to, <em>and</em> it
allows multiple processes to run on a single core, alternating between these different processes over
different <em>cycles</em> of the central processing unit. Things work differently
on a shared resource like an HPCC. The main, interesting problem of cluster computing is basically
this: lots of people want to use the cluster to run big jobs, but the cluster does not
run like a single computer.</p>
<p>The cluster is not happy to give lots of different jobs
from lots of different users a chance to all run on the same core, sharing time by dividing up cycles.
When a user wants to use the computational resources of an HPCC,
she cannot just start a job and be confident that it will launch immediately and be granted
at least a few CPU cycles every now and again. Rather, on an HPCC, every <em>job</em> that is submitted
by a user will be assigned to a dedicated core (or several cores, if requested, and granted)
with a dedicated amount of memory.
If a core is not available for use, the job “gets in line” (into the “queue”, so to speak)
where it sits and waits (doing nothing) until a core
(with sufficient associated resources, like RAM memory) becomes available. When such a core
becomes available in the cluster, the job gets launched on it. All of this is orchestrated by the job scheduler,
of which SLURM is an example.</p>
<p>In this computing model, a job, once it is launched, ties up the core and the memory
that has been allocated to it until the job is finished. While that job is running, no one
else’s jobs or processes can run on the core or share the RAM memory that was allocated to the job.
For this reason, the job scheduler, needs to know, <em>ahead of time</em>, how long each job might run and
what resources will be required during that time. A simple contrived example illustrates things easily:
imagine that Joe and Cheryl each have 1000 separate jobs to run. Each of Cheryl’s jobs involves running
a machine-learning algorithm to identify seabirds in high-resolution, aerial images of the ocean, and
takes only about 20 minutes running on a single core. Each of Joe’s jobs, on the other hand, involves mapping billions of
sequencing reads, a task which requires about 36 hours when run on a single core.
If their cluster has only 640 cores, and Joe submits his jobs first,
then, if the job scheduler were naive, it might put all of his jobs in line first, requiring some 50 or 60 hours
before the first of Cheryl’s jobs even runs. This would be a huge buzz kill for Cheryl.
However, if Cheryl and Joe both have to provide estimates to the scheduler of how
long their jobs will run, the scheduler can make more equitable decisions, starting a few of Joe’s jobs, but retaining
many more cores for Cheryl’s jobs, each of which runs much faster.</p>
<p>Thus, when you want to run any jobs on a cluster, you must provide an estimate of the resources
that the job will require. The three main axes upon which these resources are measured are:</p>
<ol style="list-style-type: decimal">
<li>The number of cores the job will require.</li>
<li>The maximum amount of RAM (memory) the job will require.</li>
<li>The amount of time for which the job will run.</li>
</ol>
<p>Requests for large amounts of resources for long periods of time generally take longer to start.
There are two main reasons for this: either 1) the scheduler does not want to launch too many
long-duration, high-memory jobs because it anticipates other users will want to use resources
down the road and no single user should tie up the compute resources
for too long; or 2) there are so many jobs running on myriad nodes and cores, that only
infrequently do nodes with sufficient numbers of cores and RAM come available to start
the new jobs.</p>
<p>The second reason is a particular bane of new cluster users who unwittingly request more resources
than actually exist (i.e. 52 cores, when no single node has more than 32; or 50 Gb of RAM when no single
node has more than 48 Gb). Unfortunately (or, perhaps comically, if you have a sick sense of
humor), some job schedulers will not notify you of this sort of transgression.
Rather, your job will just sit in line waiting to be launched, but it never will be, because sufficient
resources never become available!<br />
(Foruntately, however, the SLURM scheduler is “wise” enough to notify the user if
she or he has requested more resources than will ever become available.)</p>
<p>It is worth noting that regardless of whether reason 1 or reason 2 is the dominant cause influencing
how long it takes to start a job, asking for fewer resources for less time will generally allow your
jobs to start faster. Particularly because of reason #2, however, breaking your jobs down (if possible) into
small chunks that will run relatively quickly on a single core with low RAM needs can render many more
opportunities for your jobs to start, letting you tap into resources that are not often fully utilized
in a cluster. Since I started working on a large cluster in which it took a long time to start a job
that required all or most of the cores on a single node—but in which there were many nodes
harboring a few cores that were not being used—I tend to endorse the approach of breaking jobs
down into small units that will run relatively quickly, with low RAM requirements, on a
single core.</p>
<p>Since the requested resources for a job play such a large role in wait times for jobs to start,
you might wonder why people don’t intentionally underestimate the resources they request for their
jobs. The answer is simple: the job scheduler is a highly efficient and completely dispassionate
manager. If you requested 2 hours for your job, but your job has not finished in that amount of time,
the job scheduler will waste no time hemming and hawing or having an emotional struggle with itself
about whether it should stop your job. No, at 2 hours and 0.2 seconds it WILL kill your job, regardless
of whether it is just about to finish, or not. Similarly, if you requested 4 Gb of RAM, but five hours into
your job, the program you are running ends up using 5 Gb or RAM to store a large chunk of data, your
job WILL be killed, immediately.</p>
<p>Thus, it is best to be able to accurately estimate the time and resources a job will
require. You always want to request more time and resources than your job will
actually need, but not too much more. A large part of getting good at computing
in a shared cluster resource is gaining experience in predicting how long different jobs will
run, and how much RAM they will require. Later we will describe how the records of your
jobs, stored by the job scheduler, can be accessed and organized to aid in predicting
the resource demand of future jobs.</p>
</div>
<div id="slurm-info" class="section level2">
<h2><span class="header-section-number">8.3</span> Learning about the resources on your HPCC</h2>
<p>Most computing clusters have a web page that describes the configuration of the system
and the resources available on it. However, you can use various SLURM commands to
learn about those resources as well, and also to gain information about which
of the resources are in use and how many users are waiting to use them.</p>
<p>Requests for resources, and for <em>information</em> about the computing cluster, are made to the job scheduler
using a few basic commands. As said, we will focus on the commands available in a SLURM-based system.
In a later section, (after a discussion of installing software that you might need on your cluster)
we will more fully cover the commands used to <em>launch</em>, <em>schedule</em>, and manage jobs. Here
we will first explore the SLURM commands that
you can use to “get to know” your cluster.</p>
<p>All SLURM commands begin with an <code>s</code>, and all SLURM systems support the <code>sinfo</code> command
that gives you information about the cluster’s nodes and their status (whether they are currently
running jobs or not.) On your cluster, <code>man sinfo</code> will tell you about this command.
On the <code>Sedna</code> cluster, which just got installed and therefore does not
have many users, we see:</p>
<div class="sourceCode" id="cb288"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb288-1"><a href="chap-HPCC.html#cb288-1"></a><span class="ex">%</span> sinfo</span>
<span id="cb288-2"><a href="chap-HPCC.html#cb288-2"></a><span class="ex">PARTITION</span> AVAIL  TIMELIMIT  NODES  STATE NODELIST</span>
<span id="cb288-3"><a href="chap-HPCC.html#cb288-3"></a><span class="ex">nodes*</span>       up   infinite     28   idle node[01-28]</span>
<span id="cb288-4"><a href="chap-HPCC.html#cb288-4"></a><span class="ex">himem</span>        up   infinite      1  alloc himem01</span>
<span id="cb288-5"><a href="chap-HPCC.html#cb288-5"></a><span class="ex">himem</span>        up   infinite      2   idle himem[02-03]</span></code></pre></div>
<p>which tells us that there are two <em>partitions</em> (collection of nodes)
named <code>nodes</code> and <code>himem</code>. The <code>himem</code> partition has one node which is
currently allocated to a job (<code>STATE</code> = <code>alloc</code>), and two more that
are not currently allocated to jobs. The <code>himem</code> partition holds machines
with lots of RAM memory for tasks like sequence assembly (and, when I ran that
command, one of the nodes in <code>himem</code> was busy assembling the genome of
some interesting sea creature [need to ask Krista again what the hell it was…].
It also has 28 compute nodes in the
<code>nodes</code> partition that are free. This is a very small cluster.</p>
<p>If you do the same command on SUMMIT you get many more lines of output. There
are many more different partitions, and there is a lot of information about how many
nodes are in each:</p>
<div class="sourceCode" id="cb289"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb289-1"><a href="chap-HPCC.html#cb289-1"></a><span class="ex">%</span> sinfo</span>
<span id="cb289-2"><a href="chap-HPCC.html#cb289-2"></a><span class="ex">PARTITION</span>        AVAIL  TIMELIMIT  NODES  STATE NODELIST</span>
<span id="cb289-3"><a href="chap-HPCC.html#cb289-3"></a><span class="ex">shas*</span>               up 1-00:00:00      2 drain* shas[0136-0137]</span>
<span id="cb289-4"><a href="chap-HPCC.html#cb289-4"></a><span class="ex">shas*</span>               up 1-00:00:00      2  down* shas[0404,0506]</span>
<span id="cb289-5"><a href="chap-HPCC.html#cb289-5"></a><span class="ex">shas*</span>               up 1-00:00:00      1  drain shas0101</span>
<span id="cb289-6"><a href="chap-HPCC.html#cb289-6"></a><span class="ex">shas*</span>               up 1-00:00:00      3   resv shas[0102,0521,0853]</span>
<span id="cb289-7"><a href="chap-HPCC.html#cb289-7"></a><span class="ex">shas*</span>               up 1-00:00:00     74    mix shas[0125,0130,0133,0138,0141,0149,0156,0158,0218,0222,0229,0236-0237,0240-0241,0243,0246,0255,0303,0311,0314,0322,0335,0337,0341,0343,0351,0357,0402,0411,0413,0415-0416,0418,0423,0428,0432-0433,0435-0436,0440,0452,0455-0456,0459,0501,0504,0514,0522-0524,0526-0527,0556,0608,0611-0613,0615-0616,0631,0637,0801,0810-0811,0815,0834-0835,0850,0855,0907-0909,0921]</span>
<span id="cb289-8"><a href="chap-HPCC.html#cb289-8"></a><span class="ex">shas*</span>               up 1-00:00:00    359  alloc shas[0103-0124,0126-0129,0131-0132,0134-0135,0139-0140,0142-0148,0150-0155,0157,0159-0160,0201-0217,0219-0221,0223-0225,0227-0228,0230-0235,0238-0239,0242,0247-0254,0256-0260,0301-0302,0304-0310,0312-0313,0315-0321,0323-0334,0336,0338-0340,0342,0344-0350,0352-0356,0358-0360,0401,0405-0410,0412,0414,0417,0419-0422,0424-0427,0429-0431,0434,0437-0439,0442-0451,0453-0454,0457-0458,0460,0502-0503,0505,0507-0513,0515-0520,0525,0528-0555,0560,0605-0607,0609,0614,0617-0630,0632-0636,0638-0650,0652-0664,0802-0809,0812-0814,0816-0833,0836-0849,0851-0852,0854,0856-0860,0901-0906,0910-0913,0915-0920,0922-0932]</span>
<span id="cb289-9"><a href="chap-HPCC.html#cb289-9"></a><span class="ex">shas*</span>               up 1-00:00:00     11   idle shas[0226,0244-0245,0403,0441,0557-0559,0610,0651,0914]</span>
<span id="cb289-10"><a href="chap-HPCC.html#cb289-10"></a><span class="ex">shas-testing</span>        up   infinite      2 drain* shas[0136-0137]</span>
<span id="cb289-11"><a href="chap-HPCC.html#cb289-11"></a><span class="ex">shas-testing</span>        up   infinite      2  down* shas[0404,0506]</span>
<span id="cb289-12"><a href="chap-HPCC.html#cb289-12"></a><span class="ex">shas-testing</span>        up   infinite      1  drain shas0101</span>
<span id="cb289-13"><a href="chap-HPCC.html#cb289-13"></a><span class="ex">shas-testing</span>        up   infinite      3   resv shas[0102,0521,0853]</span>
<span id="cb289-14"><a href="chap-HPCC.html#cb289-14"></a><span class="ex">shas-testing</span>        up   infinite     74    mix shas[0125,0130,0133,0138,0141,0149,0156,0158,0218,0222,0229,0236-0237,0240-0241,0243,0246,0255,0303,0311,0314,0322,0335,0337,0341,0343,0351,0357,0402,0411,0413,0415-0416,0418,0423,0428,0432-0433,0435-0436,0440,0452,0455-0456,0459,0501,0504,0514,0522-0524,0526-0527,0556,0608,0611-0613,0615-0616,0631,0637,0801,0810-0811,0815,0834-0835,0850,0855,0907-0909,0921]</span>
<span id="cb289-15"><a href="chap-HPCC.html#cb289-15"></a><span class="ex">shas-testing</span>        up   infinite    359  alloc shas[0103-0124,0126-0129,0131-0132,0134-0135,0139-0140,0142-0148,0150-0155,0157,0159-0160,0201-0217,0219-0221,0223-0225,0227-0228,0230-0235,0238-0239,0242,0247-0254,0256-0260,0301-0302,0304-0310,0312-0313,0315-0321,0323-0334,0336,0338-0340,0342,0344-0350,0352-0356,0358-0360,0401,0405-0410,0412,0414,0417,0419-0422,0424-0427,0429-0431,0434,0437-0439,0442-0451,0453-0454,0457-0458,0460,0502-0503,0505,0507-0513,0515-0520,0525,0528-0555,0560,0605-0607,0609,0614,0617-0630,0632-0636,0638-0650,0652-0664,0802-0809,0812-0814,0816-0833,0836-0849,0851-0852,0854,0856-0860,0901-0906,0910-0913,0915-0920,0922-0932]</span>
<span id="cb289-16"><a href="chap-HPCC.html#cb289-16"></a><span class="ex">shas-testing</span>        up   infinite     11   idle shas[0226,0244-0245,0403,0441,0557-0559,0610,0651,0914]</span>
<span id="cb289-17"><a href="chap-HPCC.html#cb289-17"></a><span class="ex">shas-interactive</span>    up   infinite      2 drain* shas[0136-0137]</span>
<span id="cb289-18"><a href="chap-HPCC.html#cb289-18"></a><span class="ex">shas-interactive</span>    up   infinite      2  down* shas[0404,0506]</span>
<span id="cb289-19"><a href="chap-HPCC.html#cb289-19"></a><span class="ex">shas-interactive</span>    up   infinite      1  drain shas0101</span>
<span id="cb289-20"><a href="chap-HPCC.html#cb289-20"></a><span class="ex">shas-interactive</span>    up   infinite      3   resv shas[0102,0521,0853]</span>
<span id="cb289-21"><a href="chap-HPCC.html#cb289-21"></a><span class="ex">shas-interactive</span>    up   infinite     74    mix shas[0125,0130,0133,0138,0141,0149,0156,0158,0218,0222,0229,0236-0237,0240-0241,0243,0246,0255,0303,0311,0314,0322,0335,0337,0341,0343,0351,0357,0402,0411,0413,0415-0416,0418,0423,0428,0432-0433,0435-0436,0440,0452,0455-0456,0459,0501,0504,0514,0522-0524,0526-0527,0556,0608,0611-0613,0615-0616,0631,0637,0801,0810-0811,0815,0834-0835,0850,0855,0907-0909,0921]</span>
<span id="cb289-22"><a href="chap-HPCC.html#cb289-22"></a><span class="ex">shas-interactive</span>    up   infinite    359  alloc shas[0103-0124,0126-0129,0131-0132,0134-0135,0139-0140,0142-0148,0150-0155,0157,0159-0160,0201-0217,0219-0221,0223-0225,0227-0228,0230-0235,0238-0239,0242,0247-0254,0256-0260,0301-0302,0304-0310,0312-0313,0315-0321,0323-0334,0336,0338-0340,0342,0344-0350,0352-0356,0358-0360,0401,0405-0410,0412,0414,0417,0419-0422,0424-0427,0429-0431,0434,0437-0439,0442-0451,0453-0454,0457-0458,0460,0502-0503,0505,0507-0513,0515-0520,0525,0528-0555,0560,0605-0607,0609,0614,0617-0630,0632-0636,0638-0650,0652-0664,0802-0809,0812-0814,0816-0833,0836-0849,0851-0852,0854,0856-0860,0901-0906,0910-0913,0915-0920,0922-0932]</span>
<span id="cb289-23"><a href="chap-HPCC.html#cb289-23"></a><span class="ex">shas-interactive</span>    up   infinite     11   idle shas[0226,0244-0245,0403,0441,0557-0559,0610,0651,0914]</span>
<span id="cb289-24"><a href="chap-HPCC.html#cb289-24"></a><span class="ex">sgpu</span>                up 1-00:00:00      1   resv sgpu0501</span>
<span id="cb289-25"><a href="chap-HPCC.html#cb289-25"></a><span class="ex">sgpu</span>                up 1-00:00:00      1  alloc sgpu0502</span>
<span id="cb289-26"><a href="chap-HPCC.html#cb289-26"></a><span class="ex">sgpu</span>                up 1-00:00:00      9   idle sgpu[0101-0102,0201-0202,0301-0302,0401-0402,0801]</span>
<span id="cb289-27"><a href="chap-HPCC.html#cb289-27"></a><span class="ex">sgpu-testing</span>        up   infinite      1   resv sgpu0501</span>
<span id="cb289-28"><a href="chap-HPCC.html#cb289-28"></a><span class="ex">sgpu-testing</span>        up   infinite      1  alloc sgpu0502</span>
<span id="cb289-29"><a href="chap-HPCC.html#cb289-29"></a><span class="ex">sgpu-testing</span>        up   infinite      9   idle sgpu[0101-0102,0201-0202,0301-0302,0401-0402,0801]</span>
<span id="cb289-30"><a href="chap-HPCC.html#cb289-30"></a><span class="ex">sknl</span>                up 1-00:00:00      1  drain sknl0710</span>
<span id="cb289-31"><a href="chap-HPCC.html#cb289-31"></a><span class="ex">sknl</span>                up 1-00:00:00      1   resv sknl0706</span>
<span id="cb289-32"><a href="chap-HPCC.html#cb289-32"></a><span class="ex">sknl</span>                up 1-00:00:00     18  alloc sknl[0701-0705,0707-0709,0711-0720]</span>
<span id="cb289-33"><a href="chap-HPCC.html#cb289-33"></a><span class="ex">sknl-testing</span>        up   infinite      1  drain sknl0710</span>
<span id="cb289-34"><a href="chap-HPCC.html#cb289-34"></a><span class="ex">sknl-testing</span>        up   infinite      1   resv sknl0706</span>
<span id="cb289-35"><a href="chap-HPCC.html#cb289-35"></a><span class="ex">sknl-testing</span>        up   infinite     18  alloc sknl[0701-0705,0707-0709,0711-0720]</span>
<span id="cb289-36"><a href="chap-HPCC.html#cb289-36"></a><span class="ex">smem</span>                up 7-00:00:00      1   drng smem0201</span>
<span id="cb289-37"><a href="chap-HPCC.html#cb289-37"></a><span class="ex">smem</span>                up 7-00:00:00      4  alloc smem[0101,0301,0401,0501]</span>
<span id="cb289-38"><a href="chap-HPCC.html#cb289-38"></a><span class="ex">ssky</span>                up 1-00:00:00      1   drng ssky0944</span>
<span id="cb289-39"><a href="chap-HPCC.html#cb289-39"></a><span class="ex">ssky</span>                up 1-00:00:00      1    mix ssky0952</span>
<span id="cb289-40"><a href="chap-HPCC.html#cb289-40"></a><span class="ex">ssky</span>                up 1-00:00:00      3  alloc ssky[0942-0943,0951]</span>
<span id="cb289-41"><a href="chap-HPCC.html#cb289-41"></a><span class="ex">ssky-preemptable</span>    up 1-00:00:00      1   drng ssky0944</span>
<span id="cb289-42"><a href="chap-HPCC.html#cb289-42"></a><span class="ex">ssky-preemptable</span>    up 1-00:00:00      1    mix ssky0952</span>
<span id="cb289-43"><a href="chap-HPCC.html#cb289-43"></a><span class="ex">ssky-preemptable</span>    up 1-00:00:00      9  alloc ssky[0933-0934,0937-0940,0942-0943,0951]</span>
<span id="cb289-44"><a href="chap-HPCC.html#cb289-44"></a><span class="ex">ssky-preemptable</span>    up 1-00:00:00      9   idle ssky[0935-0936,0941,0945-0950]</span>
<span id="cb289-45"><a href="chap-HPCC.html#cb289-45"></a><span class="ex">ssky-ucb-aos</span>        up 7-00:00:00      6  alloc ssky[0933-0934,0937-0940]</span>
<span id="cb289-46"><a href="chap-HPCC.html#cb289-46"></a><span class="ex">ssky-ucb-aos</span>        up 7-00:00:00      3   idle ssky[0935-0936,0941]</span>
<span id="cb289-47"><a href="chap-HPCC.html#cb289-47"></a><span class="ex">ssky-csu-mbp</span>        up 7-00:00:00      1   idle ssky0945</span>
<span id="cb289-48"><a href="chap-HPCC.html#cb289-48"></a><span class="ex">ssky-csu-asb</span>        up 7-00:00:00      1   idle ssky0946</span>
<span id="cb289-49"><a href="chap-HPCC.html#cb289-49"></a><span class="ex">ssky-csu-rsp</span>        up 7-00:00:00      4   idle ssky[0947-0950]</span></code></pre></div>
<p>Yikes! That is a lot of info. The numbers that you see on the ends of the lines there
are node numbers.</p>
<p>If you wanted to see information about each node in the <code>shas</code> partition, you could
print long information for each node like this:</p>
<div class="sourceCode" id="cb290"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb290-1"><a href="chap-HPCC.html#cb290-1"></a><span class="ex">%</span> sinfo -l -N</span></code></pre></div>
<p>On Summit, that creates almost 1500 lines of output. Some nodes are listed several times
because they belong to different partitions. To look at results for just the standard compute
parition (<code>shas</code>: 380 nodes with Intel Xeon Haswell processors) you can use</p>
<div class="sourceCode" id="cb291"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb291-1"><a href="chap-HPCC.html#cb291-1"></a><span class="ex">%</span> sinfo -l -n -p shas</span></code></pre></div>
<p>Try that command. (Or try a similar command with an appropriate partition name on your
own cluster.) If you want to see explicitly how many cores are available vs allocated
on each node, how much total memory each node has, and how much of that total
memory is free, in that partition you can do:</p>
<div class="sourceCode" id="cb292"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb292-1"><a href="chap-HPCC.html#cb292-1"></a><span class="ex">%</span> sinfo -N -p shas -O nodelist,cpusstate,memory,allocmem,freemem</span></code></pre></div>
<p>The top part of the output from that command on SUMMIT looks like:</p>
<pre><code>NODELIST            CPUS(A/I/O/T)       MEMORY              ALLOCMEM            FREE_MEM            
shas0101            0/0/24/24           116368              0                   125156              
shas0102            0/24/0/24           116368              0                   112325              
shas0103            24/0/0/24           116368              116352              87228               
shas0104            24/0/0/24           116368              116352              80769  </code></pre>
<p>This says <code>shas0101</code> has 24 CPUs that are <em>Out</em> (not functional at this point).
<code>shas0102</code>, on the other hand, has 24 CPUs that are <em>Idle</em>, while <code>shas0103</code> has
24 CPUs that are allocated, and so forth. (In our parlance, here,
CPU is being used to mean “core”). All of the nodes have
116 Gb of memory total. Most of them have about that much memory allocated to
the jobs they are running.</p>
<p>We can throw down some <code>awk</code> to count the total number of available cores
(and the total number of all the cores):</p>
<div class="sourceCode" id="cb294"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb294-1"><a href="chap-HPCC.html#cb294-1"></a><span class="ex">%</span> sinfo -N -p shas -O  nodelist,cpusstate <span class="kw">|</span> <span class="fu">awk</span> -F<span class="st">&quot;/&quot;</span> <span class="st">&#39;{avail+=$2; tots+=$4} END {print &quot;Out of&quot;, tots, &quot;cores, there are&quot;, avail, &quot;available&quot;}&#39;</span> </span>
<span id="cb294-2"><a href="chap-HPCC.html#cb294-2"></a><span class="ex">Out</span> of 10848 cores, there are 331 available</span></code></pre></div>
<p>That could explain why it can be hard to get time on the supercomputer: at this time, only about 3% of the cores in the system
are idle, waiting for jobs to go on them.</p>
<p>If you want to see how many jobs are in line, waiting to be launched, you can use the
<code>squeue</code> command. Simply issuing the command <code>squeue</code> will give a (typically very long)
list of all jobs that are either currently running or are in the queue,
waiting to be launched. This is worth doing in order to see how many
different people are using (or waiting to use) the resources.</p>
<p>If we run the command on Sedna, we see that (like we saw before)
only one job is currently running:</p>
<div class="sourceCode" id="cb295"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb295-1"><a href="chap-HPCC.html#cb295-1"></a><span class="ex">%</span> squeue</span>
<span id="cb295-2"><a href="chap-HPCC.html#cb295-2"></a>             <span class="ex">JOBID</span> PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)</span>
<span id="cb295-3"><a href="chap-HPCC.html#cb295-3"></a>               <span class="ex">160</span>     himem MaSuRCA_   ggoetz  R 34-15:56:16      1 himem01</span></code></pre></div>
<p>Holy cow! Looking at the TIME column you can see that the genome assembly job
has been running for over 34 days! (The time format is DAYS-Hours:Minutes:Seconds).</p>
<p>The output can be filtered to just PENDING
or RUNNING jobs using the <code>-t</code> option. So,</p>
<div class="sourceCode" id="cb296"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb296-1"><a href="chap-HPCC.html#cb296-1"></a><span class="ex">squeue</span> -t PENDING</span></code></pre></div>
<p>lists all jobs waiting to be launched. If you wanted to see jobs that a particular
user (like yourself) has running or pending, you can use the <code>-u</code> option. For example,</p>
<div class="sourceCode" id="cb297"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb297-1"><a href="chap-HPCC.html#cb297-1"></a><span class="ex">squeue</span> -u eriq@colostate.edu</span></code></pre></div>
<p>In fact that is such a worthwhile command that it is worth creating an alias
for it by adding a line such as the following to your <code>~/.bashrc</code>:</p>
<div class="sourceCode" id="cb298"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb298-1"><a href="chap-HPCC.html#cb298-1"></a><span class="bu">alias</span> myjobs=<span class="st">&#39;squeue -u eriq@colostate.edu&#39;</span></span></code></pre></div>
<p>Then, typing <code>myjobs</code> shows all of the jobs that you have running, or waiting to be launched
on the cluster.</p>
</div>
<div id="getting-compute-resources-allocated-to-your-jobs-on-an-hpcc" class="section level2">
<h2><span class="header-section-number">8.4</span> Getting compute resources allocated to your jobs on an HPCC</h2>
<div id="interactive-sessions" class="section level3">
<h3><span class="header-section-number">8.4.1</span> Interactive sessions</h3>
<p>Although most of your serious computing on the cluster will occur in <em>batch</em> jobs
that get submitted to the scheduler and run without any further interaction from you, the user,
we will begin our foray into the SLURM job scheduling commands by getting an <em>interactive shell</em>
on a compute node. “Why?” you might ask? Well, before you launch any newly-written
script as a <em>batch</em> job
on the cluster, you really should run through the code, line by line, inside that script and ensure that
things work as they should. Since your script likely does some heavy computing, you can’t do this on the
head nodes (i.e., login nodes) of the cluster.</p>
<p>The system administrators of every different HPCC seem to have their own preferred (or required)
way of requesting an interactive shell on a compute node. Before you start doing work
on an HPCC, you should always read through the documents (usually available on a website
associated with the HPCC) to learn the accepted way of doing certain tasks (like requesting
an interactive shell). The following is a short survey of three different ways I have
seen for requesting an interactive session. None of these methods is universally applicable
to all HPCCs. (Again, read the documents for your own HPCC.)</p>
<p>One way to get an interactive shell on a compute node of a cluster is to request it with:</p>
<div class="sourceCode" id="cb299"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb299-1"><a href="chap-HPCC.html#cb299-1"></a><span class="ex">srun</span> --pty /bin/bash</span></code></pre></div>
<p>This tells slurm to run <code>bash</code> (which is typically found at <code>/bin/bash</code> on all Unix system) within
a pseudo-terminal (the <code>--pty</code> part). If you run this command you might be given a bash shell on a compute
node. This works on the Sedna cluster, but not on all others.</p>
<p>On the SUMMIT cluster at CU Boulder, there is
a different recommended way of requesting an interactive shell. The
sysadmins have written their own script to do it, called <code>sinteractive</code>.
So, you could do:</p>
<div class="sourceCode" id="cb300"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb300-1"><a href="chap-HPCC.html#cb300-1"></a><span class="ex">sinteractive</span></span></code></pre></div>
<p>On the Hummingbird cluster, at UCSC, it takes a few more steps. The webpage at <a href="https://www.hb.ucsc.edu/getting-started/">https://www.hb.ucsc.edu/getting-started/</a>
tells you how to do that. Go to that page and search for <code>Job Allocation – Interactive, Serial</code>. That takes you to
the instructions. Minimally, here is what it looks like to get one core
on a node on the Instruction partition, for 1 hour, with 1 Gigabyte of memory
allocated to you:</p>
<div class="sourceCode" id="cb301"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb301-1"><a href="chap-HPCC.html#cb301-1"></a>[<span class="ex">~</span>]<span class="ex">--%</span> salloc --partition=Instruction --nodes=1 --time=01:00:00 --mem=1G --cpus-per-task=1</span>
<span id="cb301-2"><a href="chap-HPCC.html#cb301-2"></a><span class="ex">salloc</span>: Granted job allocation 48130</span>
<span id="cb301-3"><a href="chap-HPCC.html#cb301-3"></a>[<span class="ex">~</span>]<span class="ex">--%</span> ssh <span class="va">$SLURM_NODELIST</span></span>
<span id="cb301-4"><a href="chap-HPCC.html#cb301-4"></a></span>
<span id="cb301-5"><a href="chap-HPCC.html#cb301-5"></a><span class="co"># check that you are on the right host.</span></span>
<span id="cb301-6"><a href="chap-HPCC.html#cb301-6"></a>[<span class="ex">~</span>]<span class="ex">--%</span> hostname</span>
<span id="cb301-7"><a href="chap-HPCC.html#cb301-7"></a><span class="ex">hbcomp-005.hbhpc.ucsc.edu</span></span></code></pre></div>
<p>Regardless of the method used to request an interactive shell, when the command
returns with a command prompt, you should always check the hostname to make sure
that you are not, inadvertently, still on the login node. Do that with the <code>hostname</code>
command:</p>
<div class="sourceCode" id="cb302"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb302-1"><a href="chap-HPCC.html#cb302-1"></a><span class="fu">hostname</span></span></code></pre></div>
<p>Or, to make it very easy to see which node you are logged into, you can change
the settings for your Unix command prompt to show you the name of the host you
are logged into (Section <a href="essential-unixlinux-terminal-knowledge.html#comm-prompt">4.2.2</a>). For example, to include in your
command prompt the short name of the computer you are logged into, and the current
working directory, separated by a <code>:</code> and a space, you could modify your .bashrc
file to define the <code>PS1</code> environment
variable like so:</p>
<div class="sourceCode" id="cb303"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb303-1"><a href="chap-HPCC.html#cb303-1"></a><span class="va">PS1=</span><span class="st">&#39;[\h: \W]--% &#39;</span></span></code></pre></div>
<p>Now, whenever you see your command prompt, it will indicate the name of the computer
you are logged into.</p>
<p>It is also worth using your alias <code>myjobs</code> (see the last part of Section <a href="chap-HPCC.html#slurm-info">8.3</a>
to learn how to make such an alias) to check that your interactive shell session is listed amongst
your running jobs.</p>
<p>Once you have obtained an interactive shell on a compute node by one of the above
methods, it should behave like any other shell you work on. You can give commands
with it and define shell variables, etc. In short, it is perfect for running through your
scripts, line by line, to make sure that they are working.</p>
<p>When you are done using your interactive shell, you should logout from it. This can be
done by typing <code>logout</code>, or by hitting the <code>d</code> key while holding down the <code>control</code> button
(i.e. <code>&lt;cntrl&gt;-d</code>). This returns the core that you were using to the pool of cores that can
be allocated to other users.</p>
<p>Different HPCCs have different settings for the default amount of time an interactive shell will be
granted to a user. On some systems, you can request a shell for a certain amount of time using the
<code>--time</code> option to the slurm command.</p>
</div>
<div id="slurm-batch" class="section level3">
<h3><span class="header-section-number">8.4.2</span> Batch jobs</h3>
<p>After you have tested your bioinformatic scripts in an interactive shell and are ready to
run them “for real,” you will want to launch the script in a non-interactive, or <em>batch</em> job.
Such a job is one that you submit to the job scheduler, and then, when resources for the job become available,
the scheduler will launch the job without any futher interaction from you. The job will run until
it completes (or fails, or runs out of time, etc.), all without you having to interact directly
with the computing cluster after submitting the job.</p>
<p>As you might expect, the SLURM command used to submit batch jobs is named (…wait for it!…) <code>sbatch</code>.<br />
The syntax for using <code>sbatch</code> is:</p>
<pre><code>sbatch sbatch_options script.sh script_arguments</code></pre>
<p>where:</p>
<ul>
<li><code>sbatch</code> is the command</li>
<li><code>sbatch_options</code> shows the place where different options to the <code>sbatch</code> command should
be placed. These options can be a number of things like <code>--time=00:10:00</code> and <code>--mem=4G</code>.
We will talk about <code>sbatch</code> options in more detail below.</li>
<li><code>script.sh</code> is the script that you have written that you want to run on the HPCC. We refer to this
as the <em>job script</em>. It does not
have to be named <code>script.sh</code>, but, since it is a shell script, it ought to have the <code>.sh</code> extension.
The contents of this script file effectively
define the <em>job</em> that you wish to run. It should be a shell script, and, on most SLURM systems
you need to be specific about what “flavor” of shell it should be run in. This is done using the
“shebang” line at the top of the script, like <code>#!/bin/bash</code> (see the paragraph immediately
<em>before</em> <a href="shell-programming.html#paragraph-before-shebang">5.2.1</a>). If you want to know what
path should come after the <code>#!</code> (the “shebang”), you can give the command <code>which bash</code> from the shell
on one of your cluster’s login nodes (or an interactive shell on a compute node), and the
shell should respond with the absolute path to the bash shell interpreter.</li>
<li><code>script_arguments</code> are any other options, or other arguments (like filenames) that you
may want to pass to the script.</li>
</ul>
<p>For practice in submitting batch jobs, we will, first write a simple job script that
prints a few pieces of information about the shell that is running and the environment
variables defined within that shell.</p>
<p>So, first, change into your scratch directory
and make a new directory called <code>sbatch-play</code> and <code>cd</code> into it. Then, using a text editor,
copy the following lines to a shell script called <code>easy.sh</code> and save it within
your current working directory (<code>sbatch-play</code>). Note that the first line might
have to be modified for your system, depending on the output of <code>which bash</code>, as mentioned
above:</p>
<div class="sourceCode" id="cb305"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb305-1"><a href="chap-HPCC.html#cb305-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb305-2"><a href="chap-HPCC.html#cb305-2"></a></span>
<span id="cb305-3"><a href="chap-HPCC.html#cb305-3"></a><span class="bu">echo</span> <span class="st">&quot;Starting at </span><span class="va">$(</span><span class="fu">date</span><span class="va">)</span><span class="st">&quot;</span></span>
<span id="cb305-4"><a href="chap-HPCC.html#cb305-4"></a><span class="bu">echo</span> </span>
<span id="cb305-5"><a href="chap-HPCC.html#cb305-5"></a><span class="bu">echo</span> <span class="st">&quot;Current working directory is: </span><span class="va">$PWD</span><span class="st">&quot;</span></span>
<span id="cb305-6"><a href="chap-HPCC.html#cb305-6"></a><span class="bu">echo</span> <span class="st">&quot;Current user is: </span><span class="va">$USER</span><span class="st">&quot;</span></span>
<span id="cb305-7"><a href="chap-HPCC.html#cb305-7"></a><span class="bu">echo</span> <span class="st">&quot;Current hostname is: </span><span class="va">$(</span><span class="fu">hostname</span><span class="va">)</span><span class="st">&quot;</span></span>
<span id="cb305-8"><a href="chap-HPCC.html#cb305-8"></a><span class="bu">echo</span></span>
<span id="cb305-9"><a href="chap-HPCC.html#cb305-9"></a><span class="bu">echo</span> <span class="st">&quot;Currently, the PATH is set to: </span><span class="va">$PATH</span><span class="st">&quot;</span></span>
<span id="cb305-10"><a href="chap-HPCC.html#cb305-10"></a><span class="bu">echo</span></span>
<span id="cb305-11"><a href="chap-HPCC.html#cb305-11"></a></span>
<span id="cb305-12"><a href="chap-HPCC.html#cb305-12"></a><span class="co"># now, let&#39;s print the states of all the the environment variables</span></span>
<span id="cb305-13"><a href="chap-HPCC.html#cb305-13"></a><span class="co"># and send them to stderr:</span></span>
<span id="cb305-14"><a href="chap-HPCC.html#cb305-14"></a><span class="fu">env</span> <span class="op">&gt;</span> /dev/stderr</span>
<span id="cb305-15"><a href="chap-HPCC.html#cb305-15"></a></span>
<span id="cb305-16"><a href="chap-HPCC.html#cb305-16"></a><span class="fu">sleep</span> 180</span>
<span id="cb305-17"><a href="chap-HPCC.html#cb305-17"></a><span class="bu">echo</span> <span class="st">&quot;Done at </span><span class="va">$(</span><span class="fu">date</span><span class="va">)</span><span class="st">&quot;</span></span></code></pre></div>
<p>Once you have made that file, schedule it to run, telling the scheduler
to put it in the queue to run, <em>and</em> that you are asking for only
5 minutes of computer time to run it, and to send <em>stdout</em> from the
job to a file named <code>output-XXXXX</code>, and <em>stderr</em> to a file named
<code>error-XXXXX</code>, where <code>XXXXX</code> is the job number that was assigned to your job.
Job numbers get assigned in series by the job scheduler, so if the number
is 4373238, then yours is the 4,373,238-th job that has been requested
on the cluster.</p>
<div class="sourceCode" id="cb306"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb306-1"><a href="chap-HPCC.html#cb306-1"></a><span class="ex">sbatch</span> --time=00:05:00 --output=output-%j --error=error-%j  easy.sh</span></code></pre></div>
<p>You can check with your <code>myjobs</code> alias to see if the job has been scheduled
and/or if it is running. Once it is running, you should see two new files
in your current working directory <code>output-XXXXX</code> and <code>error-XXXXX</code>.</p>
<p>First, check the <code>output-XXXXX</code> file, using the <code>cat</code> command. You should
see that the current working directory printed by the <code>easy.sh</code> script
is the same as the current working directory from which you scheduled
the job. Additionally, the PATH variable accessed inside the <code>easy.sh</code> script
is the same as the PATH variable in the shell that you scheduled
the job from. You can check this by issuing:</p>
<pre><code>echo $PATH</code></pre>
<p>in the shell from which you scheduled the job.</p>
<p>The take-home message from this little exploration is that the shell
in which your job script, <code>easy.sh</code>, gets executed, inherits some of the important environment
variables (PATH, PWD, etc.) that are in effect in the shell from which you
scheduled the job.</p>
<p>However, a number of new environment variables have also been defined, most of them
supplying extra information from SLURM itself. To see that, we can compare the
file <code>error-XXXXX</code> (which holds information about all the environment variables
in effect when the job scheduler launched <code>easy.sh</code>) to the output of the
<code>env</code> command (which prints the values of all the environment variables) when we
invoke it in the shell from which the job was scheduled. Try doing that like this:</p>
<div class="sourceCode" id="cb308"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb308-1"><a href="chap-HPCC.html#cb308-1"></a><span class="co"># first, make a file of the environment variables in effect in the</span></span>
<span id="cb308-2"><a href="chap-HPCC.html#cb308-2"></a><span class="co"># shell the job was scheduled from.  Sort these alphabetically to</span></span>
<span id="cb308-3"><a href="chap-HPCC.html#cb308-3"></a><span class="co"># make it easier to compare between the two environments</span></span>
<span id="cb308-4"><a href="chap-HPCC.html#cb308-4"></a><span class="fu">env</span> <span class="kw">|</span> <span class="fu">sort</span> <span class="op">&gt;</span> sheduling_env.txt</span>
<span id="cb308-5"><a href="chap-HPCC.html#cb308-5"></a></span>
<span id="cb308-6"><a href="chap-HPCC.html#cb308-6"></a><span class="co"># also, sort the error-XXXXX file</span></span>
<span id="cb308-7"><a href="chap-HPCC.html#cb308-7"></a><span class="fu">sort</span> error-XXXXX <span class="op">&gt;</span> slurm_running_env.txt</span>
<span id="cb308-8"><a href="chap-HPCC.html#cb308-8"></a></span>
<span id="cb308-9"><a href="chap-HPCC.html#cb308-9"></a><span class="co"># then use less to compare sched_env.txt and error-XXXXX.</span></span>
<span id="cb308-10"><a href="chap-HPCC.html#cb308-10"></a></span>
<span id="cb308-11"><a href="chap-HPCC.html#cb308-11"></a><span class="co"># The adventurous can use tmux to split their screen vertically into </span></span>
<span id="cb308-12"><a href="chap-HPCC.html#cb308-12"></a><span class="co"># two panes to make this easier:</span></span></code></pre></div>
<p>Sorting the files, as we have done above, garbles up some multi-line bash functions, but
still makes it easy to see how the files differ. In particular, the
large block of environment variables that start with <code>SLURM_</code> give information
about the job resources and job numbers, etc. These can come in handy, though
we will end up being more concerned with some of those variables when we
start using SLURM <em>job arrays</em>.</p>
<div id="the-sbatch---test-only-option" class="section level4">
<h4><span class="header-section-number">8.4.2.1</span> The <code>sbatch --test-only</code> option</h4>
<p>Especially when you start using an HPCC, scheduling jobs can be a confusing and somewhat
daunting process. I used to always find my blood pressure going up when I submitted large
jobs. Questions swirled in my head: “Will my script run? How long will I have to wait
for my job to launch? Have I requested more resources than are available on the cluster
such that my job will never launch?”, etc. SLURM provides the <code>--test-only</code> option to
<code>sbatch</code> that can be helpful in this regard. When <code>sbatch</code> is run with the <code>--test-only</code> option,
the scheduler verifies that your script looks like a shell script, and then gives you information
about how many cores in how many nodes and in which partition the job will be scheduled for.
Try it like this:</p>
<div class="sourceCode" id="cb309"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb309-1"><a href="chap-HPCC.html#cb309-1"></a><span class="ex">sbatch</span> --test-only --time=00:05:00 --output=output-%j --error=error-%j  easy.sh</span>
<span id="cb309-2"><a href="chap-HPCC.html#cb309-2"></a></span>
<span id="cb309-3"><a href="chap-HPCC.html#cb309-3"></a><span class="co"># then see what happens if you request more than 24 hours:</span></span>
<span id="cb309-4"><a href="chap-HPCC.html#cb309-4"></a><span class="ex">sbatch</span> --test-only --time=24:05:00 --output=output-%j --error=error-%j  easy.sh</span></code></pre></div>
<p>As the name of the option implies, when you are using <code>--test-only</code>, your job <em>does not</em>
actually get scheduled.
<code>sbatch</code> with the <code>--test-only</code> option also gives an estimate of the time
when your job would be launched. This estimate seems to
be completely uninformative. It might tell you that it estimates your job would launch in
3 hours, but it could be much faster than that. One absolutely critical piece of information the
<code>--test-only</code> option will give you is a nice big warning if you are requesting more resources than
are available to you. For example, on the <code>shas</code> partition on SUMMIT, job run lengths are
capped at 24 hours, so you can’t request more than that amount of time. You can always use the <code>--test-only</code> option to
ensure that you are not requesting more resources than are available.</p>
<p>We will note here that the <code>--test-only</code> option is somewhat akin to <code>rclone</code>’s <code>--dry-run</code>
option, which lets you see what files would be copied, without actually copying them. This
sort of option is always a nice feature when you want to check that you have set your requests
up properly before committing computing or network resources to them.</p>
</div>
<div id="get-messages-from-slurm" class="section level4">
<h4><span class="header-section-number">8.4.2.2</span> Get messages from SLURM</h4>
<p>If you want to be notified by email when a job starts and finishes, (or fails
or gets killed by the scheduler). You can use the <code>--mail-user</code> option to supply
an email address and the <code>--mail-type</code> option to tell the scheduler when it should
email you with updates regarding your job status. For example, supply your own
email address in the following, and try it:</p>
<div class="sourceCode" id="cb310"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb310-1"><a href="chap-HPCC.html#cb310-1"></a><span class="ex">sbatch</span> --mail-type=ALL --mail-user=yourname@yourmail.edu --time=00:05:00 --output=output-%j --error=error-%j  easy.sh</span></code></pre></div>
<p>You should receive an email from the cluster when your job starts and completes.</p>
<p>This can be a handy feature when you are not running too many jobs. If you have a large number
of jobs, being notified
every time a job starts and finishes can quickly fill up your inbox. In those cases, consider
setting <code>--mail-type=FAIL</code> to only be notified when jobs have failed.</p>
</div>
<div id="include-options-inside-your-job-script" class="section level4">
<h4><span class="header-section-number">8.4.2.3</span> Include options <em>inside</em> your job script</h4>
<p>You might note
that the above command line is getting quite long and cumbersome, and it would surely become
very difficult to remember and type commands with even more options, every time you wanted to start a job.
On top of that, the options that you would want to invoke are typically going to be specific to the job
script that you are launching (i.e. <code>easy.sh</code> in the above example). Consequently, SLURM
let’s you specify the options passed to <code>sbatch</code> <em>inside the script file itself</em>. This leads
to much simpler commands on the command line, and makes your workflows somewhat more reproducible.
To include the <code>sbatch</code> options in the job script, you add them below the “shebang” line, in lines
that start with <code>#SBATCH</code> before any other commands in the script.</p>
<p>For example, we could make a new file called <code>easy2.sh</code> like this:</p>
<div class="sourceCode" id="cb311"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb311-1"><a href="chap-HPCC.html#cb311-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb311-2"><a href="chap-HPCC.html#cb311-2"></a></span>
<span id="cb311-3"><a href="chap-HPCC.html#cb311-3"></a><span class="co">#SBATCH --time=00:05:00</span></span>
<span id="cb311-4"><a href="chap-HPCC.html#cb311-4"></a><span class="co">#SBATCH --output=output-%j</span></span>
<span id="cb311-5"><a href="chap-HPCC.html#cb311-5"></a><span class="co">#SBATCH --error=error-%j </span></span>
<span id="cb311-6"><a href="chap-HPCC.html#cb311-6"></a></span>
<span id="cb311-7"><a href="chap-HPCC.html#cb311-7"></a></span>
<span id="cb311-8"><a href="chap-HPCC.html#cb311-8"></a><span class="bu">echo</span> <span class="st">&quot;Starting at </span><span class="va">$(</span><span class="fu">date</span><span class="va">)</span><span class="st">&quot;</span></span>
<span id="cb311-9"><a href="chap-HPCC.html#cb311-9"></a><span class="bu">echo</span> </span>
<span id="cb311-10"><a href="chap-HPCC.html#cb311-10"></a><span class="bu">echo</span> <span class="st">&quot;Current working directory is: </span><span class="va">$PWD</span><span class="st">&quot;</span></span>
<span id="cb311-11"><a href="chap-HPCC.html#cb311-11"></a><span class="bu">echo</span> <span class="st">&quot;Current user is: </span><span class="va">$USER</span><span class="st">&quot;</span></span>
<span id="cb311-12"><a href="chap-HPCC.html#cb311-12"></a><span class="bu">echo</span> <span class="st">&quot;Current hostname is: </span><span class="va">$(</span><span class="fu">hostname</span><span class="va">)</span><span class="st">&quot;</span></span>
<span id="cb311-13"><a href="chap-HPCC.html#cb311-13"></a><span class="bu">echo</span></span>
<span id="cb311-14"><a href="chap-HPCC.html#cb311-14"></a><span class="bu">echo</span> <span class="st">&quot;Currently, the PATH is set to: </span><span class="va">$PATH</span><span class="st">&quot;</span></span>
<span id="cb311-15"><a href="chap-HPCC.html#cb311-15"></a><span class="bu">echo</span></span>
<span id="cb311-16"><a href="chap-HPCC.html#cb311-16"></a></span>
<span id="cb311-17"><a href="chap-HPCC.html#cb311-17"></a><span class="co"># now, let&#39;s print the states of all the the environment variables</span></span>
<span id="cb311-18"><a href="chap-HPCC.html#cb311-18"></a><span class="co"># and send them to stderr:</span></span>
<span id="cb311-19"><a href="chap-HPCC.html#cb311-19"></a><span class="fu">env</span> <span class="op">&gt;</span> /dev/stderr</span>
<span id="cb311-20"><a href="chap-HPCC.html#cb311-20"></a></span>
<span id="cb311-21"><a href="chap-HPCC.html#cb311-21"></a><span class="fu">sleep</span> 180</span></code></pre></div>
<p>And we could then test it like this:</p>
<div class="sourceCode" id="cb312"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb312-1"><a href="chap-HPCC.html#cb312-1"></a><span class="ex">sbatch</span> --test-only easy2.sh</span></code></pre></div>
<p>or schedule it like this:</p>
<div class="sourceCode" id="cb313"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb313-1"><a href="chap-HPCC.html#cb313-1"></a><span class="ex">sbatch</span> easy2.sh</span></code></pre></div>
<p>As you might imagine, <code>sbatch</code> can take many different options.
We list the main ones that you will
be concerned with while doing bioinformatics in Table <a href="chap-HPCC.html#tab:sbatchopts">8.1</a>.</p>
<table style="width:96%;">
<caption><span id="tab:sbatchopts">TABLE 8.1: </span> Commonly used options to <code>sbatch</code>.</caption>
<colgroup>
<col width="47%" />
<col width="48%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Option</th>
<th align="left">What it does</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><code>--cpus-per-task=&lt;ncpus&gt;</code></td>
<td align="left">Indicate that ncpus cores are
needed for each task in the
job</td>
</tr>
<tr class="even">
<td align="left"><code>--chdir=&lt;directory&gt;</code></td>
<td align="left">Set the working directory of
the batch script to
“directory” before the script
is run. This is seldom used
if you schedule batch scripts
from the directory in which
they should be run.</td>
</tr>
<tr class="odd">
<td align="left"><code>--error=&lt;filename pattern&gt;</code></td>
<td align="left">Path to which <em>stderr</em> should
be written. Like the
<code>--output</code> option this llows
for <code>%j</code>, <code>%A</code>, and <code>%a</code> (see
below).</td>
</tr>
<tr class="even">
<td align="left"><code>--job-name=&lt;jobname&gt;</code></td>
<td align="left">Assign a name to the batch
job. This name then appears
in, for example, <code>squeve -u user</code>. It should be <em>short</em>
and descriptive.</td>
</tr>
<tr class="odd">
<td align="left"><code>--mail-type=&lt;type&gt;</code></td>
<td align="left">Upon which job-scheduling
events for this job should
email be sent to the user?
Choices include NONE, BEGIN,
END, FAIL, REQUEUE, ALL.</td>
</tr>
<tr class="even">
<td align="left"><code>--mail-user=&lt;user&gt;</code></td>
<td align="left">Address to which notification
emails should be sent.</td>
</tr>
<tr class="odd">
<td align="left"><code>--mem=&lt;size[units]&gt;</code></td>
<td align="left">How much memory is requested
<em>per node</em>. Most easily
specified in gigabytes with a
trailing as in <code>4G</code>. For
example, <code>--mem=4G</code>.</td>
</tr>
<tr class="even">
<td align="left"><code>--mem-per-cpu=&lt;size[units]&gt;</code></td>
<td align="left">How much memory is requested
<em>per core</em>. For example,
<code>--mem-per-cpu=4.6G</code>. Only
one of <code>--mem</code> or
<code>--mem-per-cpu</code> should be
given.</td>
</tr>
<tr class="odd">
<td align="left"><code>--output=&lt;filename pattern&gt;</code></td>
<td align="left">Path to which <em>stdout</em> should
be written while executing the
batch script. Path can include
<code>%j</code>, which expands to the job
ID, or, if running as a job
array, <code>%A</code> expands to the job
ID, and <code>%a</code> to the job array
index.</td>
</tr>
<tr class="even">
<td align="left"><code>--partition=&lt;partition_names&gt;</code></td>
<td align="left">Request an allocation of
resources from a compute node
in one of the partitions
listed in <code>partition_names</code>,
which is a comma-separated
list of partitions. For
example,
<code>--partition=shas,compute,himem</code></td>
</tr>
<tr class="odd">
<td align="left"><code>--time=&lt;time&gt;</code></td>
<td align="left">Request an allocation of a
certain amount of time.
Specified in
<code>days-hours:minutes:seconds</code>
or <code>hours:minutes:seconds</code>,
for example:
<code>--time=2-12:00:00</code> or
<code>--time=05:00:00</code>. Jobs
running longer than this time
will be killed.</td>
</tr>
<tr class="even">
<td align="left"><code>--test-only</code></td>
<td align="left">Usually given directly on the
command line to verify the job
submission is compliant
without actually scheduling
the job.</td>
</tr>
</tbody>
</table>
</div>
<div id="override-within-file-sbatch-directives-from-the-command-line" class="section level4">
<h4><span class="header-section-number">8.4.2.4</span> Override within-file <code>sbatch</code> directives from the command line</h4>
<p>Since you can write options for the <code>sbatch</code> command either 1) on the command
line (as in <code>sbatch --time=00:03:00 ...</code>), or 2) within the script file given to
the <code>sbatch</code> command (i.e. lines like <code>#SBATCH --time=00:05:00</code> within
the top part of the script), you might wonder what happens when the same
option is supplied both on the command line and within the script.
It turns out that the version of the option on the command line takes precedence.
That it, any options that are given <em>on the command line</em> itself will
override the options if they are given within the file. For example,
if we submitted the above <code>easy2.sh</code> script with the command:</p>
<div class="sourceCode" id="cb314"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb314-1"><a href="chap-HPCC.html#cb314-1"></a><span class="ex">sbatch</span> --time=01:00:00 easy2.sh</span></code></pre></div>
<p>it would be submitted with a time limit of 1 hour (<code>01:00:00</code>) rather
than of 5 minutes (<code>00:05:00</code>, as given within the file.)</p>
<p>This feature is particularly useful when you find yourself needing
to re-run just certain elements of a SLURM job array, as we will see in a few sections.</p>
</div>
<div id="the-vagaries-of-conda-within-sbatch" class="section level4">
<h4><span class="header-section-number">8.4.2.5</span> The vagaries of <code>conda</code> within <code>sbatch</code></h4>
<p>As we noted above, the shell environment in which SLURM executes
your scheduled job inherits many important variables (like PATH and
the current working directory) from the shell that you submit the job from.
One might hope that this would be sufficient to allow you to activate
a conda environment from within your job script. This turns out not to be
the case: even though several <code>conda</code> environment variables are set up
and the path to <code>conda</code> is set, there are some <code>conda</code> initialization
features that have to happen within your job script in order to be able to
explicitly set your <code>conda</code> environment within your job script.</p>
<p>To see this, first make a script called <code>conda-test1.sh</code> that
tries to activate a different <code>conda</code> environment within the job script:</p>
<div class="sourceCode" id="cb315"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb315-1"><a href="chap-HPCC.html#cb315-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb315-2"><a href="chap-HPCC.html#cb315-2"></a></span>
<span id="cb315-3"><a href="chap-HPCC.html#cb315-3"></a><span class="co">#SBATCH --time=00:03:00</span></span>
<span id="cb315-4"><a href="chap-HPCC.html#cb315-4"></a><span class="co">#SBATCH --output=conda-test1-output-%j</span></span>
<span id="cb315-5"><a href="chap-HPCC.html#cb315-5"></a><span class="co">#SBATCH --error=conda-test1-error-%j </span></span>
<span id="cb315-6"><a href="chap-HPCC.html#cb315-6"></a></span>
<span id="cb315-7"><a href="chap-HPCC.html#cb315-7"></a></span>
<span id="cb315-8"><a href="chap-HPCC.html#cb315-8"></a><span class="bu">echo</span> <span class="st">&quot;Starting at </span><span class="va">$(</span><span class="fu">date</span><span class="va">)</span><span class="st">&quot;</span></span>
<span id="cb315-9"><a href="chap-HPCC.html#cb315-9"></a><span class="bu">echo</span> </span>
<span id="cb315-10"><a href="chap-HPCC.html#cb315-10"></a><span class="bu">echo</span> <span class="st">&quot;Current working directory is: </span><span class="va">$PWD</span><span class="st">&quot;</span></span>
<span id="cb315-11"><a href="chap-HPCC.html#cb315-11"></a><span class="bu">echo</span> <span class="st">&quot;Current user is: </span><span class="va">$USER</span><span class="st">&quot;</span></span>
<span id="cb315-12"><a href="chap-HPCC.html#cb315-12"></a><span class="bu">echo</span> <span class="st">&quot;Current hostname is: </span><span class="va">$(</span><span class="fu">hostname</span><span class="va">)</span><span class="st">&quot;</span></span>
<span id="cb315-13"><a href="chap-HPCC.html#cb315-13"></a><span class="bu">echo</span></span>
<span id="cb315-14"><a href="chap-HPCC.html#cb315-14"></a><span class="bu">echo</span> <span class="st">&quot;Currently, the PATH is set to: </span><span class="va">$PATH</span><span class="st">&quot;</span></span>
<span id="cb315-15"><a href="chap-HPCC.html#cb315-15"></a><span class="bu">echo</span></span>
<span id="cb315-16"><a href="chap-HPCC.html#cb315-16"></a><span class="bu">echo</span> <span class="st">&quot;Activating environment bioinf&quot;</span></span>
<span id="cb315-17"><a href="chap-HPCC.html#cb315-17"></a><span class="ex">conda</span> activate bioinf</span>
<span id="cb315-18"><a href="chap-HPCC.html#cb315-18"></a><span class="bu">echo</span></span>
<span id="cb315-19"><a href="chap-HPCC.html#cb315-19"></a><span class="bu">echo</span> <span class="st">&quot;Now the PATH is: </span><span class="va">$PATH</span><span class="st">&quot;</span></span>
<span id="cb315-20"><a href="chap-HPCC.html#cb315-20"></a></span>
<span id="cb315-21"><a href="chap-HPCC.html#cb315-21"></a><span class="fu">sleep</span> 120</span></code></pre></div>
<p>and then run that with:</p>
<div class="sourceCode" id="cb316"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb316-1"><a href="chap-HPCC.html#cb316-1"></a><span class="ex">sbatch</span> conda-test1.sh</span></code></pre></div>
<p>After that has run, reading the contents of the <code>conda-test1-error-XXXX</code> file you should
see an error about not being able to activate a conda environment. Likewise,
if you look at the two <code>PATH</code>s printed out in <code>conda-test1-output-XXXX</code>,
you will see that they are the same. So, indeed, the <code>bioinf</code> environemnt
was not activated.</p>
<p>To remedy this problem, you need to source your <code>~/.bashrc</code>
(which seems to be where Miniconda3 establishes its “conda init” block)
from within your job script.</p>
<p>For example, create a new file called <code>conda-init2.sh</code> and fill it with
the following:</p>
<div class="sourceCode" id="cb317"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb317-1"><a href="chap-HPCC.html#cb317-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb317-2"><a href="chap-HPCC.html#cb317-2"></a></span>
<span id="cb317-3"><a href="chap-HPCC.html#cb317-3"></a><span class="co">#SBATCH --time=00:03:00</span></span>
<span id="cb317-4"><a href="chap-HPCC.html#cb317-4"></a><span class="co">#SBATCH --output=conda-test2-output-%j</span></span>
<span id="cb317-5"><a href="chap-HPCC.html#cb317-5"></a><span class="co">#SBATCH --error=conda-test2-error-%j </span></span>
<span id="cb317-6"><a href="chap-HPCC.html#cb317-6"></a></span>
<span id="cb317-7"><a href="chap-HPCC.html#cb317-7"></a><span class="bu">source</span> ~/.bashrc</span>
<span id="cb317-8"><a href="chap-HPCC.html#cb317-8"></a></span>
<span id="cb317-9"><a href="chap-HPCC.html#cb317-9"></a><span class="bu">echo</span> <span class="st">&quot;Starting at </span><span class="va">$(</span><span class="fu">date</span><span class="va">)</span><span class="st">&quot;</span></span>
<span id="cb317-10"><a href="chap-HPCC.html#cb317-10"></a><span class="bu">echo</span> </span>
<span id="cb317-11"><a href="chap-HPCC.html#cb317-11"></a><span class="bu">echo</span> <span class="st">&quot;Current working directory is: </span><span class="va">$PWD</span><span class="st">&quot;</span></span>
<span id="cb317-12"><a href="chap-HPCC.html#cb317-12"></a><span class="bu">echo</span> <span class="st">&quot;Current user is: </span><span class="va">$USER</span><span class="st">&quot;</span></span>
<span id="cb317-13"><a href="chap-HPCC.html#cb317-13"></a><span class="bu">echo</span> <span class="st">&quot;Current hostname is: </span><span class="va">$(</span><span class="fu">hostname</span><span class="va">)</span><span class="st">&quot;</span></span>
<span id="cb317-14"><a href="chap-HPCC.html#cb317-14"></a><span class="bu">echo</span></span>
<span id="cb317-15"><a href="chap-HPCC.html#cb317-15"></a><span class="bu">echo</span> <span class="st">&quot;Currently, the PATH is set to: </span><span class="va">$PATH</span><span class="st">&quot;</span></span>
<span id="cb317-16"><a href="chap-HPCC.html#cb317-16"></a><span class="bu">echo</span></span>
<span id="cb317-17"><a href="chap-HPCC.html#cb317-17"></a><span class="bu">echo</span> <span class="st">&quot;Activating environment bioinf&quot;</span></span>
<span id="cb317-18"><a href="chap-HPCC.html#cb317-18"></a><span class="ex">conda</span> activate bioinf</span>
<span id="cb317-19"><a href="chap-HPCC.html#cb317-19"></a><span class="bu">echo</span></span>
<span id="cb317-20"><a href="chap-HPCC.html#cb317-20"></a><span class="bu">echo</span> <span class="st">&quot;Now the PATH is: </span><span class="va">$PATH</span><span class="st">&quot;</span></span>
<span id="cb317-21"><a href="chap-HPCC.html#cb317-21"></a><span class="bu">echo</span></span>
<span id="cb317-22"><a href="chap-HPCC.html#cb317-22"></a><span class="bu">echo</span> <span class="st">&quot;And the environment variables look like:&quot;</span></span>
<span id="cb317-23"><a href="chap-HPCC.html#cb317-23"></a><span class="bu">echo</span> <span class="st">&quot;================================&quot;</span></span>
<span id="cb317-24"><a href="chap-HPCC.html#cb317-24"></a><span class="fu">env</span></span>
<span id="cb317-25"><a href="chap-HPCC.html#cb317-25"></a></span>
<span id="cb317-26"><a href="chap-HPCC.html#cb317-26"></a><span class="fu">sleep</span> 120</span></code></pre></div>
<p>Then run it with:</p>
<div class="sourceCode" id="cb318"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb318-1"><a href="chap-HPCC.html#cb318-1"></a><span class="ex">sbatch</span> conda-test2.sh</span></code></pre></div>
<p>If <code>conda</code> is working properly within the job script there should be no output
to <code>conda-test2-error-XXXX</code> and you should see that invoking <code>conda activate bioinf</code>
changed the PATH variable to include paths from the <code>bioinf</code> environment.</p>
<p>The above points are very important if you want to use software like <code>bwa</code>, <code>samtools</code>,
and <code>bcftools</code>, that you installed using Miniconda, from within a job script.</p>
<p>To repeat: to activate a conda environment like <code>bioinf</code> from within a job script running
under <code>sbatch</code> you must add these lines:</p>
<div class="sourceCode" id="cb319"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb319-1"><a href="chap-HPCC.html#cb319-1"></a><span class="bu">source</span> ~/.bashrc</span>
<span id="cb319-2"><a href="chap-HPCC.html#cb319-2"></a><span class="ex">conda</span> activate bioinf</span></code></pre></div>
</div>
<div id="test-a-longer-running-bash-script" class="section level4">
<h4><span class="header-section-number">8.4.2.6</span> Test a longer running bash script</h4>
<p>If you have time and want to pull this all together. Navigate to the
<code>chinook-play</code> directory that we experimented in before. There should be
a directory called <code>fastq</code> in there, as well as <code>chinook-genome-idx</code>.</p>
<p>Then, make a job script called <code>map-it.sh</code> with these lines in it (modifying the
email to be your email address):</p>
<div class="sourceCode" id="cb320"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb320-1"><a href="chap-HPCC.html#cb320-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb320-2"><a href="chap-HPCC.html#cb320-2"></a></span>
<span id="cb320-3"><a href="chap-HPCC.html#cb320-3"></a><span class="co">#SBATCH --time=03:00:00</span></span>
<span id="cb320-4"><a href="chap-HPCC.html#cb320-4"></a><span class="co">#SBATCH --output=map-it-output-%j</span></span>
<span id="cb320-5"><a href="chap-HPCC.html#cb320-5"></a><span class="co">#SBATCH --error=map-it-error-%j </span></span>
<span id="cb320-6"><a href="chap-HPCC.html#cb320-6"></a><span class="co">#SBATCH --mail-type=ALL </span></span>
<span id="cb320-7"><a href="chap-HPCC.html#cb320-7"></a><span class="co">#SBATCH --mail-user=yourname@yourmail.edu</span></span>
<span id="cb320-8"><a href="chap-HPCC.html#cb320-8"></a></span>
<span id="cb320-9"><a href="chap-HPCC.html#cb320-9"></a></span>
<span id="cb320-10"><a href="chap-HPCC.html#cb320-10"></a><span class="bu">source</span> ~/.bash_profile  <span class="co"># or ~/.bashrc if that is how you are set up</span></span>
<span id="cb320-11"><a href="chap-HPCC.html#cb320-11"></a><span class="ex">conda</span> activate bioinf</span>
<span id="cb320-12"><a href="chap-HPCC.html#cb320-12"></a></span>
<span id="cb320-13"><a href="chap-HPCC.html#cb320-13"></a></span>
<span id="cb320-14"><a href="chap-HPCC.html#cb320-14"></a><span class="co"># map it, then make it a BAM then run fixmate</span></span>
<span id="cb320-15"><a href="chap-HPCC.html#cb320-15"></a><span class="ex">bwa</span> mem chinook-genome-idx/GCA_002872995.1_Otsh_v1.0_genomic.fna.gz \</span>
<span id="cb320-16"><a href="chap-HPCC.html#cb320-16"></a>  fastq/Battle_Creek_01_chinook_R1.fq.gz  \</span>
<span id="cb320-17"><a href="chap-HPCC.html#cb320-17"></a>  fastq/Battle_Creek_01_chinook_R2.fq.gz  <span class="kw">|</span> <span class="kw">\</span></span>
<span id="cb320-18"><a href="chap-HPCC.html#cb320-18"></a>  <span class="ex">samtools</span> view -b -1 - <span class="kw">|</span> <span class="kw">\</span></span>
<span id="cb320-19"><a href="chap-HPCC.html#cb320-19"></a>    <span class="ex">samtools</span> fixmate -m -O BAM - OUTPUT-fixed.bam</span>
<span id="cb320-20"><a href="chap-HPCC.html#cb320-20"></a>    </span>
<span id="cb320-21"><a href="chap-HPCC.html#cb320-21"></a><span class="co"># now coordinate-sort it and mark PCR duplicates</span></span>
<span id="cb320-22"><a href="chap-HPCC.html#cb320-22"></a><span class="ex">samtools</span> sort OUTPUT-fixed.bam <span class="kw">|</span> <span class="kw">\</span></span>
<span id="cb320-23"><a href="chap-HPCC.html#cb320-23"></a>  <span class="ex">samtools</span> markdup - OUTPUT-marked.bam</span></code></pre></div>
<p>And schedule it to run (from within the <code>chinook-play</code> directory) with:</p>
<div class="sourceCode" id="cb321"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb321-1"><a href="chap-HPCC.html#cb321-1"></a><span class="ex">sbatch</span> map-it.sh</span></code></pre></div>
</div>
</div>
<div id="slurm-job-arrays" class="section level3">
<h3><span class="header-section-number">8.4.3</span> SLURM Job Arrays</h3>
<p>Very often in bioinformatics, large jobs can be broken down into a series of
smaller tasks. As an example, aligning sequencing read data from an individual sample
to a reference genome
can always be done independently of the sequences from any other individuals. Therefore
a job that demands that sequencing data from 96 different individuals be aligned to
a reference genome can be broken into 96 different tasks: each task involves mapping
the sequencing data of just a single individual to the reference genome.</p>
<p>The bash shell <code>for</code> loop provides a way to cycle over different repetitive tasks.
In the case of alignment over multiple individuals we might use a <code>for</code> loop
in a shell script in the following fashion, to repeatedly run a script
called <code>align.sh</code> on the paired-end read from each of four different individuals:</p>
<div class="sourceCode" id="cb322"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb322-1"><a href="chap-HPCC.html#cb322-1"></a><span class="va">INDIVS=</span><span class="st">&quot;Ind_1 Ind_2 Ind_3 Ind_4&quot;</span></span>
<span id="cb322-2"><a href="chap-HPCC.html#cb322-2"></a></span>
<span id="cb322-3"><a href="chap-HPCC.html#cb322-3"></a><span class="kw">for</span> <span class="ex">ind</span> in <span class="va">$INDIVS</span><span class="kw">;</span> <span class="kw">do</span></span>
<span id="cb322-4"><a href="chap-HPCC.html#cb322-4"></a>  <span class="ex">align.sh</span> <span class="va">$ind</span>.R1.fq.gz <span class="va">$ind</span>.R2.fq.gz </span>
<span id="cb322-5"><a href="chap-HPCC.html#cb322-5"></a><span class="kw">done</span></span></code></pre></div>
<p>Such an approach will work OK on an HPCC if each task (aligning a single indvidual,
in this case) takes just a short amount of time. But, what if aligning each
individual takes almost a day of compute time, and you are only authorized to
run jobs less than 24 hours on your HPCC? What is needed in that event is a convenient
way to break jobs down into separate tasks, such that <em>each task is run as its own
separate job on the HPCC</em>.</p>
<p>SLURM provides a feature known as <em>job arrays</em> for handling just this type of use
case. When you submit a job array, you are, in effect, submitting to SLURM a recipe
that is telling the scheduler that you are submitting some number of instances of a particular
job. As resources become available, each new instance of that job starts to run. The
different “instances” can be different replicates of a complex simulation, or analyses done
on different instances of a data set. Our alignment example falls into the latter case: each
instance of the job would be aligning the reads of a different individual.</p>
<p>SLURM job arrays are submitted by using the <code>--array=&lt;array_spec&gt;</code> option to <code>sbatch</code>.
Each task of a job array is associated with a whole number, and the
<code>&lt;array_spec&gt;</code> argument to the <code>--array</code> option tells which numbers (and, hence,
which tasks) should be scheduled to run. For example, if you submit a script
named <code>mytasks.sh</code> to <code>sbatch</code>,
and the script includes in the SLURM preamble a line that says:</p>
<div class="sourceCode" id="cb323"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb323-1"><a href="chap-HPCC.html#cb323-1"></a><span class="co">#SBATCH --array=1-10</span></span></code></pre></div>
<p>then SLURM will schedule ten different jobs, indexed from 1 up to 10. That is all
well and good, but, in order to be useful, those 10 different jobs had best do
different things (for example, operate on 10 different data sets). The user must
see to it that each task of a job array performs the proper instance of a
job, and this is done using the shell environment variable <code>SLURM_ARRAY_TASK_ID</code>.</p>
<p>When the first of the 10 scheduled tasks in the above job array is launched,
the environment variable <code>SLURM_ARRAY_TASK_ID</code> is set to the value 1, and then
the contents of <code>mytasks.sh</code> are executed. When the second task is launched,
<code>SLURM_ARRAY_TASK_ID</code> is set to 2, and then <code>mytasks.sh</code> is run, and so forth,
until the tenth job is launched with <code>SLURM_ARRAY_TASK_ID</code> set to 10. Therefore, inside
the <code>mytasks.sh</code> script, the user can use the value of <code>SLURM_ARRAY_TASK_ID</code> (accessed
via variable substitution with a dollar sign, e.g., <code>$SLURM_ARRAY_TASK_ID</code>) to
direct the script to perform the proper instance of the job that is to be done.</p>
<p>Thus, we could write our alignment example with four individuals, above,
as a job array with a script file like this:</p>
<div class="sourceCode" id="cb324"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb324-1"><a href="chap-HPCC.html#cb324-1"></a><span class="co">#SBATCH --array=1-4</span></span>
<span id="cb324-2"><a href="chap-HPCC.html#cb324-2"></a></span>
<span id="cb324-3"><a href="chap-HPCC.html#cb324-3"></a><span class="va">IND=</span>Ind_<span class="va">$SLURM_ARRAY_TASK_ID</span></span>
<span id="cb324-4"><a href="chap-HPCC.html#cb324-4"></a></span>
<span id="cb324-5"><a href="chap-HPCC.html#cb324-5"></a><span class="ex">align.sh</span> <span class="va">$IND</span>.R1.fq.gz <span class="va">$IND</span>.R2.fq.gz</span></code></pre></div>
<p>Submitting that job with <code>sbatch</code> will submit four different tasks as part of
a job array, each task being the alignment of sequencing reads from one of
four different individuals.</p>
<div id="naming-slurm-job-array-tasks" class="section level4">
<h4><span class="header-section-number">8.4.3.1</span> Naming SLURM job array tasks</h4>
<p>Internally, when you submit a job array to SLURM, it creates a job ID for the
job array and then it reserves specific job numbers for the tasks within the array. However,
you will find it easiest to refer to the separate tasks in a SLURM job array using the
syntax <code>ArrayJobNumber_ArrayTaskNumber</code>. In other words, if we submitted the
job array script listed above for aligning the reads from four individuals, and SLURM
told us that the job-array had been assigned the job number 6677, then you can
refer to the different tasks as <code>6677_1</code> for the job aligning reads from Ind_1,
<code>6677_2</code> for the job aligning reads from Ind_2, and so forth.</p>
<p>When using the <code>--output</code> or <code>--error</code> options to direct <em>stdout</em> and <em>stderr</em> to
files during execution of the scripts submitted using the <code>--array</code> option to <code>sbatch</code>, you
can name those files using <code>%A</code> which expands to the overall job array number and
<code>%a</code> which expands to the SLURM array task ID. So for example:</p>
<div class="sourceCode" id="cb325"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb325-1"><a href="chap-HPCC.html#cb325-1"></a><span class="co">#SBATCH --output  stdout_%A_%a.txt</span></span></code></pre></div>
<p>would be useful.</p>
</div>
<div id="variations-on-the-array_spec" class="section level4">
<h4><span class="header-section-number">8.4.3.2</span> Variations on the <code>&lt;array_spec&gt;</code></h4>
<p>In the simplest case, you will want to evaluate tasks with indexes that
proceed from 1 up to some stopping number, stepping by 1 each time. However,
you can exert rather finely tuned control over which array task IDs you would like to
be executed. The important thing to remember with all these variations is that
<em>no spaces are allowed</em>!</p>
<p>As we have seen already, a dash is used to represent a range of numbers, such as:</p>
<div class="sourceCode" id="cb326"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb326-1"><a href="chap-HPCC.html#cb326-1"></a><span class="co">#SBATCH --array=1-50</span></span></code></pre></div>
<p>To change the step size, you can immmediately follow the second number of the range
with a colon, <code>:</code>, and give the step size. For example, to cycle over jobs with
array indexes 1, 4, 7, and 10, you could do:</p>
<div class="sourceCode" id="cb327"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb327-1"><a href="chap-HPCC.html#cb327-1"></a><span class="co">#SBATCH --array=1-10:3</span></span></code></pre></div>
<p>If you wanted to pick out specific single indexes, you can combine single
values and ranges with commas. For example, using just single values
interspersed with commas:</p>
<div class="sourceCode" id="cb328"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb328-1"><a href="chap-HPCC.html#cb328-1"></a><span class="co">#SBATCH --array=1,2,3,6,9,15</span></span></code></pre></div>
<p>You can even combine ranges with step sizes with commas. This would run
indexes 1, 11, 21 and 100, 150, 200:</p>
<div class="sourceCode" id="cb329"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb329-1"><a href="chap-HPCC.html#cb329-1"></a><span class="co">#SBATCH --array=1-21:10,100-200:50</span></span></code></pre></div>
<p>Being able to pick out single array index values to be done is wonderfully
convenient for redoing tasks that might have failed. For example, suppose you
ran a job with 500 tasks, and upon investigating the job logs you found that
array tasks 47, 368, 477, and 489 had failed because insufficient time was
allocated to them. In that case you could update the <code>--time</code> setting of your
script and then re-run just those four jobs by including:</p>
<div class="sourceCode" id="cb330"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb330-1"><a href="chap-HPCC.html#cb330-1"></a><span class="co">#SBATCH --array=47,368,477,489</span></span></code></pre></div>
<p>Be warned, however, that SLURM does not check your <code>&lt;array_spec&gt;</code> to ensure that
each task index if given only once. If you specify the same index should be run
twice, as is the case for 4 in the following:</p>
<div class="sourceCode" id="cb331"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb331-1"><a href="chap-HPCC.html#cb331-1"></a><span class="co">#SBATCH --array=1-10,4</span></span></code></pre></div>
<p>then the job will be run twice, and possibly those two jobs will run at
the same time so that output from each job will intermittently overwrite output
from the other job, leading to corrupted results. Just be careful not to
request the same task index more than once!</p>
<p>Finally, if you want to be friendly to the other users of your cluster
and want no more than X of your array tasks to be running at any time,
you can stipulate that by ending the <code>&lt;array_spec&gt;</code> with <code>%X</code>. For example,
to have no more than 5 array tasks running at any one time, you could do:</p>
<div class="sourceCode" id="cb332"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb332-1"><a href="chap-HPCC.html#cb332-1"></a><span class="co">#SBATCH --array=1-20%5</span></span></code></pre></div>
</div>
<div id="translating-array-indexes-to-job-instances" class="section level4">
<h4><span class="header-section-number">8.4.3.3</span> Translating Array Indexes to Job Instances</h4>
<p>The user is left to translate what a job array index of, say, 7, means in terms of
what <em>actions</em> that array task should take. Quite often you will want to map an array
index to a different file to analyze, or perhaps a different region of a chromosome to
do variant calling on, etc. A flexible and generic way of doing this mapping from
array indexes to job specifics is to first define the variables (things like filenames, etc.)
required by each array task in a simple TAB-delimited text file in which the first row holds
the names of the variables in different TAB-separate columns, and each row below that holds
the values that those variables should take for different values of the array index. The
array index itself should be listed in the first column.</p>
<p>For example, if we have 10 different
files to process, each with different read-group values (Section <a href="alignment-of-sequence-data-to-a-reference-genome-and-associated-steps.html#read-groups">19.2</a>),
you can specify the filenames and elements of the read group values in a TAB-delimited
text file called <code>files-and-groups.tsv</code> that looks like this:</p>
<div class="sourceCode" id="cb333"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb333-1"><a href="chap-HPCC.html#cb333-1"></a><span class="ex">index</span>   file_prefix LB  Flowcell    Lane</span>
<span id="cb333-2"><a href="chap-HPCC.html#cb333-2"></a><span class="ex">1</span>   DPCh_plate1_A01_S1_L1_R Lib-1   HTYYCBBXX   1</span>
<span id="cb333-3"><a href="chap-HPCC.html#cb333-3"></a><span class="ex">2</span>   DPCh_plate1_A01_S1_L2_R Lib-1   HTYYCBBXX   2</span>
<span id="cb333-4"><a href="chap-HPCC.html#cb333-4"></a><span class="ex">3</span>   DPCh_plate1_A01_S1_L3_R Lib-1   HTYYCBBXX   3</span>
<span id="cb333-5"><a href="chap-HPCC.html#cb333-5"></a><span class="ex">4</span>   DPCh_plate1_A01_S1_L4_R Lib-1   HTYYCBBXX   4</span>
<span id="cb333-6"><a href="chap-HPCC.html#cb333-6"></a><span class="ex">5</span>   DPCh_plate1_A01_S1_L5_R Lib-1   HTYYCBBXX   5</span>
<span id="cb333-7"><a href="chap-HPCC.html#cb333-7"></a><span class="ex">6</span>   DPCh_plate1_A01_S1_L6_R Lib-1   HTYYCBBXX   6</span>
<span id="cb333-8"><a href="chap-HPCC.html#cb333-8"></a><span class="ex">7</span>   DPCh_plate1_A01_S1_L7_R Lib-1   HTYYCBBXX   7</span>
<span id="cb333-9"><a href="chap-HPCC.html#cb333-9"></a><span class="ex">8</span>   DPCh_plate1_A01_S1_L8_R Lib-1   HTYYCBBXX   8</span>
<span id="cb333-10"><a href="chap-HPCC.html#cb333-10"></a><span class="ex">9</span>   DPCh_plate1_A02_S2_L1_R Lib-1   HTYYCBBXX   1</span>
<span id="cb333-11"><a href="chap-HPCC.html#cb333-11"></a><span class="ex">10</span>  DPCh_plate1_A02_S2_L2_R Lib-1   HTYYCBBXX   2</span></code></pre></div>
<p>Then, when executing a job array script, this file can be used to
convert the <code>SLURM_ARRAY_TASK_ID</code> into the necessary file names
and other variables with a simple <code>awk</code> script and
variable assignments within your job script
that look like this:</p>
<div class="sourceCode" id="cb334"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb334-1"><a href="chap-HPCC.html#cb334-1"></a><span class="co"># make a command line that holds assignments of values to shell variables</span></span>
<span id="cb334-2"><a href="chap-HPCC.html#cb334-2"></a><span class="va">ASSIGN=$(</span><span class="fu">awk</span> -F<span class="st">&quot;\t&quot;</span> -v N=<span class="va">$SLURM_ARRAY_TASK_ID</span> <span class="st">&#39;</span></span>
<span id="cb334-3"><a href="chap-HPCC.html#cb334-3"></a><span class="st">  NR==1 {for(i=1;i&lt;=NF;i++) vars[i]=$i; next}</span></span>
<span id="cb334-4"><a href="chap-HPCC.html#cb334-4"></a><span class="st">  $1 == N {for(i=1;i&lt;=NF;i++) printf(&quot;%s=%s; &quot;, vars[i], $i)}</span></span>
<span id="cb334-5"><a href="chap-HPCC.html#cb334-5"></a><span class="st">&#39;</span> files-and-groups.tsv<span class="va">)</span></span>
<span id="cb334-6"><a href="chap-HPCC.html#cb334-6"></a></span>
<span id="cb334-7"><a href="chap-HPCC.html#cb334-7"></a><span class="co"># make those assignments</span></span>
<span id="cb334-8"><a href="chap-HPCC.html#cb334-8"></a><span class="bu">eval</span> <span class="va">$ASSIGN</span></span></code></pre></div>
<p>Then, when SLURM_ARRAY_TASK_ID is equal to 7, for example, the variable
<code>ASSIGN</code> gets the string value:</p>
<div class="sourceCode" id="cb335"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb335-1"><a href="chap-HPCC.html#cb335-1"></a><span class="va">index=</span>7; <span class="va">file_prefix=</span>DPCh_plate1_A01_S1_L7_R; <span class="va">LB=</span>Lib-1; <span class="va">Flowcell=</span>HTYYCBBXX; <span class="va">Lane=</span>7;</span></code></pre></div>
<p>and then evaluating that string with <code>eval $ASSIGN</code> will define for you the values of the
shell variables <code>index</code>, <code>file_prefix</code>, <code>LB</code>, <code>Flowcell</code>, and <code>Lane</code>, as indicated
above. These shell variables may be used in subsequent code in your script
(via variable subtitution with the <code>$</code>, like <code>$file_prefix</code>) to ensure that the
proper files are processed, etc., for each different value of SLURM_ARRAY_TASK_ID.</p>
</div>
<div id="slurm-job-array-hands-on" class="section level4">
<h4><span class="header-section-number">8.4.3.4</span> SLURM Job Array hands-on</h4>
<p>Here, you can run some small scripts to get your feet wet with
defining slurm job arrays. Save the following lines to a script
named <code>test-arrays.sh</code> and then submit it via <code>sbatch test-arrays.sh</code>.
It should produce three output files.</p>
<div class="sourceCode" id="cb336"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb336-1"><a href="chap-HPCC.html#cb336-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb336-2"><a href="chap-HPCC.html#cb336-2"></a><span class="co">#SBATCH --time=00:05:00</span></span>
<span id="cb336-3"><a href="chap-HPCC.html#cb336-3"></a><span class="co">#SBATCH --output=my_output_%A_%a</span></span>
<span id="cb336-4"><a href="chap-HPCC.html#cb336-4"></a><span class="co">#SBATCH --array=1-3</span></span>
<span id="cb336-5"><a href="chap-HPCC.html#cb336-5"></a></span>
<span id="cb336-6"><a href="chap-HPCC.html#cb336-6"></a><span class="bu">echo</span> <span class="st">&quot;The SLURM_ARRAY_JOB_ID is : </span><span class="va">$SLURM_ARRAY_JOB_ID</span><span class="st">&quot;</span></span>
<span id="cb336-7"><a href="chap-HPCC.html#cb336-7"></a><span class="bu">echo</span> <span class="st">&quot;The SLURM_ARRAY_TASK_ID is: </span><span class="va">$SLURM_ARRAY_TASK_ID</span><span class="st">&quot;</span></span>
<span id="cb336-8"><a href="chap-HPCC.html#cb336-8"></a><span class="bu">echo</span> <span class="st">&quot;The SLURM_JOB_ID is: </span><span class="va">$SLURM_JOB_ID</span><span class="st">&quot;</span></span>
<span id="cb336-9"><a href="chap-HPCC.html#cb336-9"></a><span class="bu">echo</span></span>
<span id="cb336-10"><a href="chap-HPCC.html#cb336-10"></a><span class="bu">echo</span> <span class="st">&quot;You can refer to this individual SLURM array task as: </span><span class="va">${SLURM_ARRAY_JOB_ID}</span><span class="st">_</span><span class="va">${SLURM_ARRAY_TASK_ID}</span><span class="st">&quot;</span></span>
<span id="cb336-11"><a href="chap-HPCC.html#cb336-11"></a><span class="bu">echo</span></span>
<span id="cb336-12"><a href="chap-HPCC.html#cb336-12"></a></span>
<span id="cb336-13"><a href="chap-HPCC.html#cb336-13"></a><span class="fu">sleep</span> 180</span></code></pre></div>
<p>Once this has been submitted, use your <code>myjobs</code> alias to see how it looks
when it is queued: something like this:</p>
<pre><code>            JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
     4499013_[1-3]      shas test-arr eriq@col PD       0:00      1 (Priority)</code></pre>
<p>If it is taking a while to start this job, and you don’t have a <code>myjobs</code> alias,
put one in your <code>~/.bashrc</code>. Add these lines:</p>
<div class="sourceCode" id="cb338"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb338-1"><a href="chap-HPCC.html#cb338-1"></a><span class="co"># aliases for SLURM stuff</span></span>
<span id="cb338-2"><a href="chap-HPCC.html#cb338-2"></a><span class="bu">alias</span> myjobs=<span class="st">&#39;squeue -u username&#39;</span></span></code></pre></div>
<p>where you have to change <em>username</em> to be your login name on the HPCC. Note
that, for Colorado State Univesity affiliates on SUMMIT, that username is of the form,
<code>user@colostate</code>.</p>
<p>If you would like to see each one of the tasks in a job array listed
on its own line, you can use the <code>-r, --array</code> option to <code>squeue</code>. In terms
of a <code>myjobs</code> alias that means issuing <code>myjobs -r</code> or <code>myjobs --array</code> (both are
equivalent):</p>
<div class="sourceCode" id="cb339"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb339-1"><a href="chap-HPCC.html#cb339-1"></a><span class="ex">%</span> myjobs -r</span>
<span id="cb339-2"><a href="chap-HPCC.html#cb339-2"></a>             <span class="ex">JOBID</span> PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)</span>
<span id="cb339-3"><a href="chap-HPCC.html#cb339-3"></a>         <span class="ex">4499052_1</span>      shas test-arr eriq@col PD       0:00      1 (Priority)</span>
<span id="cb339-4"><a href="chap-HPCC.html#cb339-4"></a>         <span class="ex">4499052_2</span>      shas test-arr eriq@col PD       0:00      1 (Priority)</span>
<span id="cb339-5"><a href="chap-HPCC.html#cb339-5"></a>         <span class="ex">4499052_3</span>      shas test-arr eriq@col PD       0:00      1 (Priority)</span></code></pre></div>
<p>I want people to look closely at the variables SLURM_ARRAY_JOB_ID and
SLURM_ARRAY_TASK_ID, and to understand how those are different from
SLURM_JOB_ID in this context. We will discuss that in class.</p>
<p>Unfortunately, it seems like it can take a long time for that to start on
SUMMIT. So, here are the results:</p>
<div class="sourceCode" id="cb340"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb340-1"><a href="chap-HPCC.html#cb340-1"></a>==<span class="op">&gt;</span> <span class="ex">my_output_27234_1</span> <span class="op">&lt;</span>==</span>
<span id="cb340-2"><a href="chap-HPCC.html#cb340-2"></a><span class="ex">The</span> SLURM_ARRAY_JOB_ID is : 27234</span>
<span id="cb340-3"><a href="chap-HPCC.html#cb340-3"></a><span class="ex">The</span> SLURM_ARRAY_TASK_ID is: 1</span>
<span id="cb340-4"><a href="chap-HPCC.html#cb340-4"></a><span class="ex">The</span> SLURM_JOB_ID is: 27235</span>
<span id="cb340-5"><a href="chap-HPCC.html#cb340-5"></a></span>
<span id="cb340-6"><a href="chap-HPCC.html#cb340-6"></a><span class="ex">You</span> can refer to this individual SLURM array task as: 27234_1</span>
<span id="cb340-7"><a href="chap-HPCC.html#cb340-7"></a></span>
<span id="cb340-8"><a href="chap-HPCC.html#cb340-8"></a></span>
<span id="cb340-9"><a href="chap-HPCC.html#cb340-9"></a>==<span class="op">&gt;</span> <span class="ex">my_output_27234_2</span> <span class="op">&lt;</span>==</span>
<span id="cb340-10"><a href="chap-HPCC.html#cb340-10"></a><span class="ex">The</span> SLURM_ARRAY_JOB_ID is : 27234</span>
<span id="cb340-11"><a href="chap-HPCC.html#cb340-11"></a><span class="ex">The</span> SLURM_ARRAY_TASK_ID is: 2</span>
<span id="cb340-12"><a href="chap-HPCC.html#cb340-12"></a><span class="ex">The</span> SLURM_JOB_ID is: 27236</span>
<span id="cb340-13"><a href="chap-HPCC.html#cb340-13"></a></span>
<span id="cb340-14"><a href="chap-HPCC.html#cb340-14"></a><span class="ex">You</span> can refer to this individual SLURM array task as: 27234_2</span>
<span id="cb340-15"><a href="chap-HPCC.html#cb340-15"></a></span>
<span id="cb340-16"><a href="chap-HPCC.html#cb340-16"></a></span>
<span id="cb340-17"><a href="chap-HPCC.html#cb340-17"></a>==<span class="op">&gt;</span> <span class="ex">my_output_27234_3</span> <span class="op">&lt;</span>==</span>
<span id="cb340-18"><a href="chap-HPCC.html#cb340-18"></a><span class="ex">The</span> SLURM_ARRAY_JOB_ID is : 27234</span>
<span id="cb340-19"><a href="chap-HPCC.html#cb340-19"></a><span class="ex">The</span> SLURM_ARRAY_TASK_ID is: 3</span>
<span id="cb340-20"><a href="chap-HPCC.html#cb340-20"></a><span class="ex">The</span> SLURM_JOB_ID is: 27234</span>
<span id="cb340-21"><a href="chap-HPCC.html#cb340-21"></a></span>
<span id="cb340-22"><a href="chap-HPCC.html#cb340-22"></a><span class="ex">You</span> can refer to this individual SLURM array task as: 27234_3</span></code></pre></div>
<p>Next, we want to explore the possibility of overriding the <code>#SBATCH --array</code> option
given internally in the <code>test-arrays.sh</code> with one given on the command line. Thus,
try submitting the job with:</p>
<div class="sourceCode" id="cb341"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb341-1"><a href="chap-HPCC.html#cb341-1"></a><span class="ex">sbatch</span> --array=100,200,300 test-arrays.sh</span></code></pre></div>
<p>Finally, we want to contemplate strategies for turning the <code>SLURM_ARRAY_TASK_ID</code>
into “directions” for the script about what to do (which files to process, etc.)</p>
<p>We discussed one approach, above, using a TAB-delimited file, but want to note here
that depending on how your actual script is configured, any number of approaches
might be suitable.</p>
<p>For an example germane to doing alignment to a reference genome in our
homework <code>chr-32-bioinformatics</code>, consider that you have 1280 pairs of files
to process, and that you will launch jobs to process 128 such pairs at a time.
A script called <code>run-single-job.sh</code> that mapped the first 128 pairs of files
looked like this:</p>
<div class="sourceCode" id="cb342"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb342-1"><a href="chap-HPCC.html#cb342-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb342-2"><a href="chap-HPCC.html#cb342-2"></a></span>
<span id="cb342-3"><a href="chap-HPCC.html#cb342-3"></a><span class="co">#SBATCH --time=05:00:00</span></span>
<span id="cb342-4"><a href="chap-HPCC.html#cb342-4"></a><span class="co">#SBATCH --output=run-single-out-%j</span></span>
<span id="cb342-5"><a href="chap-HPCC.html#cb342-5"></a><span class="co">#SBATCH --error=run-single-error-%j</span></span>
<span id="cb342-6"><a href="chap-HPCC.html#cb342-6"></a><span class="co">#SBATCH --mail-type=ALL</span></span>
<span id="cb342-7"><a href="chap-HPCC.html#cb342-7"></a><span class="co">#SBATCH --mail-user=user@email.edu</span></span>
<span id="cb342-8"><a href="chap-HPCC.html#cb342-8"></a></span>
<span id="cb342-9"><a href="chap-HPCC.html#cb342-9"></a></span>
<span id="cb342-10"><a href="chap-HPCC.html#cb342-10"></a><span class="ex">./map-N-files-from-K.sh</span> 128 1</span></code></pre></div>
<p>If you wanted to map the next 128 pairs of files, the final line in the above
script should look like:</p>
<div class="sourceCode" id="cb343"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb343-1"><a href="chap-HPCC.html#cb343-1"></a><span class="ex">./map-N-files-from-K.sh</span> 128 129</span></code></pre></div>
<p>and to do the next 128 pairs of files after that, the line should look like:</p>
<div class="sourceCode" id="cb344"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb344-1"><a href="chap-HPCC.html#cb344-1"></a><span class="ex">./map-N-files-from-K.sh</span> 128 257</span></code></pre></div>
<p>and so forth.</p>
<p>So, if everyone has already run the first 128 pairs of files (as everyone should
have finished for the homework) I contend that, if we wanted to modify this script
so that the remaining 1152 pairs of files could be run as a job array, our new script
could look like this:</p>
<div class="sourceCode" id="cb345"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb345-1"><a href="chap-HPCC.html#cb345-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb345-2"><a href="chap-HPCC.html#cb345-2"></a></span>
<span id="cb345-3"><a href="chap-HPCC.html#cb345-3"></a><span class="co">#SBATCH --time=02:00:00</span></span>
<span id="cb345-4"><a href="chap-HPCC.html#cb345-4"></a><span class="co">#SBATCH --output=run-array-out-%A_%a</span></span>
<span id="cb345-5"><a href="chap-HPCC.html#cb345-5"></a><span class="co">#SBATCH --error=run-array-error-%A_%a</span></span>
<span id="cb345-6"><a href="chap-HPCC.html#cb345-6"></a><span class="co">#SBATCH --mail-type=ALL</span></span>
<span id="cb345-7"><a href="chap-HPCC.html#cb345-7"></a><span class="co">#SBATCH --mail-user=user@email.edu</span></span>
<span id="cb345-8"><a href="chap-HPCC.html#cb345-8"></a><span class="co">#SBATCH --array=1-9</span></span>
<span id="cb345-9"><a href="chap-HPCC.html#cb345-9"></a></span>
<span id="cb345-10"><a href="chap-HPCC.html#cb345-10"></a></span>
<span id="cb345-11"><a href="chap-HPCC.html#cb345-11"></a><span class="ex">./map-N-files-from-K.sh</span> 128 <span class="va">$((</span>SLURM_ARRAY_TASK_ID * 128 + 1<span class="va">))</span></span></code></pre></div>
<p>Save that file as <code>run-array-job.sh</code></p>
<p>Note the changes:</p>
<ul>
<li>output and error files modified to use <code>%A_%a</code></li>
<li>Added the –array command running from 1 to 9.</li>
<li>Changed the starting index from 1 to <code>$((SLURM_ARRAY_TASK_ID * 128 + 1))</code></li>
</ul>
<p>The last one deserves some explanation, it uses the <em>integer arithmetic</em>
capabilities of the bash shell (Section <a href="shell-programming.html#bash-arithmetic">5.3.6</a>).</p>
<p>So, what is that last line doing? You should check it for yourself. Doing so
will emphasize an important point when it comes to testing your own job-array scripts:
if you are testing the code in them interactively, you can simply assign any value you
want to a variable named <code>SLURM_ARRAY_TASK_ID</code> and verify the results are what you would
like them to be. So, try this:</p>
<div class="sourceCode" id="cb346"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb346-1"><a href="chap-HPCC.html#cb346-1"></a><span class="va">SLURM_ARRAY_TASK_ID=</span>1</span>
<span id="cb346-2"><a href="chap-HPCC.html#cb346-2"></a><span class="bu">echo</span> ./map-N-files-from-K.sh 128 <span class="va">$((</span>SLURM_ARRAY_TASK_ID * 128 + 1<span class="va">))</span></span></code></pre></div>
<p>And to check them all, including when SLURM_ARRAY_TASK_ID=0 (which is the case that
you already did for your homework, running it as a single job), try this:</p>
<div class="sourceCode" id="cb347"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb347-1"><a href="chap-HPCC.html#cb347-1"></a><span class="kw">for</span> <span class="ex">SLURM_ARRAY_TASK_ID</span> in <span class="dt">{0..9}</span><span class="kw">;</span> <span class="kw">do</span></span>
<span id="cb347-2"><a href="chap-HPCC.html#cb347-2"></a>  <span class="bu">echo</span> ./map-N-files-from-K.sh 128 <span class="va">$((</span>SLURM_ARRAY_TASK_ID * 128 + 1<span class="va">))</span></span>
<span id="cb347-3"><a href="chap-HPCC.html#cb347-3"></a><span class="kw">done</span></span></code></pre></div>
<p>Once you have convinced yourself that your <code>run-array-job.sh</code> script will
work correctly, and it is set for an array=1-9, and you have configured
your email properly within it, then you should copy it to your
<code>chr-32-bioinformatics</code> directory and submit it as a job to finish aligning
the reads from all the remaining individuals, so we will have them for
variant calling down the road.</p>
<p><strong>THAT LAST STEP ABOVE IS PART OF YOUR HOMEWORK FOR NEXT WEEK THAT WILL BE DUE NEXT TUESDAY </strong></p>
</div>
</div>
</div>
<div id="prepation-interlude-an-in-class-exercise-to-make-sure-everything-is-configured-correctly" class="section level2">
<h2><span class="header-section-number">8.5</span> PREPATION INTERLUDE: An in-class exercise to make sure everything is configured correctly</h2>
<p>Here, we run through a number of steps to start aligning sequence data to a reference
genome. The main purpose of this exercise is to ensure that everyone has their systems
configured correctly. Namely, we want to make sure that:</p>
<ul>
<li>You can log in to your cluster (SUMMIT, Hummingbird, etc.)</li>
<li>You can use a few SLURM (job scheduler) commands.</li>
<li>You can find your way to your <em>scratch</em> space on SUMMIT (or Hummingbird). <em>scratch</em> is fast, largely
unlimited short-term storage. It is where you will do the majority of your bioinformatics.</li>
<li>You can use <code>rclone</code> to tranfer files from Google Drive to your cluster. Doing things this way will allow
you to access your Lab’s Team Drive. The model for bioinformatic work is:
<ol style="list-style-type: lower-alpha">
<li>Copy data from your Lab’s Google Team Drive to <em>scratch</em> on the cluster.</li>
<li>Do analyses of the data (in <em>scratch</em>)</li>
<li>Copy the results back to your Lab’s Google Team Drive before it gets deleted (by the system administrators)
off of <em>scratch</em>
Note: today you are not using your lab’s Team drive, you are using the “class” shared drive,
and you won’t be copying anything back to it. But this is a start at least…</li>
</ol></li>
<li>You have Miniconda installed, that you have a conda environment with bioinformatic
tools in it, and that you can activate that environment and use the tools within it.
Miniconda makes it easy to install software that you need to run analyses.</li>
</ul>
<p>Being able to successfully do all these things will be necessary to complete
the next homework assignment which involves aligning short reads to a reference
genome and will be assigned after spring break.</p>
<ol style="list-style-type: decimal">
<li><p>If you are not already in a tmux session, start a new <code>tmux</code> session called <code>inclass</code> on the login node.</p>
<div class="sourceCode" id="cb348"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb348-1"><a href="chap-HPCC.html#cb348-1"></a><span class="ex">tmux</span> new -s inclass</span></code></pre></div></li>
<li><p>Get a bash shell on a compute node</p>
<div class="sourceCode" id="cb349"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb349-1"><a href="chap-HPCC.html#cb349-1"></a><span class="co"># on summit:</span></span>
<span id="cb349-2"><a href="chap-HPCC.html#cb349-2"></a><span class="ex">sinteractive</span></span>
<span id="cb349-3"><a href="chap-HPCC.html#cb349-3"></a></span>
<span id="cb349-4"><a href="chap-HPCC.html#cb349-4"></a><span class="co"># on hummingbird</span></span>
<span id="cb349-5"><a href="chap-HPCC.html#cb349-5"></a><span class="ex">salloc</span> --partition=Instruction --nodes=1 --time=02:00:00 --mem=4G --cpus-per-task=1</span>
<span id="cb349-6"><a href="chap-HPCC.html#cb349-6"></a><span class="fu">ssh</span> <span class="va">$SLURM_NODELIST</span></span>
<span id="cb349-7"><a href="chap-HPCC.html#cb349-7"></a></span>
<span id="cb349-8"><a href="chap-HPCC.html#cb349-8"></a><span class="co"># otherwise, you can try this (will work on Sedna)</span></span>
<span id="cb349-9"><a href="chap-HPCC.html#cb349-9"></a><span class="ex">srun</span> --pty /bin/bash</span></code></pre></div></li>
<li><p>Check what host you are on with <code>hostname</code></p>
<div class="sourceCode" id="cb350"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb350-1"><a href="chap-HPCC.html#cb350-1"></a><span class="fu">hostname</span></span>
<span id="cb350-2"><a href="chap-HPCC.html#cb350-2"></a></span>
<span id="cb350-3"><a href="chap-HPCC.html#cb350-3"></a><span class="co"># The hostname should be something like shasXXX. </span></span>
<span id="cb350-4"><a href="chap-HPCC.html#cb350-4"></a><span class="co">#  Do not proceed if you are on a login node still (ie. hostname is loginXX)</span></span></code></pre></div>
<p>If you are still on a login node, try running <code>sinteractive</code> again.</p></li>
<li><p>Use your alias <code>myjobs</code> (if you have created it) to see what jobs you have running.</p>
<div class="sourceCode" id="cb351"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb351-1"><a href="chap-HPCC.html#cb351-1"></a><span class="ex">myjobs</span></span>
<span id="cb351-2"><a href="chap-HPCC.html#cb351-2"></a></span>
<span id="cb351-3"><a href="chap-HPCC.html#cb351-3"></a><span class="co"># or, if you don&#39;t have that aliased:</span></span>
<span id="cb351-4"><a href="chap-HPCC.html#cb351-4"></a><span class="ex">squeue</span> -u your_username</span>
<span id="cb351-5"><a href="chap-HPCC.html#cb351-5"></a></span>
<span id="cb351-6"><a href="chap-HPCC.html#cb351-6"></a><span class="co"># You should see that you have one job running.  That &quot;job&quot; is a bash shell</span></span>
<span id="cb351-7"><a href="chap-HPCC.html#cb351-7"></a><span class="co"># that you are interactively working with.</span></span></code></pre></div></li>
<li><p>Navigate to your scratch directory using <code>cd</code>. On SUMMIT that will look like this:</p>
<div class="sourceCode" id="cb352"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb352-1"><a href="chap-HPCC.html#cb352-1"></a><span class="bu">cd</span> /scratch/summit/wcfunk\@colostate.edu/</span></code></pre></div>
<p>You can make a symbolic link to that in your home directory like this:</p>
<div class="sourceCode" id="cb353"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb353-1"><a href="chap-HPCC.html#cb353-1"></a><span class="bu">cd</span>  <span class="co"># returns you to hour home directory</span></span>
<span id="cb353-2"><a href="chap-HPCC.html#cb353-2"></a></span>
<span id="cb353-3"><a href="chap-HPCC.html#cb353-3"></a><span class="co"># this line makes a symbolic link called scratch in your home directory</span></span>
<span id="cb353-4"><a href="chap-HPCC.html#cb353-4"></a><span class="co"># that points to your scratch space at /scratch/summit/wcfunk\@colostate.edu/</span></span>
<span id="cb353-5"><a href="chap-HPCC.html#cb353-5"></a><span class="fu">ln</span> -s /scratch/summit/wcfunk\@colostate.edu/ scratch</span></code></pre></div>
<p>Just be sure to use your own user name instead of wcfunk. Once that symbolic link is set up, you can
<code>cd</code> into it, just like it was your scratch directory:</p>
<div class="sourceCode" id="cb354"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb354-1"><a href="chap-HPCC.html#cb354-1"></a><span class="bu">cd</span> scratch</span></code></pre></div>
<p>But if you don’t want to set up a symbolic link just use the full path:</p>
<div class="sourceCode" id="cb355"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb355-1"><a href="chap-HPCC.html#cb355-1"></a><span class="bu">cd</span> /scratch/summit/wcfunk\@colostate.edu/</span></code></pre></div>
<p>On hummingbird you go to</p>
<div class="sourceCode" id="cb356"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb356-1"><a href="chap-HPCC.html#cb356-1"></a><span class="bu">cd</span> /hb/scratch/username/</span></code></pre></div>
<p>where username is your ucsc username. Note that if you want to make a symbolic link to that, then
<em>while in your home directory</em> do this:</p>
<div class="sourceCode" id="cb357"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb357-1"><a href="chap-HPCC.html#cb357-1"></a><span class="fu">ln</span> -s /hb/scratch/username scratch</span></code></pre></div></li>
<li><p>Within your account’s scratch space, make a directory called <code>chinook-play</code> and <code>cd</code> into it:</p>
<div class="sourceCode" id="cb358"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb358-1"><a href="chap-HPCC.html#cb358-1"></a><span class="fu">mkdir</span> chinook-play</span>
<span id="cb358-2"><a href="chap-HPCC.html#cb358-2"></a><span class="bu">cd</span> chinook-play/</span></code></pre></div></li>
<li><p>Make sure you have followed the link in your email to the shared google drive folder I sent you last night. If the email
went to an email address that is not associated with your rclone configuration for google drive, then request access
(at google drive) for that email. (Or setup a new rclone config for your other gmail address…)</p></li>
<li><p>Now, use rclone to list the files in the folder I shared with you:</p>
<div class="sourceCode" id="cb359"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb359-1"><a href="chap-HPCC.html#cb359-1"></a><span class="ex">rclone</span> lsd --drive-shared-with-me gdrive-pers:CSU-con-gen-2020</span></code></pre></div>
<p>Note that <code>gdrive-pers</code> above is the name of my rclone configuration associated with the gmail account where
I got an email (from myself at my CSU account) about the shared folder. You will have to change <code>gdrive-pers</code>
to be appropriate to your config, depending on how you set it up. Note also how you access folders shared with
you using <code>--drive-shared-with-me</code>.</p></li>
<li><p>After you get the above command to work, you should see something like the following, though it
will change as more things get added to this directory…</p>
<div class="sourceCode" id="cb360"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb360-1"><a href="chap-HPCC.html#cb360-1"></a><span class="ex">-1</span> 2020-03-09 17:51:06        -1 chr32-160-chinook</span>
<span id="cb360-2"><a href="chap-HPCC.html#cb360-2"></a><span class="ex">-1</span> 2020-03-05 07:33:21        -1 pre-indexed-chinook-genome</span></code></pre></div></li>
<li><p>Use rclone to download <code>big-fastq-play.zip</code> (about 170 MB). Once again you have to change
the name of the rclone remoate (<code>gdrive-pers</code> in this case), to the name of your Google drive
rclone remote.</p>
<div class="sourceCode" id="cb361"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb361-1"><a href="chap-HPCC.html#cb361-1"></a><span class="ex">rclone</span> copy -P  --drive-shared-with-me gdrive-pers:CSU-con-gen-2020/big-fastq-play.zip ./</span></code></pre></div></li>
<li><p>Unzip the file you just downloaded. Note: on Hummingbird, <code>unzip</code> is not installed by default.
So I recommend doing <code>conda install unzip</code> to add it to your base Miniconda environment.</p>
<div class="sourceCode" id="cb362"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb362-1"><a href="chap-HPCC.html#cb362-1"></a><span class="fu">unzip</span> big-fastq-play.zip</span></code></pre></div>
<p>This creates a directory called <code>big-fastq-play</code> within your <code>chinook-play</code> directory.</p></li>
<li><p>Create a directory called <code>fastq</code> and move the two paired-end FASTQ files from <code>big-fastq-play</code> into it:</p>
<div class="sourceCode" id="cb363"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb363-1"><a href="chap-HPCC.html#cb363-1"></a><span class="fu">mkdir</span> fastq </span>
<span id="cb363-2"><a href="chap-HPCC.html#cb363-2"></a><span class="fu">mv</span> big-fastq-play/data/Battle_Creek_01_chinook_R* fastq/</span>
<span id="cb363-3"><a href="chap-HPCC.html#cb363-3"></a><span class="co"># then make sure that they got moved</span></span>
<span id="cb363-4"><a href="chap-HPCC.html#cb363-4"></a><span class="fu">ls</span> fastq</span>
<span id="cb363-5"><a href="chap-HPCC.html#cb363-5"></a><span class="co"># if the files were moved, then feel free to remove the big fastq stuff</span></span>
<span id="cb363-6"><a href="chap-HPCC.html#cb363-6"></a><span class="fu">rm</span> -r big-fastq-play*</span></code></pre></div></li>
<li><p>Download the Chinook salmon genome into a directory called <code>genome</code></p>
<div class="sourceCode" id="cb364"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb364-1"><a href="chap-HPCC.html#cb364-1"></a> <span class="fu">mkdir</span> genome</span>
<span id="cb364-2"><a href="chap-HPCC.html#cb364-2"></a> <span class="bu">cd</span> genome</span>
<span id="cb364-3"><a href="chap-HPCC.html#cb364-3"></a> <span class="fu">wget</span> ftp://ftp.ncbi.nlm.nih.gov/genomes/genbank/vertebrate_other/Oncorhynchus_tshawytscha/all_assembly_versions/GCA_002872995.1_Otsh_v1.0/GCA_002872995.1_Otsh_v1.0_genomic.fna.gz</span></code></pre></div>
<p>That dude is 760 Mb, but it goes really quickly (at least on SUMMIT. Hummingbird seems to be a different story…)</p></li>
<li><p>Activate your <code>bioinf</code> conda environment to be able to use <code>bwa</code>:</p>
<div class="sourceCode" id="cb365"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb365-1"><a href="chap-HPCC.html#cb365-1"></a><span class="ex">conda</span> activate bioinf</span></code></pre></div></li>
<li><p>Index the genome with <code>bwa</code> in order to align reads to it:</p>
<div class="sourceCode" id="cb366"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb366-1"><a href="chap-HPCC.html#cb366-1"></a><span class="ex">bwa</span> index GCA_002872995.1_Otsh_v1.0_genomic.fna.gz </span></code></pre></div>
<p>This will take about 10 to 15 minutes. During that time, do this:</p>
<ul>
<li>If you are doing this within a <code>tmux</code> session, create a new tmux window (<code>cntrl-b c</code>). Note that this shell opens on the login node (the one that you started your tmux session on.)</li>
<li>Rename this new tmux window “browse” (<code>cntrl-b ,</code>)</li>
<li>Navigate to <code>~/scratch/chinook-play/genome/</code> and list the files there. These new files
are being created by bwa.</li>
<li>Switch back to your original tmux window.</li>
</ul></li>
<li><p>If <code>bwa</code> is still running, ask me some questions. Or maybe I will tell you more about how conda works…</p></li>
<li><p>If <code>bwa</code> is still running after that, ask me about <em>nested tmux sessions</em>.</p></li>
<li><p>Truth be told. Indexing this thing can take a long time (30 to 40 minutes on a single core, perhaps). So,
instead, here is what we will do:</p>
<ol style="list-style-type: lower-alpha">
<li>Kill the current <code>bwa</code> job but issuing <code>cntrl-c</code> in the shell where it is running.</li>
<li>Use <code>rclone</code> to get the genome and a <code>bwa</code> index for it from my Google Drive. We want to put the
result in the <code>chinook-play</code> directory inside a directory called <code>chinook-genome-idx</code>. NOTE: Again,
you will likely have to use your name for your rclone remote, rather than <code>gdrive-pers</code>.</li>
</ol>
<div class="sourceCode" id="cb367"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb367-1"><a href="chap-HPCC.html#cb367-1"></a><span class="bu">cd</span> ../ <span class="co"># move up to the chinook-play directory</span></span>
<span id="cb367-2"><a href="chap-HPCC.html#cb367-2"></a><span class="ex">rclone</span> copy  -P  --drive-shared-with-me gdrive-pers:CSU-con-gen-2020/pre-indexed-chinook-genome chinook-genome-idx </span></code></pre></div>
<p>That thing is 4.6 Gb, but can take less than a minute to transfer. (Let’s see if things slow down with everyone
running this command at the same time.)</p></li>
<li><p>Now when you do <code>ls</code> you should see something like this:</p>
<div class="sourceCode" id="cb368"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb368-1"><a href="chap-HPCC.html#cb368-1"></a><span class="kw">(</span><span class="ex">bioinf</span><span class="kw">)</span> [<span class="ex">chinook-play</span>]--% ls</span>
<span id="cb368-2"><a href="chap-HPCC.html#cb368-2"></a><span class="ex">chinook-genome-idx</span>  fastq  genome</span></code></pre></div></li>
<li><p>For a final hurrah, we will start aligning those fastqs to the chinook genome. We will put the result in a directory
we create called <code>sam</code>. To make it clearer what we are doing we
will define some shell variables.:</p>
<div class="sourceCode" id="cb369"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb369-1"><a href="chap-HPCC.html#cb369-1"></a><span class="fu">mkdir</span> sam</span>
<span id="cb369-2"><a href="chap-HPCC.html#cb369-2"></a><span class="va">GENOME=</span>chinook-genome-idx/GCA_002872995.1_Otsh_v1.0_genomic.fna.gz</span>
<span id="cb369-3"><a href="chap-HPCC.html#cb369-3"></a><span class="va">FQB=</span>fastq/Battle_Creek_01_chinook_R</span>
<span id="cb369-4"><a href="chap-HPCC.html#cb369-4"></a><span class="va">SAMOUT=$(</span><span class="fu">basename</span> <span class="va">$FQB)</span>.sam</span>
<span id="cb369-5"><a href="chap-HPCC.html#cb369-5"></a></span>
<span id="cb369-6"><a href="chap-HPCC.html#cb369-6"></a><span class="co"># for fun, try echoing each of those variables and make sure they look right.</span></span>
<span id="cb369-7"><a href="chap-HPCC.html#cb369-7"></a><span class="co"># for example:</span></span>
<span id="cb369-8"><a href="chap-HPCC.html#cb369-8"></a><span class="bu">echo</span> <span class="va">$FQB</span></span>
<span id="cb369-9"><a href="chap-HPCC.html#cb369-9"></a></span>
<span id="cb369-10"><a href="chap-HPCC.html#cb369-10"></a><span class="co"># ...</span></span>
<span id="cb369-11"><a href="chap-HPCC.html#cb369-11"></a></span>
<span id="cb369-12"><a href="chap-HPCC.html#cb369-12"></a><span class="co"># then, launch bwa mem to map those reads to the genome.</span></span>
<span id="cb369-13"><a href="chap-HPCC.html#cb369-13"></a><span class="co"># the syntax is:</span></span>
<span id="cb369-14"><a href="chap-HPCC.html#cb369-14"></a><span class="co"># Usage: bwa mem [options] &lt;idxbase&gt; &lt;in1.fq&gt; [in2.fq]</span></span>
<span id="cb369-15"><a href="chap-HPCC.html#cb369-15"></a><span class="ex">bwa</span> mem <span class="va">$GENOME</span> <span class="va">${FQB}</span>1.fq.gz <span class="va">${FQB}</span>2.fq.gz <span class="op">&gt;</span> sam/<span class="va">$SAMOUT</span> <span class="op">2&gt;</span>sam/<span class="va">$SAMOUT</span>.error</span></code></pre></div></li>
<li><p>Once that has started, it is hard to know anything is happening, because we have
redirected both stdout and stderr to files. So, do what you did before, use tmux to
go to another shell and then use <code>tail</code> or <code>less</code> to look at the output files, which
are in <code>sam/Battle_Creek_01_chinook_R.sam</code> and <code>sam/Battle_Creek_01_chinook_R.sam.error</code>
within the <code>chinook-play</code> directory in <code>scratch</code>. You might even count how many
lines of alignments file have been formed:</p>
<div class="sourceCode" id="cb370"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb370-1"><a href="chap-HPCC.html#cb370-1"></a><span class="fu">awk</span> <span class="st">&#39;!/^@/&#39;</span> Battle_Creek_01_chinook_R.sam <span class="kw">|</span> <span class="fu">wc</span> </span></code></pre></div></li>
<li><p>Note that, in practice, we would not usually save the <code>.sam</code> file to disk. Rather, we would
convert it into a compressed <code>.bam</code> file right as it comes off of <code>bwa mem</code>, and maybe even sort it
as it is coming off…</p></li>
<li><p>Note that this process might not finish before the end of class. That is not a huge problem
if you are running the job within <code>tmux</code>. You can logout and it will still keep running.</p></li>
</ol>
<p>However, as said before, most of the time you will run <em>batch</em> jobs rather than interactive jobs.
I just wanted to first give everyone a chance to step through these tasks in an interactive shell
on a compute node because that is crucial when developing your scripts, and we wanted to
make sure that everyone has rclone, and miniconda installed and working.</p>
</div>
<div id="more-boneyard" class="section level2">
<h2><span class="header-section-number">8.6</span> More Boneyard…</h2>
<p>Here is some nice stuff for summarizing all the information from the different runs from the chinook-wgs project:</p>
<div class="sourceCode" id="cb371"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb371-1"><a href="chap-HPCC.html#cb371-1"></a><span class="ex">qacct</span> -o eriq -b 09271925 -j ml <span class="kw">|</span> <span class="ex">tidy-qacct</span></span></code></pre></div>
<p>Explain scratch space and how clusters are configured with respect to storage, etc.</p>
<p>Strategies—break names up with consistent characters:</p>
<ul>
<li>dashes within population names</li>
<li>underscores for different groups of chromosomes</li>
<li>periods for catenating pairs of pops</li>
</ul>
<p>etc. Basically, it just makes it much easier to split things up
when the time comes.</p>
</div>
<div id="the-queue-slurmsgeuge" class="section level2">
<h2><span class="header-section-number">8.7</span> The Queue (SLURM/SGE/UGE)</h2>
</div>
<div id="modules-package" class="section level2">
<h2><span class="header-section-number">8.8</span> Modules package</h2>
</div>
<div id="compiling-programs-without-admin-privileges" class="section level2">
<h2><span class="header-section-number">8.9</span> Compiling programs without admin privileges</h2>
<p>Inevitably you will want to use a piece of software that is not available as
a module or is not otherwise installed on they system.</p>
<p>Typically these software programs have a frightful web of dependencies.</p>
<p>Unix/Linux distros typically maintain all these dependencies as libraries or packages
that can be installed using a <code>rpm</code> or <code>yum</code>. However, the simple “plug-and-play” approach
to using these programs requires have administrator privileges so that the software can
be installed in one of the (typically protected) paths in the root (like <code>/usr/bin</code>).</p>
<p>But, you can use these programs to install packages into your home directory. Once you have done
that, you need to let your system know where to look for these packages when it needs them
(i.e., when running a program or <em>linking</em> to it whilst compiling up a program that uses it
as a dependency.</p>
<p>Hoffman2 runs CentOS. Turns out that CentOS uses <code>yum</code> as a package manager.</p>
<p>Let’s see if we can install llvm using yum.</p>
<div class="sourceCode" id="cb372"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb372-1"><a href="chap-HPCC.html#cb372-1"></a><span class="ex">yum</span> search all llvm <span class="co"># &lt;- this got me to devtoolset-7-all.x86_64 : Package shipping all available toolsets.</span></span>
<span id="cb372-2"><a href="chap-HPCC.html#cb372-2"></a></span>
<span id="cb372-3"><a href="chap-HPCC.html#cb372-3"></a><span class="co"># a little web searching made it look like llvm-toolset-7-5.0.1-4.el7.x86_64.rpm or devtoolset-7-llvm-7.0-5.el7.x86_64.rpm</span></span>
<span id="cb372-4"><a href="chap-HPCC.html#cb372-4"></a><span class="co"># might be what we want.  The first is a dependency of the second...</span></span>
<span id="cb372-5"><a href="chap-HPCC.html#cb372-5"></a><span class="fu">mkdir</span> ~/centos</span></code></pre></div>
<p>Was using instructions at <a href="https://stackoverflow.com/questions/36651091/how-to-install-packages-in-linux-centos-without-root-user-with-automatic-depen">https://stackoverflow.com/questions/36651091/how-to-install-packages-in-linux-centos-without-root-user-with-automatic-depen</a></p>
<p>Couldn’t get yum downloader to download any packages. The whole thing looked like it was going to
be a mess, so I thought I would try with miniconda.</p>
<p>I installed miniconda (python 2.7 version) into <code>/u/nobackup/kruegg/eriq/programs/miniconda/</code> and then did this:</p>
<div class="sourceCode" id="cb373"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb373-1"><a href="chap-HPCC.html#cb373-1"></a><span class="co"># probably could have listed them all at once, but wanted to watch them go </span></span>
<span id="cb373-2"><a href="chap-HPCC.html#cb373-2"></a><span class="co"># one at a time...</span></span>
<span id="cb373-3"><a href="chap-HPCC.html#cb373-3"></a><span class="ex">conda</span> install numpy</span>
<span id="cb373-4"><a href="chap-HPCC.html#cb373-4"></a><span class="ex">conda</span> install scipy</span>
<span id="cb373-5"><a href="chap-HPCC.html#cb373-5"></a><span class="ex">conda</span> install pandas</span>
<span id="cb373-6"><a href="chap-HPCC.html#cb373-6"></a><span class="ex">conda</span> install numba</span>
<span id="cb373-7"><a href="chap-HPCC.html#cb373-7"></a></span>
<span id="cb373-8"><a href="chap-HPCC.html#cb373-8"></a><span class="co"># those all ran great.</span></span>
<span id="cb373-9"><a href="chap-HPCC.html#cb373-9"></a></span>
<span id="cb373-10"><a href="chap-HPCC.html#cb373-10"></a><span class="ex">conda</span> install pysnptools</span>
<span id="cb373-11"><a href="chap-HPCC.html#cb373-11"></a></span>
<span id="cb373-12"><a href="chap-HPCC.html#cb373-12"></a><span class="co"># that one didn&#39;t find a match, but I found on the web that I should try:</span></span>
<span id="cb373-13"><a href="chap-HPCC.html#cb373-13"></a><span class="ex">conda</span> install -c bioconda pysnptools </span>
<span id="cb373-14"><a href="chap-HPCC.html#cb373-14"></a></span>
<span id="cb373-15"><a href="chap-HPCC.html#cb373-15"></a><span class="co"># that worked!</span></span></code></pre></div>
<p>Also we want to touch briefly on LD_PATH (linking failures—and note that libraries are often
named libxxx.a) and CPATH (for failure to find xxxx.h), etc.</p>
</div>
<div id="job-arrays" class="section level2">
<h2><span class="header-section-number">8.10</span> Job arrays</h2>
<p>Quick note: Redefine IFS to break on TABs so you can have full commands in there.
This is super useful for parsing job-array COMMLINES files.</p>
<pre><code>IFS=$&#39;\t\n&#39;; BOP=($(echo boing | awk &#39;{printf(&quot;first\tsecond\tthird that is long\tfourth\n&quot;);}&#39;)); IFS=$&#39; \t\n&#39;;</code></pre>
<p>Definitely mention the <code>eval</code> keyword in bash for when you want to print
command lines with redirects.</p>
<p>Show the routine for it, and develop a good approach to efficiently
orchestrating redos. If you know the taskIDs of the ones that failed
then it is pretty easy to write an awk script that picks out the
commands and puts them in a new file. Actually, it is probably
better to just cycle over the numbers and use the -t option
to launch each. Then there is now changing the job-ids file.</p>
<p>In fact, I am starting to think that the -t option is better than
putting it into the file.</p>
<p>Question: if you give something on the command line, does that override
the directive in the header of the file? If so, then you don’t even
need to change the file. Note that using the qsub command line options
instead of the directives really opens up a lot of possibilities for
writing useful scripts that are flexible.</p>
<p>Also use short names for the jobs and have a system for naming the
redos (append numbers so you know which round it is, too)
possibly base the name on the ways things failed the first time. Like,
<code>fsttf1</code> = “Fst run for things that failed due to time limits, 1”. Or
structure things so that redos can just be done by invoking it with -t
and the jobid.</p>
</div>
<div id="writing-stdout-and-stderr-to-files" class="section level2">
<h2><span class="header-section-number">8.11</span> Writing stdout and stderr to files</h2>
<p>This is always good to do. Note that <code>stdbuf</code> is super useful here so that
things don’t get buffered super long. (PCAngsd doesn’t seem to write antyhing till
the end…)</p>
</div>
<div id="breaking-stuff-down" class="section level2">
<h2><span class="header-section-number">8.12</span> Breaking stuff down</h2>
<p>It is probably worth talking about how problems can be broken down into
smaller ones. Maybe give an example, and then say that we will be talking about
this for every step of the way in bioinformatic pipelines.</p>
<p>One thing to note—sometimes processes go awry for one reason or another.
When things are in smaller chunks it is not such a huge investment to
re-run it. (Unlike stuff that runs for two weeks before you realize that
it ain’t working right).</p>

</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="working-on-remote-servers.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="introduction-to-reproducible-research.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/eriqande/eca-bioinf-handbook/edit/master/1.035-hpc.Rmd",
"text": "Edit"
},
"history": {
"link": "https://github.com/eriqande/eca-bioinf-handbook/commits/master/1.035-hpc.Rmd",
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["eca-bioinf-handbook.pdf", "eca-bioinf-handbook.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
