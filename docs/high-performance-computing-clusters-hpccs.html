<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 High Performance Computing Clusters (HPCC’s) | Practical Computing and Bioinformatics for Conservation and Evolutionary Genomics</title>
  <meta name="description" content="A book example for a Chapman &amp; Hall book." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 High Performance Computing Clusters (HPCC’s) | Practical Computing and Bioinformatics for Conservation and Evolutionary Genomics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A book example for a Chapman &amp; Hall book." />
  <meta name="github-repo" content="yihui/bookdown-crc" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 High Performance Computing Clusters (HPCC’s) | Practical Computing and Bioinformatics for Conservation and Evolutionary Genomics" />
  
  <meta name="twitter:description" content="A book example for a Chapman &amp; Hall book." />
  

<meta name="author" content="Eric C. Anderson" />


<meta name="date" content="2020-02-25" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="working-on-remote-servers.html"/>
<link rel="next" href="introduction-to-reproducible-research.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">ECA's Bioinformatics Handbook</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="erics-notes-of-what-he-might-do.html"><a href="erics-notes-of-what-he-might-do.html"><i class="fa fa-check"></i><b>2</b> Eric’s Notes of what he might do</a><ul>
<li class="chapter" data-level="2.1" data-path="erics-notes-of-what-he-might-do.html"><a href="erics-notes-of-what-he-might-do.html#table-of-topics"><i class="fa fa-check"></i><b>2.1</b> Table of topics</a></li>
</ul></li>
<li class="part"><span><b>I Part I: Essential Computing Skills</b></span></li>
<li class="chapter" data-level="3" data-path="overview-of-essential-computing-skills.html"><a href="overview-of-essential-computing-skills.html"><i class="fa fa-check"></i><b>3</b> Overview of Essential Computing Skills</a></li>
<li class="chapter" data-level="4" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html"><i class="fa fa-check"></i><b>4</b> Essential Unix/Linux Terminal Knowledge</a><ul>
<li class="chapter" data-level="4.1" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#getting-a-bash-shell-on-your-system"><i class="fa fa-check"></i><b>4.1</b> Getting a bash shell on your system</a></li>
<li class="chapter" data-level="4.2" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#navigating-the-unix-filesystem"><i class="fa fa-check"></i><b>4.2</b> Navigating the Unix filesystem</a><ul>
<li class="chapter" data-level="4.2.1" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#changing-the-working-directory-with-cd"><i class="fa fa-check"></i><b>4.2.1</b> Changing the working directory with <code>cd</code></a></li>
<li class="chapter" data-level="4.2.2" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#updating-your-command-prompt"><i class="fa fa-check"></i><b>4.2.2</b> Updating your command prompt</a></li>
<li class="chapter" data-level="4.2.3" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#tab-completion-for-paths"><i class="fa fa-check"></i><b>4.2.3</b> TAB-completion for paths</a></li>
<li class="chapter" data-level="4.2.4" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#listing-the-contents-of-a-directory-with-ls"><i class="fa fa-check"></i><b>4.2.4</b> Listing the contents of a directory with <code>ls</code></a></li>
<li class="chapter" data-level="4.2.5" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#globbing"><i class="fa fa-check"></i><b>4.2.5</b> Globbing</a></li>
<li class="chapter" data-level="4.2.6" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#what-makes-a-good-file-name"><i class="fa fa-check"></i><b>4.2.6</b> What makes a good file-name?</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#the-anatomy-of-a-unix-command"><i class="fa fa-check"></i><b>4.3</b> The anatomy of a Unix command</a><ul>
<li class="chapter" data-level="4.3.1" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#anatomy-command"><i class="fa fa-check"></i><b>4.3.1</b> The <code>command</code></a></li>
<li class="chapter" data-level="4.3.2" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#the-options"><i class="fa fa-check"></i><b>4.3.2</b> The <em>options</em></a></li>
<li class="chapter" data-level="4.3.3" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#arguments"><i class="fa fa-check"></i><b>4.3.3</b> Arguments</a></li>
<li class="chapter" data-level="4.3.4" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#getting-information-about-unix-commands"><i class="fa fa-check"></i><b>4.3.4</b> Getting information about Unix commands</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#handling-manipulating-and-viewing-files-and-streams"><i class="fa fa-check"></i><b>4.4</b> Handling, Manipulating, and Viewing files and streams</a><ul>
<li class="chapter" data-level="4.4.1" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#creating-new-directories"><i class="fa fa-check"></i><b>4.4.1</b> Creating new directories</a></li>
<li class="chapter" data-level="4.4.2" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#fundamental-file-handling-commands"><i class="fa fa-check"></i><b>4.4.2</b> Fundamental file-handling commands</a></li>
<li class="chapter" data-level="4.4.3" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#viewing-files"><i class="fa fa-check"></i><b>4.4.3</b> “Viewing” Files</a></li>
<li class="chapter" data-level="4.4.4" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#redirecting-standard-output-and"><i class="fa fa-check"></i><b>4.4.4</b> Redirecting standard output: <code>&gt;</code> and <code>&gt;&gt;</code></a></li>
<li class="chapter" data-level="4.4.5" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#stdin-and"><i class="fa fa-check"></i><b>4.4.5</b> stdin, <code>&lt;</code> and <code>|</code></a></li>
<li class="chapter" data-level="4.4.6" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#stderr"><i class="fa fa-check"></i><b>4.4.6</b> stderr</a></li>
<li class="chapter" data-level="4.4.7" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#symbolic-links"><i class="fa fa-check"></i><b>4.4.7</b> Symbolic links</a></li>
<li class="chapter" data-level="4.4.8" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#file-permissions"><i class="fa fa-check"></i><b>4.4.8</b> File Permissions</a></li>
<li class="chapter" data-level="4.4.9" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#editing-text-files-at-the-terminal"><i class="fa fa-check"></i><b>4.4.9</b> Editing text files at the terminal</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#unix-env"><i class="fa fa-check"></i><b>4.5</b> Customizing your Environment</a><ul>
<li class="chapter" data-level="4.5.1" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#appearances-matter"><i class="fa fa-check"></i><b>4.5.1</b> Appearances matter</a></li>
<li class="chapter" data-level="4.5.2" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#where-are-my-programscommands-at"><i class="fa fa-check"></i><b>4.5.2</b> Where are my programs/commands at?!</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#a-few-more-important-keystrokes"><i class="fa fa-check"></i><b>4.6</b> A Few More Important Keystrokes</a></li>
<li class="chapter" data-level="4.7" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#a-short-list-of-additional-useful-commands."><i class="fa fa-check"></i><b>4.7</b> A short list of additional useful commands.</a></li>
<li class="chapter" data-level="4.8" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#two-important-computing-concepts"><i class="fa fa-check"></i><b>4.8</b> Two important computing concepts</a><ul>
<li class="chapter" data-level="4.8.1" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#compression"><i class="fa fa-check"></i><b>4.8.1</b> Compression</a></li>
<li class="chapter" data-level="4.8.2" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#hashing"><i class="fa fa-check"></i><b>4.8.2</b> Hashing</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="essential-unixlinux-terminal-knowledge.html"><a href="essential-unixlinux-terminal-knowledge.html#unix-quick-study-guide"><i class="fa fa-check"></i><b>4.9</b> Unix: Quick Study Guide</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="shell-programming.html"><a href="shell-programming.html"><i class="fa fa-check"></i><b>5</b> Shell programming</a><ul>
<li class="chapter" data-level="5.1" data-path="shell-programming.html"><a href="shell-programming.html#an-example-script"><i class="fa fa-check"></i><b>5.1</b> An example script</a></li>
<li class="chapter" data-level="5.2" data-path="shell-programming.html"><a href="shell-programming.html#the-structure-of-a-bash-script"><i class="fa fa-check"></i><b>5.2</b> The Structure of a Bash Script</a><ul>
<li class="chapter" data-level="5.2.1" data-path="shell-programming.html"><a href="shell-programming.html#a-bit-more-on-and"><i class="fa fa-check"></i><b>5.2.1</b> A bit more on <code>;</code> and <code>&amp;</code></a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="shell-programming.html"><a href="shell-programming.html#variables"><i class="fa fa-check"></i><b>5.3</b> Variables</a><ul>
<li class="chapter" data-level="5.3.1" data-path="shell-programming.html"><a href="shell-programming.html#assigning-values-to-variables"><i class="fa fa-check"></i><b>5.3.1</b> Assigning values to variables</a></li>
<li class="chapter" data-level="5.3.2" data-path="shell-programming.html"><a href="shell-programming.html#accessing-values-from-variables"><i class="fa fa-check"></i><b>5.3.2</b> Accessing values from variables</a></li>
<li class="chapter" data-level="5.3.3" data-path="shell-programming.html"><a href="shell-programming.html#what-does-the-shell-do-with-the-value-substituted-for-a-variable"><i class="fa fa-check"></i><b>5.3.3</b> What does the shell do with the value substituted for a variable?</a></li>
<li class="chapter" data-level="5.3.4" data-path="shell-programming.html"><a href="shell-programming.html#double-and-single-quotation-marks-and-variable-substitution"><i class="fa fa-check"></i><b>5.3.4</b> Double and Single Quotation Marks and Variable Substitution</a></li>
<li class="chapter" data-level="5.3.5" data-path="shell-programming.html"><a href="shell-programming.html#one-useful-fancy-variable-substitution-method"><i class="fa fa-check"></i><b>5.3.5</b> One useful, fancy, variable-substitution method</a></li>
<li class="chapter" data-level="5.3.6" data-path="shell-programming.html"><a href="shell-programming.html#variable-arrays"><i class="fa fa-check"></i><b>5.3.6</b> Variable arrays</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="shell-programming.html"><a href="shell-programming.html#evaluate-a-command-and-substitute-the-result-on-the-command-line"><i class="fa fa-check"></i><b>5.4</b> Evaluate a command and substitute the result on the command line</a></li>
<li class="chapter" data-level="5.5" data-path="shell-programming.html"><a href="shell-programming.html#groupingcollecting-output-from-multiple-commands-commands-and-commands"><i class="fa fa-check"></i><b>5.5</b> Grouping/Collecting output from multiple commands: <code>(commands)</code> and <code>{ commands; }</code></a></li>
<li class="chapter" data-level="5.6" data-path="shell-programming.html"><a href="shell-programming.html#exit-status"><i class="fa fa-check"></i><b>5.6</b> Exit Status</a><ul>
<li class="chapter" data-level="5.6.1" data-path="shell-programming.html"><a href="shell-programming.html#combinations-of-exit-statuses"><i class="fa fa-check"></i><b>5.6.1</b> Combinations of exit statuses</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="shell-programming.html"><a href="shell-programming.html#loops-and-repetition"><i class="fa fa-check"></i><b>5.7</b> Loops and repetition</a></li>
<li class="chapter" data-level="5.8" data-path="shell-programming.html"><a href="shell-programming.html#more-conditional-evaluation-if-then-else-and-friends"><i class="fa fa-check"></i><b>5.8</b> More Conditional Evaluation: <code>if</code>, <code>then</code>, <code>else</code>, and friends</a></li>
<li class="chapter" data-level="5.9" data-path="shell-programming.html"><a href="shell-programming.html#finallypositional-parameters"><i class="fa fa-check"></i><b>5.9</b> Finally…positional parameters</a></li>
<li class="chapter" data-level="5.10" data-path="shell-programming.html"><a href="shell-programming.html#basename-and-dirname-two-useful-little-utilities"><i class="fa fa-check"></i><b>5.10</b> <code>basename</code> and <code>dirname</code> two useful little utilities</a></li>
<li class="chapter" data-level="5.11" data-path="shell-programming.html"><a href="shell-programming.html#bash-functions"><i class="fa fa-check"></i><b>5.11</b> <code>bash</code> functions</a></li>
<li class="chapter" data-level="5.12" data-path="shell-programming.html"><a href="shell-programming.html#reading-files-line-by-line"><i class="fa fa-check"></i><b>5.12</b> reading files line by line</a></li>
<li class="chapter" data-level="5.13" data-path="shell-programming.html"><a href="shell-programming.html#further-reading"><i class="fa fa-check"></i><b>5.13</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="sed-awk-and-regular-expressions.html"><a href="sed-awk-and-regular-expressions.html"><i class="fa fa-check"></i><b>6</b> Sed, awk, and regular expressions</a><ul>
<li class="chapter" data-level="6.1" data-path="sed-awk-and-regular-expressions.html"><a href="sed-awk-and-regular-expressions.html#awk"><i class="fa fa-check"></i><b>6.1</b> awk</a><ul>
<li class="chapter" data-level="6.1.1" data-path="sed-awk-and-regular-expressions.html"><a href="sed-awk-and-regular-expressions.html#line-cycling-tests-and-actions"><i class="fa fa-check"></i><b>6.1.1</b> Line-cycling, tests and actions</a></li>
<li class="chapter" data-level="6.1.2" data-path="sed-awk-and-regular-expressions.html"><a href="sed-awk-and-regular-expressions.html#column-splitting-fields--f-nf-print-ofs-and-begin"><i class="fa fa-check"></i><b>6.1.2</b> Column splitting, fields, <code>-F</code>, <code>$</code>, <code>NF</code>, <code>print</code>, <code>OFS</code> and <code>BEGIN</code></a></li>
<li class="chapter" data-level="6.1.3" data-path="sed-awk-and-regular-expressions.html"><a href="sed-awk-and-regular-expressions.html#a-brief-introduction-to-regular-expressions"><i class="fa fa-check"></i><b>6.1.3</b> A brief introduction to regular expressions</a></li>
<li class="chapter" data-level="6.1.4" data-path="sed-awk-and-regular-expressions.html"><a href="sed-awk-and-regular-expressions.html#a-variety-of-tests"><i class="fa fa-check"></i><b>6.1.4</b> A variety of tests</a></li>
<li class="chapter" data-level="6.1.5" data-path="sed-awk-and-regular-expressions.html"><a href="sed-awk-and-regular-expressions.html#code-in-the-action-blocks"><i class="fa fa-check"></i><b>6.1.5</b> Code in the action blocks</a></li>
<li class="chapter" data-level="6.1.6" data-path="sed-awk-and-regular-expressions.html"><a href="sed-awk-and-regular-expressions.html#using-awk-to-assign-to-shell-variables"><i class="fa fa-check"></i><b>6.1.6</b> Using <code>awk</code> to assign to shell variables</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="sed-awk-and-regular-expressions.html"><a href="sed-awk-and-regular-expressions.html#sed"><i class="fa fa-check"></i><b>6.2</b> sed</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html"><i class="fa fa-check"></i><b>7</b> Working on remote servers</a><ul>
<li class="chapter" data-level="7.1" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#accessing-remote-computers"><i class="fa fa-check"></i><b>7.1</b> Accessing remote computers</a><ul>
<li class="chapter" data-level="7.1.1" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#windows"><i class="fa fa-check"></i><b>7.1.1</b> Windows</a></li>
<li class="chapter" data-level="7.1.2" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#hummingbird"><i class="fa fa-check"></i><b>7.1.2</b> Hummingbird</a></li>
<li class="chapter" data-level="7.1.3" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#summit"><i class="fa fa-check"></i><b>7.1.3</b> Summit</a></li>
<li class="chapter" data-level="7.1.4" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#sedna"><i class="fa fa-check"></i><b>7.1.4</b> Sedna</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#transferring-files-to-remote-computers"><i class="fa fa-check"></i><b>7.2</b> Transferring files to remote computers</a><ul>
<li class="chapter" data-level="7.2.1" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#sftp-via-lftp"><i class="fa fa-check"></i><b>7.2.1</b> <code>sftp</code> (via <code>lftp</code>)</a></li>
<li class="chapter" data-level="7.2.2" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#git"><i class="fa fa-check"></i><b>7.2.2</b> git</a></li>
<li class="chapter" data-level="7.2.3" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#globus"><i class="fa fa-check"></i><b>7.2.3</b> Globus</a></li>
<li class="chapter" data-level="7.2.4" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#interfacing-with-the-cloud"><i class="fa fa-check"></i><b>7.2.4</b> Interfacing with “The Cloud”</a></li>
<li class="chapter" data-level="7.2.5" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#getting-files-from-a-sequencing-center"><i class="fa fa-check"></i><b>7.2.5</b> Getting files from a sequencing center</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#tmux"><i class="fa fa-check"></i><b>7.3</b> <code>tmux</code>: the terminal multiplexer</a><ul>
<li class="chapter" data-level="7.3.1" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#an-analogy-for-how-tmux-works"><i class="fa fa-check"></i><b>7.3.1</b> An analogy for how <code>tmux</code> works</a></li>
<li class="chapter" data-level="7.3.2" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#first-steps-with-tmux"><i class="fa fa-check"></i><b>7.3.2</b> First steps with <code>tmux</code></a></li>
<li class="chapter" data-level="7.3.3" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#further-steps-with-tmux"><i class="fa fa-check"></i><b>7.3.3</b> Further steps with <code>tmux</code></a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="working-on-remote-servers.html"><a href="working-on-remote-servers.html#using-neovim-and-nvim-r-and-tmux-to-use-r-well-on-the-cluster"><i class="fa fa-check"></i><b>7.4</b> Using neovim and Nvim-R and tmux to use R well on the cluster</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="high-performance-computing-clusters-hpccs.html"><a href="high-performance-computing-clusters-hpccs.html"><i class="fa fa-check"></i><b>8</b> High Performance Computing Clusters (HPCC’s)</a><ul>
<li class="chapter" data-level="8.1" data-path="high-performance-computing-clusters-hpccs.html"><a href="high-performance-computing-clusters-hpccs.html#an-oversimplified-but-useful-view-of-a-computing-cluster"><i class="fa fa-check"></i><b>8.1</b> An oversimplified, but useful, view of a computing cluster</a></li>
<li class="chapter" data-level="8.2" data-path="high-performance-computing-clusters-hpccs.html"><a href="high-performance-computing-clusters-hpccs.html#cluster-computing-and-the-job-scheduler"><i class="fa fa-check"></i><b>8.2</b> Cluster computing and the job scheduler</a></li>
<li class="chapter" data-level="8.3" data-path="high-performance-computing-clusters-hpccs.html"><a href="high-performance-computing-clusters-hpccs.html#the-slurm-commands"><i class="fa fa-check"></i><b>8.3</b> The SLURM commands</a></li>
<li class="chapter" data-level="8.4" data-path="high-performance-computing-clusters-hpccs.html"><a href="high-performance-computing-clusters-hpccs.html#activatinginstalling-software"><i class="fa fa-check"></i><b>8.4</b> Activating/Installing software</a><ul>
<li class="chapter" data-level="8.4.1" data-path="high-performance-computing-clusters-hpccs.html"><a href="high-performance-computing-clusters-hpccs.html#modules"><i class="fa fa-check"></i><b>8.4.1</b> Modules</a></li>
<li class="chapter" data-level="8.4.2" data-path="high-performance-computing-clusters-hpccs.html"><a href="high-performance-computing-clusters-hpccs.html#miniconda"><i class="fa fa-check"></i><b>8.4.2</b> Miniconda</a></li>
<li class="chapter" data-level="8.4.3" data-path="high-performance-computing-clusters-hpccs.html"><a href="high-performance-computing-clusters-hpccs.html#exporting-environments"><i class="fa fa-check"></i><b>8.4.3</b> Exporting environments</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="high-performance-computing-clusters-hpccs.html"><a href="high-performance-computing-clusters-hpccs.html#boneyard"><i class="fa fa-check"></i><b>8.5</b> Boneyard</a></li>
<li class="chapter" data-level="8.6" data-path="high-performance-computing-clusters-hpccs.html"><a href="high-performance-computing-clusters-hpccs.html#the-queue-slurmsgeuge"><i class="fa fa-check"></i><b>8.6</b> The Queue (SLURM/SGE/UGE)</a></li>
<li class="chapter" data-level="8.7" data-path="high-performance-computing-clusters-hpccs.html"><a href="high-performance-computing-clusters-hpccs.html#modules-package"><i class="fa fa-check"></i><b>8.7</b> Modules package</a></li>
<li class="chapter" data-level="8.8" data-path="high-performance-computing-clusters-hpccs.html"><a href="high-performance-computing-clusters-hpccs.html#compiling-programs-without-admin-privileges"><i class="fa fa-check"></i><b>8.8</b> Compiling programs without admin privileges</a></li>
<li class="chapter" data-level="8.9" data-path="high-performance-computing-clusters-hpccs.html"><a href="high-performance-computing-clusters-hpccs.html#job-arrays"><i class="fa fa-check"></i><b>8.9</b> Job arrays</a></li>
<li class="chapter" data-level="8.10" data-path="high-performance-computing-clusters-hpccs.html"><a href="high-performance-computing-clusters-hpccs.html#writing-stdout-and-stderr-to-files"><i class="fa fa-check"></i><b>8.10</b> Writing stdout and stderr to files</a></li>
<li class="chapter" data-level="8.11" data-path="high-performance-computing-clusters-hpccs.html"><a href="high-performance-computing-clusters-hpccs.html#breaking-stuff-down"><i class="fa fa-check"></i><b>8.11</b> Breaking stuff down</a></li>
</ul></li>
<li class="part"><span><b>II Part II: Reproducible Research Strategies</b></span></li>
<li class="chapter" data-level="9" data-path="introduction-to-reproducible-research.html"><a href="introduction-to-reproducible-research.html"><i class="fa fa-check"></i><b>9</b> Introduction to Reproducible Research</a></li>
<li class="chapter" data-level="10" data-path="rstudio-and-project-centered-organization.html"><a href="rstudio-and-project-centered-organization.html"><i class="fa fa-check"></i><b>10</b> Rstudio and Project-centered Organization</a><ul>
<li class="chapter" data-level="10.1" data-path="rstudio-and-project-centered-organization.html"><a href="rstudio-and-project-centered-organization.html#organizing-big-projects"><i class="fa fa-check"></i><b>10.1</b> Organizing big projects</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="version-control.html"><a href="version-control.html"><i class="fa fa-check"></i><b>11</b> Version control</a><ul>
<li class="chapter" data-level="11.1" data-path="version-control.html"><a href="version-control.html#why-use-version-control"><i class="fa fa-check"></i><b>11.1</b> Why use version control?</a></li>
<li class="chapter" data-level="11.2" data-path="version-control.html"><a href="version-control.html#git-workings"><i class="fa fa-check"></i><b>11.2</b> How git works</a></li>
<li class="chapter" data-level="11.3" data-path="version-control.html"><a href="version-control.html#git-workflow-patterns"><i class="fa fa-check"></i><b>11.3</b> git workflow patterns</a></li>
<li class="chapter" data-level="11.4" data-path="version-control.html"><a href="version-control.html#using-git-with-rstudio"><i class="fa fa-check"></i><b>11.4</b> using git with Rstudio</a></li>
<li class="chapter" data-level="11.5" data-path="version-control.html"><a href="version-control.html#git-on-the-command-line"><i class="fa fa-check"></i><b>11.5</b> git on the command line</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="a-fast-furious-overview-of-the-tidyverse.html"><a href="a-fast-furious-overview-of-the-tidyverse.html"><i class="fa fa-check"></i><b>12</b> A fast, furious overview of the tidyverse</a></li>
<li class="chapter" data-level="13" data-path="authoring-reproducibly-with-rmarkdown.html"><a href="authoring-reproducibly-with-rmarkdown.html"><i class="fa fa-check"></i><b>13</b> Authoring reproducibly with Rmarkdown</a><ul>
<li class="chapter" data-level="13.1" data-path="authoring-reproducibly-with-rmarkdown.html"><a href="authoring-reproducibly-with-rmarkdown.html#notebooks"><i class="fa fa-check"></i><b>13.1</b> Notebooks</a></li>
<li class="chapter" data-level="13.2" data-path="authoring-reproducibly-with-rmarkdown.html"><a href="authoring-reproducibly-with-rmarkdown.html#references"><i class="fa fa-check"></i><b>13.2</b> References</a><ul>
<li class="chapter" data-level="13.2.1" data-path="authoring-reproducibly-with-rmarkdown.html"><a href="authoring-reproducibly-with-rmarkdown.html#zotero-and-rmarkdown"><i class="fa fa-check"></i><b>13.2.1</b> Zotero and Rmarkdown</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="authoring-reproducibly-with-rmarkdown.html"><a href="authoring-reproducibly-with-rmarkdown.html#bookdown"><i class="fa fa-check"></i><b>13.3</b> Bookdown</a></li>
<li class="chapter" data-level="13.4" data-path="authoring-reproducibly-with-rmarkdown.html"><a href="authoring-reproducibly-with-rmarkdown.html#google-docs"><i class="fa fa-check"></i><b>13.4</b> Google Docs</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="using-python.html"><a href="using-python.html"><i class="fa fa-check"></i><b>14</b> Using python</a></li>
<li class="part"><span><b>III Part III: Bioinformatic Analyses</b></span></li>
<li class="chapter" data-level="15" data-path="overview-of-bioinformatic-analyses.html"><a href="overview-of-bioinformatic-analyses.html"><i class="fa fa-check"></i><b>15</b> Overview of Bioinformatic Analyses</a></li>
<li class="chapter" data-level="16" data-path="dna-sequences-and-sequencing.html"><a href="dna-sequences-and-sequencing.html"><i class="fa fa-check"></i><b>16</b> DNA Sequences and Sequencing</a><ul>
<li class="chapter" data-level="16.1" data-path="dna-sequences-and-sequencing.html"><a href="dna-sequences-and-sequencing.html#dna-stuff"><i class="fa fa-check"></i><b>16.1</b> DNA Stuff</a><ul>
<li class="chapter" data-level="16.1.1" data-path="dna-sequences-and-sequencing.html"><a href="dna-sequences-and-sequencing.html#dna-replication-with-dna-polymerase"><i class="fa fa-check"></i><b>16.1.1</b> DNA Replication with DNA Polymerase</a></li>
<li class="chapter" data-level="16.1.2" data-path="dna-sequences-and-sequencing.html"><a href="dna-sequences-and-sequencing.html#the-importance-of-the-3-hydroxyl"><i class="fa fa-check"></i><b>16.1.2</b> The importance of the 3’ hydroxyl…</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="dna-sequences-and-sequencing.html"><a href="dna-sequences-and-sequencing.html#sanger-sequencing"><i class="fa fa-check"></i><b>16.2</b> Sanger sequencing</a></li>
<li class="chapter" data-level="16.3" data-path="dna-sequences-and-sequencing.html"><a href="dna-sequences-and-sequencing.html#illumina-sequencing-by-synthesis"><i class="fa fa-check"></i><b>16.3</b> Illumina Sequencing by Synthesis</a></li>
<li class="chapter" data-level="16.4" data-path="dna-sequences-and-sequencing.html"><a href="dna-sequences-and-sequencing.html#library-prep-protocols"><i class="fa fa-check"></i><b>16.4</b> Library Prep Protocols</a><ul>
<li class="chapter" data-level="16.4.1" data-path="dna-sequences-and-sequencing.html"><a href="dna-sequences-and-sequencing.html#wgs"><i class="fa fa-check"></i><b>16.4.1</b> WGS</a></li>
<li class="chapter" data-level="16.4.2" data-path="dna-sequences-and-sequencing.html"><a href="dna-sequences-and-sequencing.html#rad-seq-methods"><i class="fa fa-check"></i><b>16.4.2</b> RAD-Seq methods</a></li>
<li class="chapter" data-level="16.4.3" data-path="dna-sequences-and-sequencing.html"><a href="dna-sequences-and-sequencing.html#amplicon-sequencing"><i class="fa fa-check"></i><b>16.4.3</b> Amplicon Sequencing</a></li>
<li class="chapter" data-level="16.4.4" data-path="dna-sequences-and-sequencing.html"><a href="dna-sequences-and-sequencing.html#capture-arrays-rapture-etc."><i class="fa fa-check"></i><b>16.4.4</b> Capture arrays, RAPTURE, etc.</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html"><i class="fa fa-check"></i><b>17</b> Bioinformatic file formats</a><ul>
<li class="chapter" data-level="17.1" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#sequences"><i class="fa fa-check"></i><b>17.1</b> Sequences</a></li>
<li class="chapter" data-level="17.2" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#fastq"><i class="fa fa-check"></i><b>17.2</b> FASTQ</a><ul>
<li class="chapter" data-level="17.2.1" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#line-1-illumina-identifier-lines"><i class="fa fa-check"></i><b>17.2.1</b> Line 1: Illumina identifier lines</a></li>
<li class="chapter" data-level="17.2.2" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#line-4-base-quality-scores"><i class="fa fa-check"></i><b>17.2.2</b> Line 4: Base quality scores</a></li>
<li class="chapter" data-level="17.2.3" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#a-fastq-tidyverse-interlude"><i class="fa fa-check"></i><b>17.2.3</b> A FASTQ ‘tidyverse’ Interlude</a></li>
<li class="chapter" data-level="17.2.4" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#comparing-read-1-to-read-2"><i class="fa fa-check"></i><b>17.2.4</b> Comparing read 1 to read 2</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#fasta"><i class="fa fa-check"></i><b>17.3</b> FASTA</a><ul>
<li class="chapter" data-level="17.3.1" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#genomic-ranges"><i class="fa fa-check"></i><b>17.3.1</b> Genomic ranges</a></li>
<li class="chapter" data-level="17.3.2" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#extracting-genomic-ranges-from-a-fasta-file"><i class="fa fa-check"></i><b>17.3.2</b> Extracting genomic ranges from a FASTA file</a></li>
<li class="chapter" data-level="17.3.3" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#downloading-reference-genomes-from-ncbi"><i class="fa fa-check"></i><b>17.3.3</b> Downloading reference genomes from NCBI</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#alignments"><i class="fa fa-check"></i><b>17.4</b> Alignments</a><ul>
<li class="chapter" data-level="17.4.1" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#how-might-i-align-to-thee-let-me-count-the-ways"><i class="fa fa-check"></i><b>17.4.1</b> How might I align to thee? Let me count the ways…</a></li>
<li class="chapter" data-level="17.4.2" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#play-with-simple-alignments"><i class="fa fa-check"></i><b>17.4.2</b> Play with simple alignments</a></li>
<li class="chapter" data-level="17.4.3" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#sam-flags"><i class="fa fa-check"></i><b>17.4.3</b> SAM Flags</a></li>
<li class="chapter" data-level="17.4.4" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#the-cigar-string"><i class="fa fa-check"></i><b>17.4.4</b> The CIGAR string</a></li>
<li class="chapter" data-level="17.4.5" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#the-seq-and-qual-columns"><i class="fa fa-check"></i><b>17.4.5</b> The SEQ and QUAL columns</a></li>
<li class="chapter" data-level="17.4.6" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#sam-file-headers"><i class="fa fa-check"></i><b>17.4.6</b> SAM File Headers</a></li>
<li class="chapter" data-level="17.4.7" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#the-bam-format"><i class="fa fa-check"></i><b>17.4.7</b> The BAM format</a></li>
<li class="chapter" data-level="17.4.8" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#quick-self-study"><i class="fa fa-check"></i><b>17.4.8</b> Quick self study</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#variants"><i class="fa fa-check"></i><b>17.5</b> Variants</a></li>
<li class="chapter" data-level="17.6" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#segments"><i class="fa fa-check"></i><b>17.6</b> Segments</a></li>
<li class="chapter" data-level="17.7" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#conversionextractions-between-different-formats"><i class="fa fa-check"></i><b>17.7</b> Conversion/Extractions between different formats</a></li>
<li class="chapter" data-level="17.8" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#visualization-of-genomic-data"><i class="fa fa-check"></i><b>17.8</b> Visualization of Genomic Data</a><ul>
<li class="chapter" data-level="17.8.1" data-path="bioinformatic-file-formats.html"><a href="bioinformatic-file-formats.html#sample-data"><i class="fa fa-check"></i><b>17.8.1</b> Sample Data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="genome-assembly.html"><a href="genome-assembly.html"><i class="fa fa-check"></i><b>18</b> Genome Assembly</a></li>
<li class="chapter" data-level="19" data-path="alignment-of-sequence-data-to-a-reference-genome.html"><a href="alignment-of-sequence-data-to-a-reference-genome.html"><i class="fa fa-check"></i><b>19</b> Alignment of sequence data to a reference genome</a><ul>
<li class="chapter" data-level="19.1" data-path="alignment-of-sequence-data-to-a-reference-genome.html"><a href="alignment-of-sequence-data-to-a-reference-genome.html#preprocess"><i class="fa fa-check"></i><b>19.1</b> Preprocess ?</a></li>
<li class="chapter" data-level="19.2" data-path="alignment-of-sequence-data-to-a-reference-genome.html"><a href="alignment-of-sequence-data-to-a-reference-genome.html#read-groups"><i class="fa fa-check"></i><b>19.2</b> Read Groups</a></li>
<li class="chapter" data-level="19.3" data-path="alignment-of-sequence-data-to-a-reference-genome.html"><a href="alignment-of-sequence-data-to-a-reference-genome.html#merging-bam-files"><i class="fa fa-check"></i><b>19.3</b> Merging BAM files</a></li>
<li class="chapter" data-level="19.4" data-path="alignment-of-sequence-data-to-a-reference-genome.html"><a href="alignment-of-sequence-data-to-a-reference-genome.html#divide-and-conquer-strategies"><i class="fa fa-check"></i><b>19.4</b> Divide and Conquer Strategies</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="variant-calling-with-gatk.html"><a href="variant-calling-with-gatk.html"><i class="fa fa-check"></i><b>20</b> Variant calling with GATK</a></li>
<li class="chapter" data-level="21" data-path="bioinformatics-for-rad-seq-data-with-and-without-a-reference-genome.html"><a href="bioinformatics-for-rad-seq-data-with-and-without-a-reference-genome.html"><i class="fa fa-check"></i><b>21</b> Bioinformatics for RAD seq data with and without a reference genome</a></li>
<li class="chapter" data-level="22" data-path="processing-amplicon-sequencing-data.html"><a href="processing-amplicon-sequencing-data.html"><i class="fa fa-check"></i><b>22</b> Processing amplicon sequencing data</a></li>
<li class="chapter" data-level="23" data-path="genome-annotation.html"><a href="genome-annotation.html"><i class="fa fa-check"></i><b>23</b> Genome Annotation</a></li>
<li class="chapter" data-level="24" data-path="whole-genome-alignment-strategies.html"><a href="whole-genome-alignment-strategies.html"><i class="fa fa-check"></i><b>24</b> Whole genome alignment strategies</a><ul>
<li class="chapter" data-level="24.1" data-path="whole-genome-alignment-strategies.html"><a href="whole-genome-alignment-strategies.html#mapping-of-scaffolds-to-a-closely-related-genome"><i class="fa fa-check"></i><b>24.1</b> Mapping of scaffolds to a closely related genome</a></li>
<li class="chapter" data-level="24.2" data-path="whole-genome-alignment-strategies.html"><a href="whole-genome-alignment-strategies.html#obtaining-ancestral-states-from-an-outgroup-genome"><i class="fa fa-check"></i><b>24.2</b> Obtaining Ancestral States from an Outgroup Genome</a><ul>
<li class="chapter" data-level="24.2.1" data-path="whole-genome-alignment-strategies.html"><a href="whole-genome-alignment-strategies.html#using-lastz-to-align-coho-to-the-chinook-genome"><i class="fa fa-check"></i><b>24.2.1</b> Using LASTZ to align coho to the chinook genome</a></li>
<li class="chapter" data-level="24.2.2" data-path="whole-genome-alignment-strategies.html"><a href="whole-genome-alignment-strategies.html#try-on-the-chinook-chromosomes"><i class="fa fa-check"></i><b>24.2.2</b> Try on the chinook chromosomes</a></li>
<li class="chapter" data-level="24.2.3" data-path="whole-genome-alignment-strategies.html"><a href="whole-genome-alignment-strategies.html#explore-the-other-parameters-more"><i class="fa fa-check"></i><b>24.2.3</b> Explore the other parameters more</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Part IV: Analysis of Big Variant Data</b></span></li>
<li class="chapter" data-level="25" data-path="bioinformatic-analysis-on-variant-data.html"><a href="bioinformatic-analysis-on-variant-data.html"><i class="fa fa-check"></i><b>25</b> Bioinformatic analysis on variant data</a></li>
<li class="part"><span><b>V Part V: Population Genomics</b></span></li>
<li class="chapter" data-level="26" data-path="topics-in-pop-gen.html"><a href="topics-in-pop-gen.html"><i class="fa fa-check"></i><b>26</b> Topics in pop gen</a><ul>
<li class="chapter" data-level="26.1" data-path="topics-in-pop-gen.html"><a href="topics-in-pop-gen.html#coalescent"><i class="fa fa-check"></i><b>26.1</b> Coalescent</a></li>
<li class="chapter" data-level="26.2" data-path="topics-in-pop-gen.html"><a href="topics-in-pop-gen.html#measures-of-genetic-diversity-and-such"><i class="fa fa-check"></i><b>26.2</b> Measures of genetic diversity and such</a></li>
<li class="chapter" data-level="26.3" data-path="topics-in-pop-gen.html"><a href="topics-in-pop-gen.html#demographic-inference-with-partial-a-partial-i-and-moments"><i class="fa fa-check"></i><b>26.3</b> Demographic inference with <span class="math inline">\(\partial a \partial i\)</span> and <em>moments</em></a></li>
<li class="chapter" data-level="26.4" data-path="topics-in-pop-gen.html"><a href="topics-in-pop-gen.html#balls-in-boxes"><i class="fa fa-check"></i><b>26.4</b> Balls in Boxes</a></li>
<li class="chapter" data-level="26.5" data-path="topics-in-pop-gen.html"><a href="topics-in-pop-gen.html#some-landscape-genetics"><i class="fa fa-check"></i><b>26.5</b> Some landscape genetics</a></li>
<li class="chapter" data-level="26.6" data-path="topics-in-pop-gen.html"><a href="topics-in-pop-gen.html#relationship-inference"><i class="fa fa-check"></i><b>26.6</b> Relationship Inference</a></li>
<li class="chapter" data-level="26.7" data-path="topics-in-pop-gen.html"><a href="topics-in-pop-gen.html#tests-for-selection"><i class="fa fa-check"></i><b>26.7</b> Tests for Selection</a></li>
<li class="chapter" data-level="26.8" data-path="topics-in-pop-gen.html"><a href="topics-in-pop-gen.html#multivariate-associations-gea-etc."><i class="fa fa-check"></i><b>26.8</b> Multivariate Associations, GEA, etc.</a></li>
<li class="chapter" data-level="26.9" data-path="topics-in-pop-gen.html"><a href="topics-in-pop-gen.html#estimating-heritability-in-the-wild"><i class="fa fa-check"></i><b>26.9</b> Estimating heritability in the wild</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Practical Computing and Bioinformatics for Conservation and Evolutionary Genomics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="high-performance-computing-clusters-hpccs" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> High Performance Computing Clusters (HPCC’s)</h1>
<p>One interesting aspect of modern next generation sequencing data is simply its
sheer size: it is not uncommon to receive a half or a full terabyte of data from
a sequencing center. It is impractical to store this much data on your
laptop, let alone analyze it there. Crunching through such a quantity of data on a single
computer or server could take a very long time. Instead, you will likely break
up the analysis of such data into a number of smaller jobs, and send them off to
run on an assortment of different computers in a High Performance Computing Cluster (HPCC).</p>
<p>Thus, even if you have immersed yourself in bioinformatic data file formats and honed
your skills at shell programming and accessing remote computers, sequence data analysis
remains such a formidable foe that there is still one last key area of computing
in which you must be fluent, in order to comfortably do bioinformatics: you must understand
how to submit and manage jobs sent to an HPCC.</p>
<p>My first experience with HPCC’s occurred when I started analyzing
high-throughput sequencer output. I had over 15 years experience in shell
programming at that time, and I was given some example analysis scripts to emulate, but I still
found it took several weeks before I was moderately comfortable in an HPCC environment. Most
HPCC’s have some sort of tutorial web pages that provide a little bit of background
on cluster computing, but I didn’t find the ones available to me, at the time, to
be particularly helpful.</p>
<p>The goal of this chapter is to provide the sort of background I wish that I had
when I started doing cluster computing for bioinformatics. I will not be providing
a comprehensive overview of parallel computation. For example, we will not focus at all
upon the rich tradition of parallel computing applications through “message passing” interfaces which
can maintain a synchronized analysis from a single program executing
on multiple computers at the same time. Rather, we will focus on the manner in which
most bioinformatic problems can be broken down into a series of smaller jobs,
each of which can be run, independently, on its own processor without the
need for maintaining synchrony between multiple processes.</p>
<p>We start with an overview of what an HPCC consists of, defining a few important
terms. Then we provide some background on the fundamental problem of cluster
computing: namely that a lot of people want to use the computing power of the cluster,
but it needs to be allocated to users in an equitable fashion. An understanding of this
forms the basis for our discussion of <em>job scheduling</em> and the methods you must
use to tell the <em>job scheduler</em> what resources you will need, so that those resources
will eventually be allocated to you. We will cover a job scheduler called SLURM, which
stands for the Simple Linux Utility for Resource Management. It is the scheduler used
on the Summit supercomputer in Boulder and the Hummingbird and Sedna clusters deployed
at UCSC and the NWFSC, respectively.</p>
<p>After a discussion of SLURM, we will wrap up the chapter with a look at
methods for installing software to use for your analyses.</p>
<div id="an-oversimplified-but-useful-view-of-a-computing-cluster" class="section level2">
<h2><span class="header-section-number">8.1</span> An oversimplified, but useful, view of a computing cluster</h2>
<p>At its simplest, an HPCC can be thought of as a whole lot of computers that are all
put in a room somewhere for the purposes of doing a lot of computations. Most of these
computers do not possess all the elements that typically come to mind when you
think of a “computer.” For example, none of them are attached to monitors—each
computer resembles just the “box” part of a desktop computer. Each of these “boxes” is called
a <em>node</em>. The node is the unit in a cluster that corresponds most closely to what you think of
as a “computer.” As is typical of most computers today, each of these nodes has some
amount of Random Access Memory (RAM) that is <em>only accessible by the node itself</em>. RAM
is the space in memory that is used for active computation and calculation.</p>
<p>Each of these
nodes might also have a hard drive used for operating system software. The bulk of the
hard drive space that each node can access, however, is in the form of a large array of hard disks
that are connected to <em>all</em> of the nodes by an interface that allows data to be transferred
back and forth between each node and the hard-drive array at speeds that would be expected of
an internal hard drive (i.e., this array of hard drives is not just plugged in with a USB
cable). Memory on hard drives is used for storing data and the results of calculations, but
is not used for active calculations the way RAM is used. In order for calculations to be done
on data that are on the hard drive array, it must first be read into a node’s RAM. After the calculations
are done, the results are typically written back out onto the hard drive array.</p>
<p>On a typical cluster, there are usually several different “portions” of the hard drive
array attached to every <em>node</em>. One part of the array holds <em>home directories</em> of the
different users. Each user typically has a limited amount of storage in their home directory, but
the data in these home directories is usually safe or protected, meaning you can put a file there
and expect that it will be there next week, month, or year, etc. It is also likely that the home directories are
backed up (but check the docs for your cluster to confirm this!). Since the space in home directories is limited,
you typically will not put large data sets in your home directory for long-term storage, but you will
store scripts and programs, and such items as cloned GitHub repositories there. Another
type of storage that exists on the hard drive array is called <em>persistent long-term storage</em>. This type of storage
is purchased for use by research groups to store large quantities of
data on the cluster for long periods of time. As discussed in the last chapter, the rise of cloud-based storage solutions,
like Google Drive, offering unlimited storage to institutional users makes persistent long-term storage less
important (and less cost-effective) for many research groups. Finally, the third type of
storage in the hard drive array is called <em>scratch storage</em>. There are usually fairly
light limits (if any at all) to how much data can be placed in scratch storage, but there
will typically be Draconian <em>time limits</em> placed on your scratch storage. For example, on the
Hoffman2 cluster at UCLA, you are granted 2 Tb of <code>scratch</code> storage, but any files that
have sat unmodified in <code>scratch</code> for more than 14 days will be deleted (and if space is tight, the
system administrators may delete things from <code>scratch</code> in far fewer then 14 days.) Check
your local cluster documentation for information about time and space limits on <code>scratch</code>.</p>
<p>On many clusters, scratch space is also configured to be very fast for input and output
(for example, on many systems, the scratch storage will be composed of solid state drives rather
than spinning hard disks). On jobs that require that a lot of data be accessed from the
drive or written to it (this includes most operations on BAM files), significant decreases
in overall running time can be seen by using fast storage. Finally, scratch space exists on a cluster
expressly as <em>the place</em> to put data and outputs on the hard drive <em>when running jobs</em>. For all these
reasons, when you run jobs, you will always want to read and write data from and to <code>scratch</code>. We
will talk more about the specifics of doing so, but for now, you should be developing a generic picture
of your cluster computing workflow that looks like:</p>
<ol style="list-style-type: decimal">
<li>Download big data files from the cloud to <code>scratch</code></li>
<li>Run analyses on those data files, writing output back to <code>scratch</code>.</li>
<li>When done, copy, to the cloud, any products that you wish to keep.</li>
<li>Remove data and outputs left on <code>scratch</code>.</li>
</ol>
<p>As is the case with virtually all modern desktop or laptop computers, within each node,
there are multiple (typically between 16 and 48) <em>cores</em>, which are the computer chip units that actually
do the computations within a node. A <em>serial job</em> is one that just runs on a single core
within a <em>node</em>, while a <em>parallel job</em> might run on multiple cores, at the same time, within
a single node. In such a parallel job, each core has access to the same data within the
node’s RAM (a “shared-memory,” parallel job). The <em>core</em>
is the fundamental unit of computing machinery that gets allocated to perform jobs in an HPCC.</p>
<p>Most of the nodes in a cluster are there to hold the cores which are the computational workhorses,
slogging through calculations for the HPCC’s myriad users. However, some nodes
are more appropriate to certain types of computations than others (for example, some
might have lots of memory for doing genome assembly, while others will have
big, hurkin’, graphical processing units to be used for GPU calculations). Or, some nodes
might be available preferentially for different users than for others. For these
reasons, nodes are grouped into different collections. Depending on the system you are
using, these collections of different nodes are called, either, <em>partitions</em> or <em>queues</em>.</p>
<p>On every cluster, however, there will be one to several nodes that are reserved not for
doing computation, but for allowing users to access the cluster. These are called the
<em>login</em> nodes or the <em>head</em> nodes. These nodes are <em>solely</em> for logging in, light editing of
scripts, minor manipulation of directories, and scheduling and managing jobs. They are absolutely <em>not</em>
for doing major computations. For example, you should never login to the head node and immediately
start using it, in an interactive <code>bash</code> session to, say, sort BAM files or run <code>bwa mem</code>
to do alignments. Running commands that require a lot of computation, or a lot of
input and output from the disk, on the login nodes is an egregious
<em>faux pas</em> of cluster computing. Doing so can
negatively impact the ability of other users to login or otherwise get their work done. Therefore,
never do it! All of your hardcore computation on a cluster <em>has</em> to be done in a scheduled manner on
a <em>compute node</em>. We will show how to do that shortly, but first we will talk about why.</p>
</div>
<div id="cluster-computing-and-the-job-scheduler" class="section level2">
<h2><span class="header-section-number">8.2</span> Cluster computing and the job scheduler</h2>
<p>When you do work, or stream a video, or surf the web on your laptop computer, there are numerous
different computer processes running, to make sure that your computer keeps working and doing what it
is supposed to be doing. In the case of your laptop, the operating system, itself, orchestrates
all these different processes, making sure that each one is given some compute time on your
laptop in order to get its work done. Your laptop’s operating system has a fair bit
of flexibility in how it allocates resources to these different processes: it has multiple
cores to assign different processes to, <em>and</em> it
allows multiple processes to run on a single core, alternating between these different processes over
different <em>cycles</em> of the central processing unit. Things work differently
on a shared resource like an HPCC. The fundamental problem of cluster computing is basically
this: lots of people want to use the cluster to run big jobs, but the cluster does not
run like a single computer.</p>
<p>The cluster is not happy to give lots of different jobs
from lots of different users a chance to all run on the same core, sharing time by dividing up cycles.
When a user wants to use the computational resources of an HPCC,
she cannot just start a job and be confident that it will launch immediately and be granted
at least a few CPU cycles every now and again. Rather, on an HPCC, every <em>job</em> that is submitted
by a user will be assigned to a dedicated core (or several cores, if requested, and granted)
with a dedicated amount of memory.
If a core is not available for use, the job “gets in line” where it sits and waits (doing nothing) until a core
(with sufficient associated resources, like RAM memory) becomes available. When such a core
becomes available in the cluster, the job gets launched on it. All of this is orchestrated by the job scheduler,
of which SLURM is an example.</p>
<p>In this computing model, a job, once it is launched, ties up the core and the memory
that has been allocated to it until the job is finished. While that job is running, no one
else’s jobs or processes can run on the core or share the RAM memory that was allocated to the job.
For this reason, the job scheduler, needs to know, <em>ahead of time</em> how long each job might run and
what resources will be required during that time. A simple contrived example illustrates things easily:
imagine that Joe and Cheryl each have 1000 separate jobs to run. Each of Cheryl’s jobs involves running
a machine-learning algorithm to identify seabirds in high-resolution, aerial images of the ocean, and
takes only about 20 minutes running on a single core. Each of Joe’s jobs, on the other hand, involves mapping billions of
sequencing reads, a task which requires about 36 hours when run on a single core.
If their cluster has only 640 cores, and Joe submits his jobs first,
then, if the job scheduler were naive, it might put all of his jobs in line first, requiring some 50 or 60 hours
before the first of Cheryl’s jobs even runs. This would be a huge buzz kill for Cheryl.
However, if Cheryl and Joe both have to provide estimates to the scheduler of how
long their jobs will run, the scheduler can make more equitable decisions, starting a few of Joe’s jobs, but retaining
many more cores for Cheryl’s jobs, each of which runs much faster.</p>
<p>Thus, when you want to run any jobs on a cluster, you must provide an estimate of the resources
that the job will require. The three main axes upon which these resources are measured are:</p>
<ol style="list-style-type: decimal">
<li>The number of cores the job will require.</li>
<li>The maximum amount of RAM (memory) the job will require.</li>
<li>The amount of time for which the job will run.</li>
</ol>
<p>Requests for large amounts of resources for long periods of time generally take longer to start.
There are two main reasons for this: either 1) the scheduler does not want to launch too many
long-duration, high-memory jobs because it anticipates other users will want to use resources
down the road and no single user should tie up the compute resources
for too long; or 2) there are so many jobs running on myriad nodes and cores, that only
infrequently do nodes with sufficient numbers of cores and RAM come available to start
the new jobs.</p>
<p>The second reason is a particular bane of new cluster users who unwittingly request more resources
than actually exist (i.e. 52 cores, when no single node has more than 32, or 50 Gb of RAM when no single
node has more than 48 Gb). Unfortunately (or, perhaps comically, if you have a sick sense of
humor), most job schedulers will not notify you of this sort of transgression.
Rather, your job will just sit in line waiting to be launched, but it never will be, because sufficient
resources never become available!</p>
<p>It is worth noting that regardless of whether reason 1 or reason 2 is the dominant cause influencing
how long it takes to start a job, asking for fewer resources for less time will generally allow your
jobs to start faster. Particularly because of reason #2, however, breaking your jobs down (if possible) into
small chunks that will run relatively quickly on a single core with low RAM needs can render many more
opportunities for your jobs to start, letting you tap into resources that are not often fully utilized
in a cluster. Since I started working on large cluster in which it took a long time to start a job
that required all or most of the cores on a single node, but in which there were many nodes
harboring a few cores that were not being used, I tend to endorse this approach…</p>
<p>Since the requested resources for a job play such a large role in wait times for jobs to start,
you might wonder why people don’t intentionally underestimate the resources they request for their
jobs. The answer is simple: the job scheduler is a highly efficient and completely dispassionate
manager. If you requested 2 hours for your job, but your job has not finished in that amount of time,
the job scheduler will waste no time hemming and hawing or having an emotional struggle with itself
about whether it should stop your job. No, at 2 hours and 0.2 seconds it WILL kill your job, regardless
of whether it is just about to finish, or not. Similarly, if you requested 4 Gb or RAM, but five hours into
your job, the program you are running ends up using 5 Gb or RAM to store a large chunk of data, your
job WILL be killed, immediately.</p>
<p>Thus, it is best to be able to accurately estimate the time and resources a job will
require. You always want to request more time and resources than your job will
actually need, but not too much more. A large part of getting good at computing
in a shared cluster resource is gaining experience in predicting how long different jobs will
run, and how much RAM they will require. Later we will describe how the records of your
jobs, stored by the job scheduler, can be accessed and organized to aid in predicting
the resource demand of future jobs.</p>
</div>
<div id="the-slurm-commands" class="section level2">
<h2><span class="header-section-number">8.3</span> The SLURM commands</h2>
<p>Requests for resources (and for information about the computing cluster) are made to the job scheduler
using a few basic commands. As said, we will focus on the commands available in a SLURM-based system.
Before launching ourselves into the commands used to <em>launch</em> or <em>schedule</em> jobs we will explore the SLURM commands that
we can use to “get to know” our cluster, and to see how much of cluster’s resources are allocated to
currently running jobs.</p>
<p>Note to self: this is a good one for the .bashrc:</p>
<div class="sourceCode" id="cb233"><pre class="sourceCode sh"><code class="sourceCode bash"><a class="sourceLine" id="cb233-1" data-line-number="1"><span class="bu">alias</span> myjobs=<span class="st">&#39;squeue -u eriq@colostate.edu&#39;</span></a></code></pre></div>
<p>Maintaining focus
on such “poor-mans parallelization,” as I like to call it, allows us to maximize our
utilization of all the resources available on a computing cluster—not just those
that must be</p>
<p>Hey Eric! You might consider breaking this into two separate chatpers: 1 = working on remote computers
and 2 = high-performance computing. The first could include all the stuff about scp, globus, rclone,
and google drive.</p>
<p>This is going to be a chapter on using High Performance Computing clusters.</p>
<p>There is a lot to convey here about using queues and things.</p>
<p>I know SGE pretty well at this point, but others might use slurm.</p>
<p>Here is a good page with a comparison: <a href="https://confluence.csiro.au/display/SC/Reference+Guide%3A+Migrating+from+SGE+to+SLURM">https://confluence.csiro.au/display/SC/Reference+Guide%3A+Migrating+from+SGE+to+SLURM</a></p>
<p>And here is a good primer on SGE stuff: <a href="https://confluence.si.edu/display/HPC/Monitoring+your+Jobs">https://confluence.si.edu/display/HPC/Monitoring+your+Jobs</a></p>
<p>I guess I’ll have to see what the CSU students have access to.</p>
<p>Quick note: Redefine IFS to break on TABs so you can have full commands in there.
This is super useful for parsing job-array COMMLINES files.</p>
<pre><code>IFS=$&#39;\t\n&#39;; BOP=($(echo boing | awk &#39;{printf(&quot;first\tsecond\tthird that is long\tfourth\n&quot;);}&#39;)); IFS=$&#39; \t\n&#39;;</code></pre>
</div>
<div id="activatinginstalling-software" class="section level2">
<h2><span class="header-section-number">8.4</span> Activating/Installing software</h2>
<div id="modules" class="section level3">
<h3><span class="header-section-number">8.4.1</span> Modules</h3>
<p>This is if your sys admin has made it easy.</p>
</div>
<div id="miniconda" class="section level3">
<h3><span class="header-section-number">8.4.2</span> Miniconda</h3>
<p>This is how one will probably want to do it</p>
<div id="testing-this-on-summit" class="section level4">
<h4><span class="header-section-number">8.4.2.1</span> Testing this on Summit</h4>
<p>I just want to quickly try this:</p>
<div class="sourceCode" id="cb235"><pre class="sourceCode sh"><code class="sourceCode bash"><a class="sourceLine" id="cb235-1" data-line-number="1"><span class="fu">ssh</span> eriq@colostate.edu@login.rc.colorado.edu</a>
<a class="sourceLine" id="cb235-2" data-line-number="2"></a>
<a class="sourceLine" id="cb235-3" data-line-number="3"><span class="co"># get on compile nodes</span></a>
<a class="sourceLine" id="cb235-4" data-line-number="4"><span class="fu">ssh</span> scompile</a>
<a class="sourceLine" id="cb235-5" data-line-number="5"></a>
<a class="sourceLine" id="cb235-6" data-line-number="6"><span class="co"># I checked modules and found no samtools, bcftools, etc.</span></a>
<a class="sourceLine" id="cb235-7" data-line-number="7"></a>
<a class="sourceLine" id="cb235-8" data-line-number="8"><span class="co"># Now install miniconda</span></a>
<a class="sourceLine" id="cb235-9" data-line-number="9"><span class="fu">mkdir</span> conda_install</a>
<a class="sourceLine" id="cb235-10" data-line-number="10"><span class="bu">cd</span> conda_install/</a>
<a class="sourceLine" id="cb235-11" data-line-number="11"><span class="fu">wget</span> https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh</a>
<a class="sourceLine" id="cb235-12" data-line-number="12"><span class="fu">chmod</span> u+x Miniconda3-latest-Linux-x86_64.sh </a>
<a class="sourceLine" id="cb235-13" data-line-number="13"><span class="ex">./Miniconda3-latest-Linux-x86_64.sh</span> </a>
<a class="sourceLine" id="cb235-14" data-line-number="14"></a>
<a class="sourceLine" id="cb235-15" data-line-number="15"><span class="co"># then you follow the prompts and agree to the license and the </span></a>
<a class="sourceLine" id="cb235-16" data-line-number="16"><span class="co"># default install location.</span></a>
<a class="sourceLine" id="cb235-17" data-line-number="17"></a>
<a class="sourceLine" id="cb235-18" data-line-number="18"><span class="co">## </span><span class="al">NOTE</span><span class="co">: Might want to install to a different location if there</span></a>
<a class="sourceLine" id="cb235-19" data-line-number="19"><span class="co">## are serious caps on hard disk usage in home directory..</span></a>
<a class="sourceLine" id="cb235-20" data-line-number="20"></a>
<a class="sourceLine" id="cb235-21" data-line-number="21"><span class="bu">source</span> ~/.bashrc</a>
<a class="sourceLine" id="cb235-22" data-line-number="22"></a>
<a class="sourceLine" id="cb235-23" data-line-number="23"><span class="co"># after that I have it!</span></a>
<a class="sourceLine" id="cb235-24" data-line-number="24"><span class="kw">(</span><span class="ex">base</span><span class="kw">)</span> [<span class="ex">eriq@colostate.edu@shas0136</span> conda_install]$</a></code></pre></div>
<p>Now, let’s see if we can get samtools. Just google “samtools conda” to
get an idea of how to do it:</p>
<div class="sourceCode" id="cb236"><pre class="sourceCode sh"><code class="sourceCode bash"><a class="sourceLine" id="cb236-1" data-line-number="1"><span class="ex">conda</span> install -c bioconda samtools</a></code></pre></div>
<p>AFter that, it just works! Cool!</p>
<div class="sourceCode" id="cb237"><pre class="sourceCode sh"><code class="sourceCode bash"><a class="sourceLine" id="cb237-1" data-line-number="1"><span class="kw">(</span><span class="ex">base</span><span class="kw">)</span> [<span class="ex">eriq@colostate.edu@shas0136</span> ~]$ samtools</a>
<a class="sourceLine" id="cb237-2" data-line-number="2"></a>
<a class="sourceLine" id="cb237-3" data-line-number="3"><span class="ex">Program</span>: samtools (Tools for alignments in the SAM format)</a>
<a class="sourceLine" id="cb237-4" data-line-number="4"><span class="ex">Version</span>: 1.9 (using htslib 1.9)</a>
<a class="sourceLine" id="cb237-5" data-line-number="5"></a>
<a class="sourceLine" id="cb237-6" data-line-number="6"><span class="ex">Usage</span>:   samtools <span class="op">&lt;</span>command<span class="op">&gt;</span> [options]</a>
<a class="sourceLine" id="cb237-7" data-line-number="7"></a>
<a class="sourceLine" id="cb237-8" data-line-number="8"><span class="ex">Commands</span>:</a>
<a class="sourceLine" id="cb237-9" data-line-number="9">  <span class="ex">--</span> Indexing</a>
<a class="sourceLine" id="cb237-10" data-line-number="10">     <span class="ex">dict</span>           create a sequence dictionary file</a>
<a class="sourceLine" id="cb237-11" data-line-number="11">     <span class="ex">faidx</span>          index/extract FASTA</a>
<a class="sourceLine" id="cb237-12" data-line-number="12">     <span class="ex">fqidx</span>          index/extract FASTQ</a>
<a class="sourceLine" id="cb237-13" data-line-number="13">     <span class="ex">index</span>          index alignment</a>
<a class="sourceLine" id="cb237-14" data-line-number="14"></a>
<a class="sourceLine" id="cb237-15" data-line-number="15"><span class="ex">...</span></a></code></pre></div>
<p>All right! That is amazing. Next steps:</p>
<ol style="list-style-type: decimal">
<li>Tell students about establishing different environments.</li>
<li>Learn about how to make a minimal environment for a project and how to
record that and be able to distribute/propagate it.</li>
</ol>
<p>Along those lines, I want to see if, when I install samtools into a new environment,
it re-downloads it or not…</p>
<div class="sourceCode" id="cb238"><pre class="sourceCode sh"><code class="sourceCode bash"><a class="sourceLine" id="cb238-1" data-line-number="1"><span class="ex">conda</span> create --name quick-test</a>
<a class="sourceLine" id="cb238-2" data-line-number="2"><span class="ex">conda</span> activate quick-test</a>
<a class="sourceLine" id="cb238-3" data-line-number="3"></a>
<a class="sourceLine" id="cb238-4" data-line-number="4"><span class="co"># i also made an environment in a specified directory (you</span></a>
<a class="sourceLine" id="cb238-5" data-line-number="5"><span class="co"># could put these within a project directory)</span></a>
<a class="sourceLine" id="cb238-6" data-line-number="6"><span class="ex">conda</span> create --prefix ./test-envs-dir </a>
<a class="sourceLine" id="cb238-7" data-line-number="7"><span class="ex">conda</span> activate ./test-envs-dir</a>
<a class="sourceLine" id="cb238-8" data-line-number="8"></a>
<a class="sourceLine" id="cb238-9" data-line-number="9"><span class="co"># now, let&#39;s install bcftools there</span></a>
<a class="sourceLine" id="cb238-10" data-line-number="10"><span class="ex">conda</span> install -c bioconda bcftools</a>
<a class="sourceLine" id="cb238-11" data-line-number="11"></a>
<a class="sourceLine" id="cb238-12" data-line-number="12"><span class="co"># note that it doesn&#39;t re-download the dependencies, as far as I can tell.</span></a>
<a class="sourceLine" id="cb238-13" data-line-number="13"><span class="ex">conda</span> activate base</a>
<a class="sourceLine" id="cb238-14" data-line-number="14"><span class="ex">bcftools</span> <span class="co"># not found in environment base</span></a>
<a class="sourceLine" id="cb238-15" data-line-number="15"></a>
<a class="sourceLine" id="cb238-16" data-line-number="16"><span class="ex">conda</span> activate ./test-envs-dir</a>
<a class="sourceLine" id="cb238-17" data-line-number="17"><span class="ex">bcftools</span>  # it is found in this environment.  Cool.</a>
<a class="sourceLine" id="cb238-18" data-line-number="18"></a>
<a class="sourceLine" id="cb238-19" data-line-number="19"><span class="ex">conda</span> activate quick-test</a>
<a class="sourceLine" id="cb238-20" data-line-number="20"><span class="ex">bcftools</span>  # it ain<span class="st">&#39;t here</span></a></code></pre></div>
<p>Now, after that, bcftools is in ./test-envs-dir/bcftools</p>
<p>So, what if we install it into another environment. Does it symlink it?</p>
<div class="sourceCode" id="cb239"><pre class="sourceCode sh"><code class="sourceCode bash"><a class="sourceLine" id="cb239-1" data-line-number="1"><span class="ex">conda</span> activate base</a>
<a class="sourceLine" id="cb239-2" data-line-number="2"><span class="ex">conda</span> install -c bioconda bcftools</a>
<a class="sourceLine" id="cb239-3" data-line-number="3"><span class="ex">bcftools</span></a>
<a class="sourceLine" id="cb239-4" data-line-number="4"></a>
<a class="sourceLine" id="cb239-5" data-line-number="5"><span class="co"># whoa! That errored out!</span></a>
<a class="sourceLine" id="cb239-6" data-line-number="6"><span class="ex">bcftools</span>: error while loading shared libraries: libcrypto.so.1.0.0: cannot open shared object file: No such file or directory</a>
<a class="sourceLine" id="cb239-7" data-line-number="7"></a>
<a class="sourceLine" id="cb239-8" data-line-number="8"><span class="co"># that is a serious problem.  </span></a>
<a class="sourceLine" id="cb239-9" data-line-number="9"></a>
<a class="sourceLine" id="cb239-10" data-line-number="10"><span class="co"># Can I get it in my other environment?</span></a>
<a class="sourceLine" id="cb239-11" data-line-number="11"><span class="ex">conda</span> activate quick-test</a>
<a class="sourceLine" id="cb239-12" data-line-number="12"><span class="ex">conda</span> install -c bioconda bcftools</a>
<a class="sourceLine" id="cb239-13" data-line-number="13"><span class="ex">bcftools</span></a>
<a class="sourceLine" id="cb239-14" data-line-number="14"></a>
<a class="sourceLine" id="cb239-15" data-line-number="15"><span class="co"># that totally works, but it still doesn&#39;t in base...</span></a>
<a class="sourceLine" id="cb239-16" data-line-number="16"></a>
<a class="sourceLine" id="cb239-17" data-line-number="17"><span class="co"># so, what if we add samtools to our other environments?</span></a>
<a class="sourceLine" id="cb239-18" data-line-number="18"><span class="co"># that works fine.</span></a></code></pre></div>
<p>But, samtools/bcftools dependency issues are a known problem: <a href="https://github.com/sunbeam-labs/sunbeam/issues/181" class="uri">https://github.com/sunbeam-labs/sunbeam/issues/181</a>
Basically they rely on different versions of some ssh libs.</p>
<p>Note that installing bcftools first things work. But what if we make another environment
and install samtools first again?</p>
<div class="sourceCode" id="cb240"><pre class="sourceCode sh"><code class="sourceCode bash"><a class="sourceLine" id="cb240-1" data-line-number="1"><span class="ex">conda</span> create --name samtools-first</a>
<a class="sourceLine" id="cb240-2" data-line-number="2"><span class="ex">conda</span> activate samtools-first</a>
<a class="sourceLine" id="cb240-3" data-line-number="3"><span class="ex">conda</span> install -c bioconda samtools  # this didn<span class="st">&#39;t download anything new</span></a>
<a class="sourceLine" id="cb240-4" data-line-number="4"><span class="st">conda install -c bioconda bcftools</span></a>
<a class="sourceLine" id="cb240-5" data-line-number="5"></a>
<a class="sourceLine" id="cb240-6" data-line-number="6"><span class="st"># BOOM! THIS CREATES A FAIL.  SO, YOU GOTTA INSTALL BCFTOOLS</span></a>
<a class="sourceLine" id="cb240-7" data-line-number="7"><span class="st"># FIRST.  FAR OUT.</span></a></code></pre></div>
</div>
</div>
<div id="exporting-environments" class="section level3">
<h3><span class="header-section-number">8.4.3</span> Exporting environments</h3>
<p>Looks like we should be able to do this. Let’s do the one that works:</p>
<div class="sourceCode" id="cb241"><pre class="sourceCode sh"><code class="sourceCode bash"><a class="sourceLine" id="cb241-1" data-line-number="1"><span class="kw">(</span><span class="ex">quick-test</span><span class="kw">)</span> [<span class="ex">~</span>]<span class="ex">--%</span> conda env export <span class="op">&gt;</span> quick-test-env.yml</a>
<a class="sourceLine" id="cb241-2" data-line-number="2"></a>
<a class="sourceLine" id="cb241-3" data-line-number="3"><span class="kw">(</span><span class="ex">quick-test</span><span class="kw">)</span> [<span class="ex">~</span>]<span class="ex">--%</span> cat quick-test-env.yml </a>
<a class="sourceLine" id="cb241-4" data-line-number="4"><span class="ex">name</span>: quick-test</a>
<a class="sourceLine" id="cb241-5" data-line-number="5"><span class="ex">channels</span>:</a>
<a class="sourceLine" id="cb241-6" data-line-number="6">  <span class="ex">-</span> bioconda</a>
<a class="sourceLine" id="cb241-7" data-line-number="7">  <span class="ex">-</span> defaults</a>
<a class="sourceLine" id="cb241-8" data-line-number="8"><span class="ex">dependencies</span>:</a>
<a class="sourceLine" id="cb241-9" data-line-number="9">  <span class="ex">-</span> _libgcc_mutex=0.1=main</a>
<a class="sourceLine" id="cb241-10" data-line-number="10">  <span class="ex">-</span> bcftools=1.9=ha228f0b_4</a>
<a class="sourceLine" id="cb241-11" data-line-number="11">  <span class="ex">-</span> bzip2=1.0.8=h7b6447c_0</a>
<a class="sourceLine" id="cb241-12" data-line-number="12">  <span class="ex">-</span> ca-certificates=2019.5.15=1</a>
<a class="sourceLine" id="cb241-13" data-line-number="13">  <span class="ex">-</span> curl=7.65.3=hbc83047_0</a>
<a class="sourceLine" id="cb241-14" data-line-number="14">  <span class="ex">-</span> htslib=1.9=ha228f0b_7</a>
<a class="sourceLine" id="cb241-15" data-line-number="15">  <span class="ex">-</span> krb5=1.16.1=h173b8e3_7</a>
<a class="sourceLine" id="cb241-16" data-line-number="16">  <span class="ex">-</span> libcurl=7.65.3=h20c2e04_0</a>
<a class="sourceLine" id="cb241-17" data-line-number="17">  <span class="ex">-</span> libdeflate=1.0=h14c3975_1</a>
<a class="sourceLine" id="cb241-18" data-line-number="18">  <span class="ex">-</span> libedit=3.1.20181209=hc058e9b_0</a>
<a class="sourceLine" id="cb241-19" data-line-number="19">  <span class="ex">-</span> libgcc-ng=9.1.0=hdf63c60_0</a>
<a class="sourceLine" id="cb241-20" data-line-number="20">  <span class="ex">-</span> libssh2=1.8.2=h1ba5d50_0</a>
<a class="sourceLine" id="cb241-21" data-line-number="21">  <span class="ex">-</span> libstdcxx-ng=9.1.0=hdf63c60_0</a>
<a class="sourceLine" id="cb241-22" data-line-number="22">  <span class="ex">-</span> ncurses=6.1=he6710b0_1</a>
<a class="sourceLine" id="cb241-23" data-line-number="23">  <span class="ex">-</span> openssl=1.1.1c=h7b6447c_1</a>
<a class="sourceLine" id="cb241-24" data-line-number="24">  <span class="ex">-</span> samtools=1.9=h10a08f8_12</a>
<a class="sourceLine" id="cb241-25" data-line-number="25">  <span class="ex">-</span> tk=8.6.8=hbc83047_0</a>
<a class="sourceLine" id="cb241-26" data-line-number="26">  <span class="ex">-</span> xz=5.2.4=h14c3975_4</a>
<a class="sourceLine" id="cb241-27" data-line-number="27">  <span class="ex">-</span> zlib=1.2.11=h7b6447c_3</a>
<a class="sourceLine" id="cb241-28" data-line-number="28"><span class="ex">prefix</span>: /home/eriq@colostate.edu/miniconda3/envs/quick-test</a>
<a class="sourceLine" id="cb241-29" data-line-number="29"></a>
<a class="sourceLine" id="cb241-30" data-line-number="30"></a>
<a class="sourceLine" id="cb241-31" data-line-number="31"><span class="co"># OK, that is cool.  Now, if we wanted to email that to someone,</span></a>
<a class="sourceLine" id="cb241-32" data-line-number="32"><span class="co"># we could, and then they could do this:</span></a>
<a class="sourceLine" id="cb241-33" data-line-number="33"><span class="ex">conda</span> env create --name dupie-quick -f quick-test-env.yml </a>
<a class="sourceLine" id="cb241-34" data-line-number="34"></a>
<a class="sourceLine" id="cb241-35" data-line-number="35"><span class="co"># that environment then has samtools and bcftools</span></a>
<a class="sourceLine" id="cb241-36" data-line-number="36"></a>
<a class="sourceLine" id="cb241-37" data-line-number="37"><span class="co"># note that it probably would try to name it quick-test if we didn&#39;t</span></a>
<a class="sourceLine" id="cb241-38" data-line-number="38"><span class="co"># pass in the name there...</span></a></code></pre></div>
</div>
</div>
<div id="boneyard" class="section level2">
<h2><span class="header-section-number">8.5</span> Boneyard</h2>
<p>This is a variant of rsync that let’s you sync stuff up to google drive. It might be a better
solution than rcp for getting stuff onto and off of google drive. Here is a link:
<a href="https://rclone.org/">https://rclone.org/</a>. I need to evaluate it. It might also be a good way
to backup some of my workstuff on my laptop to Google Drive (and maybe also for other people to create replicas and have a decent backup if they have unlimited Google Drive storage).</p>
<p>I got this working. It is important to set your own OAuth client ID:
<a href="https://forum.rclone.org/t/very-slow-sync-to-google-drive/6903">https://forum.rclone.org/t/very-slow-sync-to-google-drive/6903</a></p>
<p>After that I did like this:</p>
<pre><code>rclone sync -vv --tpslimit 10 --fast-list Otsh_v1.0_genomic.fna  gdrive-rclone:spoogee-spoogee</code></pre>
<p>which did 2 Gb of fasta into the spoogee-spoogee directory pretty quickly.</p>
<p>But, with something that has lots of files, it took longer:</p>
<pre><code># this is only about 100 Mb but took a long time
rclone copy -P --tpslimit 10 --fast-list  rubias  gdrive-rclone:rubias</code></pre>
<p>However, once that is done, you can sync it and it finds that parts that have changed pretty quickly.</p>
<p>it appears to do that by file modification times:</p>
<div class="sourceCode" id="cb244"><pre class="sourceCode sh"><code class="sourceCode bash"><a class="sourceLine" id="cb244-1" data-line-number="1"><span class="ex">2019-04-19</span> 23:21 /Otsh_v1.0/--% rclone sync -vv --tpslimit 10 --fast-list Otsh_v1.0_genomic.fna  gdrive-rclone:spoogee-spoogee</a>
<a class="sourceLine" id="cb244-2" data-line-number="2"><span class="ex">2019/04/19</span> 23:21:36 DEBUG : rclone: Version <span class="st">&quot;v1.47.0&quot;</span> starting with parameters [<span class="st">&quot;rclone&quot;</span> <span class="st">&quot;sync&quot;</span> <span class="st">&quot;-vv&quot;</span> <span class="st">&quot;--tpslimit&quot;</span> <span class="st">&quot;10&quot;</span> <span class="st">&quot;--fast-list&quot;</span> <span class="st">&quot;Otsh_v1.0_genomic.fna&quot;</span> <span class="st">&quot;gdrive-rclone:spoogee-spoogee&quot;</span>]</a>
<a class="sourceLine" id="cb244-3" data-line-number="3"><span class="ex">2019/04/19</span> 23:21:36 DEBUG : Using config file from <span class="st">&quot;/Users/eriq/.config/rclone/rclone.conf&quot;</span></a>
<a class="sourceLine" id="cb244-4" data-line-number="4"><span class="ex">2019/04/19</span> 23:21:36 INFO  : Starting HTTP transaction limiter: max 10 transactions/s with burst 1</a>
<a class="sourceLine" id="cb244-5" data-line-number="5"><span class="ex">2019/04/19</span> 23:21:37 DEBUG : GCF_002872995.1_Otsh_v1.0_genomic.gff.gz: Excluded</a>
<a class="sourceLine" id="cb244-6" data-line-number="6"><span class="ex">2019/04/19</span> 23:21:37 DEBUG : Otsh_v1.0_genomic.dict: Excluded</a>
<a class="sourceLine" id="cb244-7" data-line-number="7"><span class="ex">2019/04/19</span> 23:21:37 DEBUG : Otsh_v1.0_genomic.fna.amb: Excluded</a>
<a class="sourceLine" id="cb244-8" data-line-number="8"><span class="ex">2019/04/19</span> 23:21:37 DEBUG : Otsh_v1.0_genomic.fna.ann: Excluded</a>
<a class="sourceLine" id="cb244-9" data-line-number="9"><span class="ex">2019/04/19</span> 23:21:37 DEBUG : Otsh_v1.0_genomic.fna.bwt: Excluded</a>
<a class="sourceLine" id="cb244-10" data-line-number="10"><span class="ex">2019/04/19</span> 23:21:37 DEBUG : Otsh_v1.0_genomic.fna.fai: Excluded</a>
<a class="sourceLine" id="cb244-11" data-line-number="11"><span class="ex">2019/04/19</span> 23:21:37 DEBUG : Otsh_v1.0_genomic.fna.pac: Excluded</a>
<a class="sourceLine" id="cb244-12" data-line-number="12"><span class="ex">2019/04/19</span> 23:21:37 DEBUG : Otsh_v1.0_genomic.fna.sa: Excluded</a>
<a class="sourceLine" id="cb244-13" data-line-number="13"><span class="ex">2019/04/19</span> 23:21:37 INFO  : Google drive root <span class="st">&#39;spoogee-spoogee&#39;</span>: Waiting for checks to finish</a>
<a class="sourceLine" id="cb244-14" data-line-number="14"><span class="ex">2019/04/19</span> 23:21:37 DEBUG : Otsh_v1.0_genomic.fna: Size and modification time the same (differ by 0s, within tolerance 1s)</a>
<a class="sourceLine" id="cb244-15" data-line-number="15"><span class="ex">2019/04/19</span> 23:21:37 DEBUG : Otsh_v1.0_genomic.fna: Unchanged skipping</a>
<a class="sourceLine" id="cb244-16" data-line-number="16"><span class="ex">2019/04/19</span> 23:21:37 INFO  : Google drive root <span class="st">&#39;spoogee-spoogee&#39;</span>: Waiting for transfers to finish</a>
<a class="sourceLine" id="cb244-17" data-line-number="17"><span class="ex">2019/04/19</span> 23:21:37 INFO  : Waiting for deletions to finish</a>
<a class="sourceLine" id="cb244-18" data-line-number="18"><span class="ex">2019/04/19</span> 23:21:37 INFO  : </a>
<a class="sourceLine" id="cb244-19" data-line-number="19"><span class="ex">Transferred</span>:             0 / 0 Bytes, -, 0 Bytes/s, ETA -</a>
<a class="sourceLine" id="cb244-20" data-line-number="20"><span class="ex">Errors</span>:                 0</a>
<a class="sourceLine" id="cb244-21" data-line-number="21"><span class="ex">Checks</span>:                 1 / 1, 100%</a>
<a class="sourceLine" id="cb244-22" data-line-number="22"><span class="ex">Transferred</span>:            0 / 0, -</a>
<a class="sourceLine" id="cb244-23" data-line-number="23"><span class="ex">Elapsed</span> time:        1.3s</a>
<a class="sourceLine" id="cb244-24" data-line-number="24"></a>
<a class="sourceLine" id="cb244-25" data-line-number="25"><span class="ex">2019/04/19</span> 23:21:37 DEBUG : 5 go routines active</a>
<a class="sourceLine" id="cb244-26" data-line-number="26"><span class="ex">2019/04/19</span> 23:21:37 DEBUG : rclone: Version <span class="st">&quot;v1.47.0&quot;</span> finishing with parameters [<span class="st">&quot;rclone&quot;</span> <span class="st">&quot;sync&quot;</span> <span class="st">&quot;-vv&quot;</span> <span class="st">&quot;--tpslimit&quot;</span> <span class="st">&quot;10&quot;</span> <span class="st">&quot;--fast-list&quot;</span> <span class="st">&quot;Otsh_v1.0_genomic.fna&quot;</span> <span class="st">&quot;gdrive-rclone:spoogee-spoogee&quot;</span>]</a>
<a class="sourceLine" id="cb244-27" data-line-number="27"><span class="ex">2</span></a></code></pre></div>
<p>So, for moving big files around that might be a good way forward. I will have to do a test with some big files.</p>
<p>And I need to test it with team drives so that multiple individuals can pull stuff off of the Bird Genoscape drive for example.</p>
<p>It would be nice to have safeguards so people don’t trash stuff accidentally….</p>
<div id="rclone-on-hoffman" class="section level4">
<h4><span class="header-section-number">8.5.0.1</span> rclone on Hoffman</h4>
<p>Their default install script expects sudo access to put it in /usr/local
but I don’t on hoffman, obviously, so I just downloaded the the install script and edited
the section for Linux to look like this at the relevant part</p>
<div class="sourceCode" id="cb245"><pre class="sourceCode sh"><code class="sourceCode bash"><a class="sourceLine" id="cb245-1" data-line-number="1"><span class="kw">case</span> <span class="va">$OS</span><span class="kw"> in</span></a>
<a class="sourceLine" id="cb245-2" data-line-number="2">  <span class="st">&#39;linux&#39;</span><span class="kw">)</span></a>
<a class="sourceLine" id="cb245-3" data-line-number="3">    <span class="co">#binary</span></a>
<a class="sourceLine" id="cb245-4" data-line-number="4">    <span class="fu">cp</span> rclone ~/bin/rclone.new</a>
<a class="sourceLine" id="cb245-5" data-line-number="5">    <span class="fu">chmod</span> 755 ~/bin/rclone.new</a>
<a class="sourceLine" id="cb245-6" data-line-number="6">    <span class="co">#chown root:root /usr/bin/rclone.new</span></a>
<a class="sourceLine" id="cb245-7" data-line-number="7">    <span class="fu">mv</span> ~/bin/rclone.new ~/bin/rclone</a>
<a class="sourceLine" id="cb245-8" data-line-number="8">    <span class="co">#manuals</span></a>
<a class="sourceLine" id="cb245-9" data-line-number="9">    <span class="co">#mkdir -p /usr/local/share/man/man1</span></a>
<a class="sourceLine" id="cb245-10" data-line-number="10">    <span class="co">#cp rclone.1 /usr/local/share/man/man1/</span></a>
<a class="sourceLine" id="cb245-11" data-line-number="11">    <span class="co">#mandb</span></a>
<a class="sourceLine" id="cb245-12" data-line-number="12">    <span class="kw">;;</span></a></code></pre></div>
<p>I don’t get man pages, but I get it in ~/bin no problem.</p>
<p>To set up the configuration, check where it belongs:</p>
<div class="sourceCode" id="cb246"><pre class="sourceCode sh"><code class="sourceCode bash"><a class="sourceLine" id="cb246-1" data-line-number="1"><span class="ex">%</span> rclone config file</a>
<a class="sourceLine" id="cb246-2" data-line-number="2"><span class="ex">Configuration</span> file doesn<span class="st">&#39;t exist, but rclone will use this path:</span></a>
<a class="sourceLine" id="cb246-3" data-line-number="3"><span class="st">/u/home/e/eriq/.config/rclone/rclone.conf</span></a></code></pre></div>
<p>And then I just put my config file from my laptop on there. I just pasted the stuff
in whilst emacsing it. Holy cow! That is super easy.</p>
<p>Note that the config file is where you can also set default options like tpslimit and fast-list I think.</p>
<p>So, the OAuth stuff is all stored in that config file. And if you can set it up on one machine you can
go put it on any others that you want. That is awesome.</p>
<p>When it was done, I tested it:</p>
<div class="sourceCode" id="cb247"><pre class="sourceCode sh"><code class="sourceCode bash"><a class="sourceLine" id="cb247-1" data-line-number="1"><span class="ex">%</span> rclone sync -vv  --drive-shared-with-me  gdrive-rclone:BaselinePaper  BaselinePaper_here</a>
<a class="sourceLine" id="cb247-2" data-line-number="2"><span class="ex">2019/04/29</span> 14:49:24 DEBUG : rclone: Version <span class="st">&quot;v1.47.0&quot;</span> starting with parameters [<span class="st">&quot;rclone&quot;</span> <span class="st">&quot;sync&quot;</span> <span class="st">&quot;-vv&quot;</span> <span class="st">&quot;--drive-shared-with-me&quot;</span> <span class="st">&quot;gdrive-rclone:BaselinePaper&quot;</span> <span class="st">&quot;BaselinePaper_here&quot;</span>]</a>
<a class="sourceLine" id="cb247-3" data-line-number="3"><span class="ex">2019/04/29</span> 14:49:24 DEBUG : Using config file from <span class="st">&quot;/u/home/e/eriq/.config/rclone/rclone.conf&quot;</span></a>
<a class="sourceLine" id="cb247-4" data-line-number="4"><span class="ex">2019/04/29</span> 14:49:25 INFO  : Local file system at /u/home/e/eriq/BaselinePaper_here: Waiting for checks to finish</a>
<a class="sourceLine" id="cb247-5" data-line-number="5"><span class="ex">2019/04/29</span> 14:49:25 INFO  : Local file system at /u/home/e/eriq/BaselinePaper_here: Waiting for transfers to finish</a>
<a class="sourceLine" id="cb247-6" data-line-number="6"><span class="ex">2019/04/29</span> 14:49:26 DEBUG : Local file system at /u/home/e/eriq/BaselinePaper_here: File to upload is small (41922 bytes), <span class="ex">uploading</span> instead of streaming</a>
<a class="sourceLine" id="cb247-7" data-line-number="7"><span class="ex">2019/04/29</span> 14:49:26 DEBUG : BaselinePaper_Body.docx: Failed to pre-allocate: operation not supported</a>
<a class="sourceLine" id="cb247-8" data-line-number="8"><span class="ex">2019/04/29</span> 14:49:26 INFO  : BaselinePaper_Body.docx: Copied (new)</a>
<a class="sourceLine" id="cb247-9" data-line-number="9"><span class="ex">2019/04/29</span> 14:49:26 DEBUG : BaselinePaper_Body.docx: Updating size of doc after download to 41922</a>
<a class="sourceLine" id="cb247-10" data-line-number="10"><span class="ex">2019/04/29</span> 14:49:26 INFO  : BaselinePaper_Body.docx: Copied (Rcat, new)</a>
<a class="sourceLine" id="cb247-11" data-line-number="11"><span class="ex">2019/04/29</span> 14:49:27 DEBUG : Local file system at /u/home/e/eriq/BaselinePaper_here: File to upload is small (57172 bytes), <span class="ex">uploading</span> instead of streaming</a>
<a class="sourceLine" id="cb247-12" data-line-number="12"><span class="ex">2019/04/29</span> 14:49:27 DEBUG : ResponseToReviewers_eca.docx: Failed to pre-allocate: operation not supported</a>
<a class="sourceLine" id="cb247-13" data-line-number="13"><span class="ex">2019/04/29</span> 14:49:27 INFO  : ResponseToReviewers_eca.docx: Copied (new)</a>
<a class="sourceLine" id="cb247-14" data-line-number="14"><span class="ex">2019/04/29</span> 14:49:27 DEBUG : ResponseToReviewers_eca.docx: Updating size of doc after download to 57172</a>
<a class="sourceLine" id="cb247-15" data-line-number="15"><span class="ex">2019/04/29</span> 14:49:27 INFO  : ResponseToReviewers_eca.docx: Copied (Rcat, new)</a>
<a class="sourceLine" id="cb247-16" data-line-number="16"><span class="ex">2019/04/29</span> 14:49:27 INFO  : Waiting for deletions to finish</a>
<a class="sourceLine" id="cb247-17" data-line-number="17"><span class="ex">2019/04/29</span> 14:49:27 INFO  : </a>
<a class="sourceLine" id="cb247-18" data-line-number="18"><span class="ex">Transferred</span>:      193.543k / 193.543 kBytes, 100%, 79.377 kBytes/s, ETA 0s</a>
<a class="sourceLine" id="cb247-19" data-line-number="19"><span class="ex">Errors</span>:                 0</a>
<a class="sourceLine" id="cb247-20" data-line-number="20"><span class="ex">Checks</span>:                 0 / 0, -</a>
<a class="sourceLine" id="cb247-21" data-line-number="21"><span class="ex">Transferred</span>:            4 / 4, 100%</a>
<a class="sourceLine" id="cb247-22" data-line-number="22"><span class="ex">Elapsed</span> time:        2.4s</a>
<a class="sourceLine" id="cb247-23" data-line-number="23"></a>
<a class="sourceLine" id="cb247-24" data-line-number="24"><span class="ex">2019/04/29</span> 14:49:27 DEBUG : 6 go routines active</a>
<a class="sourceLine" id="cb247-25" data-line-number="25"><span class="ex">2019/04/29</span> 14:49:27 DEBUG : rclone: Version <span class="st">&quot;v1.47.0&quot;</span> finishing with parameters [<span class="st">&quot;rclone&quot;</span> <span class="st">&quot;sync&quot;</span> <span class="st">&quot;-vv&quot;</span> <span class="st">&quot;--drive-shared-with-me&quot;</span> <span class="st">&quot;gdrive-rclone:BaselinePaper&quot;</span> <span class="st">&quot;BaselinePaper_here&quot;</span>]</a></code></pre></div>
<p>That was fast and super solid.</p>
</div>
<div id="encrypt-the-config-file" class="section level4">
<h4><span class="header-section-number">8.5.0.2</span> Encrypt the config file</h4>
<p>You can use <code>rclone config edit</code> to set a password for the config file. Then it
encrypts that so no one is able to run wild if they just get that file. You have to
provide your password to do any of the rclone commands. If you want to see the
config file use <code>rclone config show</code>. You could always copy that elsewhere, and then
re-encrypt it.</p>
<p>Here is some nice stuff for summarizing all the information from the different runs from the chinook-wgs project:</p>
<div class="sourceCode" id="cb248"><pre class="sourceCode sh"><code class="sourceCode bash"><a class="sourceLine" id="cb248-1" data-line-number="1"><span class="ex">qacct</span> -o eriq -b 09271925 -j ml <span class="kw">|</span> <span class="ex">tidy-qacct</span></a></code></pre></div>
<p>Explain scratch space and how clusters are configured with respect to storage, etc.</p>
<p>Strategies—break names up with consistent characters:</p>
<ul>
<li>dashes within population names</li>
<li>underscores for different groups of chromosomes</li>
<li>periods for catenating pairs of pops</li>
</ul>
<p>etc. Basically, it just makes it much easier to split things up
when the time comes.</p>
</div>
</div>
<div id="the-queue-slurmsgeuge" class="section level2">
<h2><span class="header-section-number">8.6</span> The Queue (SLURM/SGE/UGE)</h2>
</div>
<div id="modules-package" class="section level2">
<h2><span class="header-section-number">8.7</span> Modules package</h2>
</div>
<div id="compiling-programs-without-admin-privileges" class="section level2">
<h2><span class="header-section-number">8.8</span> Compiling programs without admin privileges</h2>
<p>Inevitably you will want to use a piece of software that is not available as
a module or is not otherwise installed on they system.</p>
<p>Typically these software programs have a frightful web of dependencies.</p>
<p>Unix/Linux distros typically maintain all these dependencies as libraries or packages
that can be installed using a <code>rpm</code> or <code>yum</code>. However, the simple “plug-and-play” approach
to using these programs requires have administrator privileges so that the software can
be installed in one of the (typically protected) paths in the root (like <code>/usr/bin</code>).</p>
<p>But, you can use these programs to install packages into your home directory. Once you have done
that, you need to let your system know where to look for these packages when it needs them
(i.e., when running a program or <em>linking</em> to it whilst compiling up a program that uses it
as a dependency.</p>
<p>Hoffman2 runs CentOS. Turns out that CentOS uses <code>yum</code> as a package manager.</p>
<p>Let’s see if we can install llvm using yum.</p>
<div class="sourceCode" id="cb249"><pre class="sourceCode sh"><code class="sourceCode bash"><a class="sourceLine" id="cb249-1" data-line-number="1"><span class="ex">yum</span> search all llvm <span class="co"># &lt;- this got me to devtoolset-7-all.x86_64 : Package shipping all available toolsets.</span></a>
<a class="sourceLine" id="cb249-2" data-line-number="2"></a>
<a class="sourceLine" id="cb249-3" data-line-number="3"><span class="co"># a little web searching made it look like llvm-toolset-7-5.0.1-4.el7.x86_64.rpm or devtoolset-7-llvm-7.0-5.el7.x86_64.rpm</span></a>
<a class="sourceLine" id="cb249-4" data-line-number="4"><span class="co"># might be what we want.  The first is a dependency of the second...</span></a>
<a class="sourceLine" id="cb249-5" data-line-number="5"><span class="fu">mkdir</span> ~/centos</a></code></pre></div>
<p>Was using instructions at <a href="https://stackoverflow.com/questions/36651091/how-to-install-packages-in-linux-centos-without-root-user-with-automatic-depen">https://stackoverflow.com/questions/36651091/how-to-install-packages-in-linux-centos-without-root-user-with-automatic-depen</a></p>
<p>Couldn’t get yum downloader to download any packages. The whole thing looked like it was going to
be a mess, so I thought I would try with miniconda.</p>
<p>I installed miniconda (python 2.7 version) into <code>/u/nobackup/kruegg/eriq/programs/miniconda/</code> and then did this:</p>
<div class="sourceCode" id="cb250"><pre class="sourceCode sh"><code class="sourceCode bash"><a class="sourceLine" id="cb250-1" data-line-number="1"><span class="co"># probably could have listed them all at once, but wanted to watch them go </span></a>
<a class="sourceLine" id="cb250-2" data-line-number="2"><span class="co"># one at a time...</span></a>
<a class="sourceLine" id="cb250-3" data-line-number="3"><span class="ex">conda</span> install numpy</a>
<a class="sourceLine" id="cb250-4" data-line-number="4"><span class="ex">conda</span> install scipy</a>
<a class="sourceLine" id="cb250-5" data-line-number="5"><span class="ex">conda</span> install pandas</a>
<a class="sourceLine" id="cb250-6" data-line-number="6"><span class="ex">conda</span> install numba</a>
<a class="sourceLine" id="cb250-7" data-line-number="7"></a>
<a class="sourceLine" id="cb250-8" data-line-number="8"><span class="co"># those all ran great.</span></a>
<a class="sourceLine" id="cb250-9" data-line-number="9"></a>
<a class="sourceLine" id="cb250-10" data-line-number="10"><span class="ex">conda</span> install pysnptools</a>
<a class="sourceLine" id="cb250-11" data-line-number="11"></a>
<a class="sourceLine" id="cb250-12" data-line-number="12"><span class="co"># that one didn&#39;t find a match, but I found on the web that I should try:</span></a>
<a class="sourceLine" id="cb250-13" data-line-number="13"><span class="ex">conda</span> install -c bioconda pysnptools </a>
<a class="sourceLine" id="cb250-14" data-line-number="14"></a>
<a class="sourceLine" id="cb250-15" data-line-number="15"><span class="co"># that worked!</span></a></code></pre></div>
<p>Also we want to touch briefly on LD_PATH (linking failures—and note that libraries are often
named libxxx.a) and CPATH (for failure to find xxxx.h), etc.</p>
</div>
<div id="job-arrays" class="section level2">
<h2><span class="header-section-number">8.9</span> Job arrays</h2>
<p>Definitely mention the <code>eval</code> keyword in bash for when you want to print
command lines with redirects.</p>
<p>Show the routine for it, and develop a good approach to efficiently
orchestrating redos. If you know the taskIDs of the ones that failed
then it is pretty easy to write an awk script that picks out the
commands and puts them in a new file. Actually, it is probably
better to just cycle over the numbers and use the -t option
to launch each. Then there is now changing the job-ids file.</p>
<p>In fact, I am starting to think that the -t option is better than
putting it into the file.</p>
<p>Question: if you give something on the command line, does that override
the directive in the header of the file? If so, then you don’t even
need to change the file. Note that using the qsub command line options
instead of the directives really opens up a lot of possibilities for
writing useful scripts that are flexible.</p>
<p>Also use short names for the jobs and have a system for naming the
redos (append numbers so you know which round it is, too)
possibly base the name on the ways things failed the first time. Like,
<code>fsttf1</code> = “Fst run for things that failed due to time limits, 1”. Or
structure things so that redos can just be done by invoking it with -t
and the jobid.</p>
</div>
<div id="writing-stdout-and-stderr-to-files" class="section level2">
<h2><span class="header-section-number">8.10</span> Writing stdout and stderr to files</h2>
<p>This is always good to do. Note that <code>stdbuf</code> is super useful here so that
things don’t get buffered super long. (PCAngsd doesn’t seem to write antyhing till
the end…)</p>
</div>
<div id="breaking-stuff-down" class="section level2">
<h2><span class="header-section-number">8.11</span> Breaking stuff down</h2>
<p>It is probably worth talking about how problems can be broken down into
smaller ones. Maybe give an example, and then say that we will be talking about
this for every step of the way in bioinformatic pipelines.</p>
<p>One thing to note—sometimes processes go awry for one reason or another.
When things are in smaller chunks it is not such a huge investment to
re-run it. (Unlike stuff that runs for two weeks before you realize that
it ain’t working right).</p>

</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="working-on-remote-servers.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="introduction-to-reproducible-research.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/eriqande/eca-bioinf-handbook/edit/master/1.035-hpc.Rmd",
"text": "Edit"
},
"history": {
"link": "https://github.com/eriqande/eca-bioinf-handbook/commits/master/1.035-hpc.Rmd",
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["eca-bioinf-handbook.pdf", "eca-bioinf-handbook.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
